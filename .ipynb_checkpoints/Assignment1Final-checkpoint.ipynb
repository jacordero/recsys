{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathyn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242, 6, 26, 1, 63, 243], [11, 9, 584, 3, 67, 27, 244, 8, 585, 71, 14, 380, 21, 1], [381, 2, 8, 245, 112, 3, 49, 98, 57, 586, 4, 17, 587, 72, 1], [205, 14, 380, 9, 588, 19, 7, 17, 50, 246, 57, 589, 10], [7, 2, 44, 33, 1, 152, 8, 5, 205, 53, 11, 174, 246, 57], [30, 4, 9, 382, 10, 14, 247, 248, 15, 127, 15, 4, 58, 18, 1], [249, 250, 206, 14, 383, 27, 384, 2, 385, 386, 1, 590], [8, 387, 5, 591, 592, 59, 20, 388, 1, 389, 8, 138, 43, 2], [593, 1, 594, 46, 175, 5, 128, 63, 23, 595, 176, 207], [295, 71, 14], [55, 9, 112, 30, 27, 390, 10, 13, 596, 99, 11, 60, 7, 30], [27, 91, 31, 8, 1, 45, 3, 296, 1, 63, 83, 3, 297, 51, 64], [51, 64, 6, 92, 20, 298, 46, 4, 53, 7, 100, 597, 7], [598, 3, 14, 13, 4, 299, 3, 84, 599, 34, 28, 19, 34, 1, 66], [7, 22, 113, 78, 391, 19, 46, 1, 63, 600, 139, 5, 392], [31, 8, 114, 393, 251, 2, 129, 34, 7, 2, 79, 252, 21], [11, 394, 3, 14, 101, 18, 7, 601, 395, 14, 248, 13, 4, 17], [115, 130, 208, 5, 63, 23, 140, 5, 393, 251, 57, 5, 392], [3, 253, 31, 8, 7, 2, 602, 23, 603, 4, 207, 395, 1, 604], [102, 7, 2, 605, 9, 141, 10, 66, 3, 56, 7, 606, 26, 5, 153], [63, 243, 300, 1, 607], [10, 254, 131, 26, 52, 11, 102, 7, 115, 98, 382, 38], [10, 1, 301, 4, 9, 3, 67, 31, 54], [1, 63, 243, 52, 608, 21, 47, 5, 609, 18, 177, 45, 2, 79], [610, 175, 26, 30, 175, 13, 11, 17, 29, 5, 131, 3, 60], [40, 611, 41, 130, 4, 73, 41, 396, 26, 5, 27, 302], [140, 1, 127, 9, 27, 302, 57, 4, 209, 27, 303, 18, 4, 17], [612, 8, 66, 15, 4, 52, 26, 3, 154, 40, 14, 2, 3, 103, 44, 9], [116, 3, 304, 210, 104, 4, 211, 3, 154, 26, 2, 212, 31, 44], [4, 9, 255, 3, 19, 7, 9, 155, 305, 3, 56, 213, 79, 4], [129, 34, 1, 613, 8, 1, 127, 2, 306, 13, 39, 65, 397, 23], [398, 2, 205, 399, 68, 2, 55, 4, 400, 614, 2, 246], [615, 142, 616, 4, 139, 26, 5, 401, 256, 48, 8, 1, 399, 15], [4, 617, 7, 9, 618, 619, 620, 19, 3, 14, 117], [621, 7, 9, 622, 4, 99, 29, 47, 3, 623, 1, 401, 18, 402], [8, 624, 307, 30, 403, 3, 178, 7, 72, 48, 8, 1, 398, 15], [4, 209, 625, 7], [127, 53, 11, 3, 41, 102, 74, 5, 257, 15, 28, 6, 92], [60, 112, 8, 626, 26, 627, 38, 628, 39, 37, 22, 60, 42, 34], [308, 132, 6, 404, 35, 83, 213, 40, 7, 258, 32, 6, 209, 80, 1, 405], [8, 1, 156, 85, 9, 27, 406, 629], [26, 26, 26, 59, 1, 257, 115, 86, 3, 133, 309, 6, 103, 38], [310, 407, 6, 143, 311, 71, 28, 66, 4, 16, 408, 6, 69, 20, 138], [409, 259, 1, 630, 8, 1, 410, 105, 42, 56, 13, 59, 20, 214], [631, 407, 26, 6, 60, 18, 12, 56, 11, 17, 632, 312], [93, 8, 28, 179, 10, 14, 313, 10, 1, 633, 2, 411, 28], [9, 29, 5, 27, 157, 412, 18, 634, 80, 14, 413, 15, 55], [9, 50, 48, 3, 414, 3, 14, 314, 7, 9, 157, 635, 3, 83, 7, 100], [415, 13, 36, 40, 1, 158, 315, 19, 79, 6, 103, 44, 416], [57, 417, 6, 143, 118, 3, 11, 17, 50, 260, 44, 416, 9, 57], [417, 140, 19, 53, 39, 65, 215, 636, 159, 3, 83], [637, 4, 75, 54, 6, 103, 32, 6, 92, 257, 158, 144, 1], [410, 38, 418, 7, 37, 316, 3, 86, 31, 261, 1, 216, 13, 317, 23], [180, 318, 638, 1, 639, 6, 60, 4, 9, 181, 419], [55, 9, 50, 48, 640, 28, 66, 15, 7, 262, 35, 641, 34, 22, 1], [158, 642, 19, 6, 92, 84, 3, 263, 61, 44, 1, 319, 8, 1, 643], [33, 12, 62, 217, 320, 160, 33, 28, 420, 644, 57, 645, 2], [4, 211, 3, 646, 15, 4, 421, 321, 647, 15, 12, 322, 396], [144, 1, 264, 49, 12, 60, 12, 58, 323, 7, 2, 44, 133], [648, 24, 422, 4, 37, 60, 42, 18, 423, 50, 7, 37, 115, 49, 3], [263, 265, 6, 92, 56, 7, 649, 43, 409], [26, 26, 26, 55, 9, 112, 266, 3, 49, 30, 11, 76, 75], [182, 54, 87, 37, 424, 42, 27, 91, 3, 324, 6, 218, 60], [87, 9, 1, 267, 6, 650, 39, 37, 219, 14, 651, 8, 652, 34], [653, 66, 87, 70, 64, 6, 145, 12, 65, 26, 68, 23, 42, 55, 106, 50], [325, 10, 1, 264, 6, 77, 268, 19, 12, 161, 425, 5, 426, 2, 13, 36, 27], [47, 5, 25, 12, 62, 19, 49, 107, 134, 269, 6, 103, 2, 68, 11], [75, 3, 67, 181, 384, 2, 52, 21, 183, 3, 41, 10, 5, 654], [179, 8, 45, 49, 107, 134, 269, 49, 107, 134, 269, 2, 427, 49], [269, 134, 107, 18, 12, 56, 15, 4, 655, 35, 326, 140, 162], [7, 262, 35, 91, 428, 85, 45, 4, 178, 7, 4, 184, 13, 4, 9, 656], [80, 2, 17, 141, 429, 3, 657, 13, 4, 9, 430, 185, 10, 185, 23], [87, 2, 183, 3, 14, 27, 658, 81, 87, 146, 42, 1, 659], [99, 12, 119, 134, 5, 426, 46, 175, 431, 431, 26, 4, 163, 142], [5, 660, 8, 661, 2, 164, 662, 2, 1, 257, 9, 100], [11, 9, 29, 5, 432, 433, 2, 4, 663, 43, 21, 3, 14, 101, 10, 5, 131], [4, 129, 43, 19, 7, 9, 22, 305, 664, 130, 14, 9, 254], [94, 327, 2, 1, 128, 63, 9, 314, 10, 328, 665, 26, 7], [55, 9, 29, 5, 131, 3, 20, 434, 165, 52, 11, 47, 1, 666, 2], [9, 141, 10, 66, 3, 296, 7, 83, 15, 7, 220, 5, 435, 51, 70, 436], [2, 437, 38, 298, 7, 36, 138, 4, 9, 295, 329, 7, 46, 4], [220, 1, 435, 19, 1, 63, 9, 50, 667, 3, 20, 208, 4, 73], [41, 10, 5, 94, 186, 147, 85, 9, 668, 43, 71, 5, 438, 8, 669, 670], [256, 1, 439], [55, 65, 440, 22, 108, 1, 147, 19, 39, 65, 22, 671, 2, 46], [11, 17, 166, 22, 1, 45, 26, 48, 441, 2, 43, 1, 221, 187, 330], [95, 4, 442, 443, 26, 1, 444, 672, 38, 4, 9, 119, 3], [67, 31, 54], [175, 4, 163, 142, 5, 24, 331, 673, 120, 22, 206, 8, 674], [222, 55, 9, 112, 21, 7, 675, 5, 445, 188, 148, 2, 11, 36], [104, 53, 9, 13, 7, 161, 676, 3, 48, 8, 1, 440, 8, 1, 147], [19, 332, 140, 1, 677, 65, 155, 153, 57, 1, 148, 9, 155, 167], [19, 34, 121, 270, 7, 59, 29, 678, 121, 8, 61, 135, 21, 1, 679], [66, 108, 4, 163, 142, 5, 186, 680, 4, 17, 29, 306, 130, 2], [329, 7, 9, 5, 24, 95, 40, 681, 333, 168, 4, 211, 1], [24, 188, 148, 10, 1, 682, 2, 3, 14, 117, 683, 7, 684], [11, 446, 1, 95, 2, 73, 13, 7, 447, 72, 5, 167, 327, 29], [91, 448, 223, 5, 685, 243, 4, 686, 26, 2, 129, 334, 1, 327], [72, 1, 687, 169, 12, 119, 400, 38, 4, 688, 3, 67, 31, 8], [13, 305, 147, 2, 689, 40, 261, 271, 690, 8, 335, 691, 2], [271, 692, 693, 19, 4, 58, 29, 258, 67, 14, 189, 144, 1], [694, 2, 258, 32, 70, 189, 59, 82, 144, 53, 109, 11, 7], [59, 20, 8, 27, 24, 152, 174, 70, 695, 51, 38, 6, 145, 6, 58], [449, 43, 47, 5, 450, 6, 60, 6, 58, 32, 6, 122, 451, 38, 3, 696], [18, 12, 56, 30, 310, 31, 8, 1, 45, 93, 17, 336, 697], [13, 11, 17, 429, 3, 60, 13, 27, 337, 93, 190, 65, 272], [55, 113, 3, 20, 50, 152, 10, 452, 71, 1, 24, 95, 30, 4, 52], [110, 3, 1, 120, 149, 453, 4, 161, 150, 254, 148, 21, 7, 57, 34], [121, 270, 5, 205, 8, 454, 18, 455, 216, 43, 47, 698, 28], [66, 4, 73, 5, 24, 273, 21, 7, 85, 456, 9, 29, 68], [130, 16, 11, 2, 108, 1, 699, 8, 1, 273, 9, 5, 700], [701, 23, 1, 159, 338, 42, 457, 702, 21, 7, 10, 153], [7, 9, 22, 27, 127, 3, 83, 338, 42, 19, 1, 703, 24, 11, 9], [29, 116, 3, 49, 13, 10, 5, 339, 50, 6, 37, 154, 104, 4, 16, 2], [56, 386, 7, 36, 224, 340, 57, 29, 18, 4, 17, 704, 312, 215], [24, 705, 40, 274, 96, 17, 118, 706, 2, 707, 43, 71, 708], [709, 2, 221, 710, 93, 22, 711, 39, 59, 29, 219], [1, 712, 454, 180, 713, 17, 714, 61, 74, 15, 13, 5, 715, 249], [716, 191, 717, 12, 32, 12, 458, 7, 155, 94, 2, 13, 32, 12, 718, 88], [341, 27, 719, 23, 5, 720, 7, 459, 721, 2, 4, 17, 115], [460, 13, 32, 12, 338, 91, 256, 5, 273, 224, 340, 7, 33], [342, 343, 3, 722, 23, 12, 723, 57, 724], [135, 28, 273, 9, 29, 224, 340, 30, 11, 725, 3, 461], [7, 2, 462, 7, 27, 215, 7, 17, 10, 463, 5, 179, 8, 726, 727], [8, 728, 729, 730, 731, 732, 733, 734, 735, 2, 249], [736, 737, 4, 27, 76, 344, 7, 80], [], [], [], [44, 5, 345, 464, 16, 11, 6, 69, 20, 455, 43, 47, 5], [2, 30, 7, 9, 190, 4, 9, 81, 122, 738, 333, 168, 2, 14, 346], [739, 43, 34, 1, 53, 13, 4, 9, 81, 1, 158, 465, 18, 116], [144, 1, 24, 95, 72, 13, 740, 169, 104, 135, 4], [466, 18, 5, 337, 467, 3, 56, 32, 4, 9, 116, 3, 741, 121, 742], [4, 184, 5, 24, 743, 40, 28, 18, 7, 161, 309, 12, 62, 16], [11, 3, 41, 10, 70, 116, 31, 468, 47, 5, 347, 6, 103], [44, 6, 218, 20, 47, 79, 2, 4, 211, 3, 321, 44, 1, 744, 8, 5], [347, 33, 47, 102, 1, 347, 33, 745, 31, 18, 4, 58, 29, 219], [119, 245, 208, 74, 5, 89], [102, 5, 192, 462, 13, 112, 123, 336, 4, 746, 21, 116], [72, 1, 169, 34, 98, 19, 332, 18, 109, 11, 46, 4, 118, 3, 1], [95, 4, 73, 4, 17, 460, 1, 24, 188, 148, 2, 46, 4], [52, 110, 3, 1, 120, 18, 7, 4, 73, 4, 58, 29, 747, 469], [7, 4, 58, 56, 7, 78, 748, 144, 1, 222, 2, 4, 211, 14], [225, 3, 749, 43, 48, 8, 1, 750, 8, 1, 120, 19, 7, 9, 155, 751], [2, 46, 4, 17, 244, 41, 31, 23, 187, 1, 109, 24, 89], [226, 26, 2, 124], [86, 55, 36, 50, 152, 10, 470, 47, 13, 16, 11, 3, 41], [181, 471, 6, 752, 12, 3, 472, 80, 28, 473, 4, 348], [474, 41, 27, 157, 753, 411, 4, 27, 754, 755, 7], [2, 427, 4, 756, 41, 30, 475, 15, 3, 757, 170, 72], [14, 176, 2, 98, 4, 349, 187, 3, 350, 14, 247, 436, 18, 245], [758, 41, 10, 5, 759, 8, 760, 4, 9, 761, 476, 41], [18, 28, 345, 477, 9, 27, 478, 8, 762, 3, 20, 227, 216], [19, 7, 36, 50, 152, 81, 53, 109, 11, 3, 763, 3, 20, 227, 216], [132, 55, 36, 764, 275, 8, 42, 351, 3, 212, 48, 765, 352], [76, 14, 353, 209, 21, 5, 24, 222, 350, 13, 9, 354, 300, 1, 120], [4, 446, 7, 2, 73, 10, 7, 5, 27, 167, 355, 21, 85, 1, 159], [134, 42, 65, 457, 224, 10, 766, 127, 6, 37, 134, 7, 16], [11, 2, 32, 7, 479, 42, 480, 448, 6, 97, 469, 1, 148, 2, 32, 7], [479, 42, 480, 767, 6, 97, 768, 300, 1, 95, 30, 140, 45, 6, 37], [67, 72, 1, 169, 2, 6, 171, 35, 769, 85, 481], [4, 770, 5, 24, 432, 2, 16, 276, 3, 41, 85, 45, 85], [45, 482, 14, 185, 21, 1, 405, 8, 14, 189, 3, 383, 85, 45, 7, 9], [483, 2, 4, 9, 78, 277, 3, 150, 13, 4, 771, 1, 193], [465, 3, 20, 136, 28, 348, 481, 46, 48, 772, 355, 19, 11], [17, 118, 30, 91, 72, 1, 45, 8, 773, 112, 19, 31, 8, 1, 45], [93, 3, 304, 13, 7, 113, 78, 774, 2, 385, 18, 356, 3, 82, 21], [10, 1, 775, 45], [30, 4, 776, 3, 777, 2, 27, 76, 344, 80, 1, 355], [], [], [], [242, 778, 1, 137, 8, 170], [484, 2, 484, 124, 11, 4, 9, 30, 91, 277, 13], [18, 1, 131, 4, 78, 485, 38, 3, 278, 157, 228, 81, 6, 77], [779, 31, 47, 1, 780, 450, 13, 119, 9, 157, 781, 101], [18, 46, 4, 129, 26, 34, 14, 101, 39, 113, 3, 20, 342, 31, 8], [328, 39, 65, 138, 30, 486, 80, 51, 70, 109, 24, 101, 6, 103], [96, 191, 178, 21, 88, 782, 2, 783, 18, 12, 81, 487, 6, 77, 136], [6, 784, 35, 20, 785, 6, 92, 20, 5, 117, 357, 155, 486, 80, 3, 389], [786, 40, 12, 12, 69, 323, 1, 225, 45, 12, 97, 19, 6, 69, 20], [787, 3, 61, 53, 11, 57, 265, 39, 172, 35, 317, 1, 45, 6, 788], [3, 82, 105, 42, 56, 6, 37, 488, 61, 5, 420, 229, 8, 789, 330, 790], [2, 4, 52, 21, 791, 3, 41, 38, 4, 59, 323, 7, 39, 69], [82, 71, 1, 792, 4, 53, 2, 38, 418, 7, 37, 316, 489], [793, 3, 48, 36, 247, 101, 2, 38, 794, 1, 795, 191, 154], [11, 36, 158, 490, 796], [797], [259, 1, 798], [23, 11, 36, 799], [51, 64, 44, 491, 6, 77, 182], [141, 79, 14, 189, 800, 476, 1, 439, 8, 1, 147, 10, 463, 4, 9], [81, 123, 223, 492, 101, 168, 2, 4, 34, 98, 139, 43, 1, 24, 188], [148, 2, 252, 80, 3, 1, 169, 95], [109, 11, 7, 9, 15, 91, 15, 4, 58, 49, 354, 26, 21, 48, 441, 3], [154, 144, 72, 1, 169, 23, 48, 353, 19, 3, 67, 144, 9, 123], [801, 223, 119, 4, 226, 26, 2, 75, 3, 493, 54], [12, 299, 3, 20, 802, 8, 494, 16, 11, 5, 117, 422, 47], [12, 4, 161, 127, 83, 28, 3, 82, 21, 470, 10, 28, 45, 495, 28], [131, 6, 146, 12, 19, 4, 52, 21, 22, 1, 193, 803, 804, 8], [170, 805, 55, 9, 5, 153, 137, 22, 108, 14, 40, 214, 333], [302, 2, 806, 149, 26, 1, 147], [102, 5, 66, 4, 230, 5, 24, 496, 8, 101, 10, 1, 315, 2], [4, 279, 807, 14, 176, 3, 56, 44, 9, 255, 7, 9, 1, 128], [63, 808, 809, 810, 23, 5, 229, 8, 128, 231, 125, 10], [48, 185, 2, 5, 153, 126, 10, 1, 221, 111, 163, 497, 334, 10, 5, 117], [339, 498, 3, 811, 15, 111, 163, 51, 1, 280, 1, 280], [51, 172, 35, 4, 20, 812, 32, 6, 143, 281, 14, 452, 11, 184, 30], [813, 13, 4, 9, 232, 3, 263, 499, 8, 121, 48, 30, 46, 1, 63], [163, 259, 14, 4, 75, 10, 5, 186, 814, 233, 32, 12, 217, 500], [1, 63, 394, 815, 358, 1, 128, 231, 125, 2, 1, 126], [2, 816, 165, 72, 1, 817, 15, 501, 15, 111, 58, 82], [11, 139, 43, 1, 126, 2, 125, 2, 15, 1, 147, 9, 27, 249, 4], [281, 818, 41, 22, 1, 66, 4, 52, 21, 182, 64, 64, 38], [234, 282, 33, 3, 250, 2, 819, 93, 52, 21, 141, 15, 820], [6, 103, 32, 6, 143, 166, 235, 10, 1, 324, 105, 42, 60, 9, 6, 1], [193, 46, 6, 118, 43, 28, 502, 6, 342, 60, 6, 97, 219, 464, 5], [24, 821, 19, 32, 6, 77, 29, 1, 193, 1, 210, 162, 33, 96], [10, 1, 301, 160, 6, 503, 13, 36, 1, 117, 822, 2, 4, 75, 823], [100, 22, 1, 274, 4, 451, 13, 65, 8, 1, 193, 504, 15, 41, 3], [56, 32, 4, 58, 84, 166, 235, 18, 121, 8, 61], [6, 77, 136, 6, 77, 29, 824, 4, 16, 18, 14, 505, 825, 10, 74, 94], [506, 2, 507, 173, 35, 82, 10, 506, 34, 22, 2, 6, 77, 136, 6, 97, 35], [20, 283, 18, 6, 62, 22, 508, 8, 93, 2, 4, 51, 4, 826, 74, 5], [27, 24, 827, 4, 36, 4, 2, 6, 77, 6, 2, 51, 64, 38, 509], [7, 22, 33, 6, 37, 194, 32, 6, 62, 22, 1, 93, 6, 510, 3, 62, 105, 42], [56, 214, 359, 828, 33, 829, 2, 214, 359, 830, 33, 831, 2], [214, 359, 832, 33, 51, 64, 6, 92, 115, 67, 3, 833, 34, 13, 270], [135, 1, 834, 120, 173, 35, 835, 105, 36, 194, 836], [837, 33, 1, 284, 8, 511, 2, 511, 33, 1, 284, 8, 512, 2], [512, 50, 13, 36, 22, 838, 6, 77, 343, 6, 69, 84, 166, 235, 18], [283, 6, 37, 194, 2, 83, 38, 513, 1, 24, 2, 4, 839, 14], [514, 21, 14, 840, 15, 32, 4, 65, 183, 313, 2, 75, 3, 841, 7], [19, 14, 233, 842, 843, 2, 844, 2, 1, 159, 99, 29, 86, 1], [193, 15, 39, 510, 3, 49], [38, 513, 1, 24, 845], [846, 151, 847, 285], [2, 848, 1, 849, 8, 1, 850], [21, 330, 188, 851], [38, 852, 111, 360, 3, 853], [38, 854, 855, 151, 856], [2, 857, 24, 858, 10], [23, 859, 860, 861], [6, 77, 136, 271, 106, 29, 1, 158, 159, 16, 109, 11, 2, 14, 176], [397, 23, 170, 54, 15, 4, 52, 21, 6, 69, 20, 283, 102, 22, 2], [6, 92, 84, 3, 82, 2, 862, 10, 13, 863, 24, 156, 2, 84, 210, 3], [50, 864, 3, 865, 23, 2, 51, 119, 30, 310, 313, 3, 866, 50, 6, 143], [206, 43, 70, 248, 40, 7, 32, 6, 77, 283, 6, 37, 361, 26, 68, 7, 37, 20, 50], [152, 180, 867, 180, 318, 26, 2, 183, 86, 43, 54, 64, 6], [92, 122, 154, 43, 2, 83, 96, 160, 6, 79, 146, 42, 13, 104, 2, 79], [32, 6, 47, 362, 13, 352, 6, 37, 86, 43, 32, 29, 6, 37, 361, 26, 68], [868, 6, 77, 307, 266, 19, 51, 64, 124, 11, 23, 5, 363, 869], [8, 170, 6, 49, 145, 39, 59, 178, 180, 318, 26, 6, 160, 30, 27, 244], [8, 362, 22, 515, 68], [15, 4, 16, 28, 4, 129, 26, 34, 14, 514, 2, 9, 277, 3, 56], [13, 4, 17, 178, 21, 48, 8, 1, 63, 36, 24, 128, 231, 125, 192], [4, 9, 182, 38, 97, 6, 84, 516, 13, 4, 53, 6, 69], [20, 483, 167, 54, 4, 118, 43, 2, 52, 3, 1, 120, 3, 870], [41, 71, 7, 2, 73, 13, 15, 871, 15, 4, 58, 872, 4, 9, 81], [40, 227, 101, 168, 2, 9, 116, 21, 517, 873, 4, 76, 73], [31, 13, 1, 364, 8, 28, 9, 1, 126, 4, 9, 482, 2, 4, 358], [7, 279, 141, 10, 66, 3, 874, 517, 165, 468], [13, 9, 5, 875, 876, 16, 11, 5, 157, 357, 518, 34, 1], [363, 519, 19, 27, 419, 3, 150, 41, 314, 10, 877, 2], [81, 18, 1, 169, 2, 4, 207, 23, 22, 878, 110, 3, 1, 24, 95], [19, 332, 1, 24, 95, 9, 449, 54, 2, 1, 24, 188, 148, 9], [354, 21, 1, 222, 120, 15, 130, 2, 93, 106, 879, 223, 119], [53, 1, 109, 477, 18, 6, 115, 9, 30, 167, 15, 28, 130, 115], [2, 6, 880, 7, 36, 155, 520, 13, 7, 33], [15, 4, 16, 881, 159, 14, 490, 521, 2, 10, 254, 131, 882], [4, 9, 43, 3, 14, 883, 10, 522, 365, 14, 104, 260, 9, 13, 4], [17, 884, 311, 72, 1, 523, 2, 10, 13, 524, 6, 97, 82, 110, 71], [525, 4, 16, 3, 41, 11, 17, 166, 3, 1, 885, 98, 10], [14, 356, 2, 17, 86, 3, 1, 886, 887, 13, 888, 12, 82], [3, 21, 1, 228, 889, 12, 150, 5, 890, 8, 891, 892, 10, 1], [523, 177, 274, 893, 10, 1, 894, 23, 895, 896, 79, 5, 438], [8, 897, 898, 2, 329, 61, 5, 525, 899, 135, 4, 76], [206, 31, 13, 4, 9, 10, 1, 137, 8, 170, 85, 4, 17, 900, 46, 4], [9, 492, 101, 168], [6, 145, 6, 526, 35, 124, 30, 91, 16, 11, 15, 4, 236, 40, 187], [3, 150, 14, 45, 31, 6, 92, 20, 901, 18, 7, 81, 6, 527, 71], [362, 902, 10, 70, 247, 170, 13, 191, 20, 5, 234, 89, 3, 20, 136], [135, 282, 33, 234, 3, 250], [141, 79, 4, 230, 286, 903, 40, 10, 1, 137, 5, 24, 45], [80, 2, 4, 236, 904, 3, 212, 31, 44, 7, 9, 34, 104, 4, 53], [7, 69, 20, 5, 905, 57, 906, 19, 79, 4, 349, 38, 167], [4, 9, 81, 2, 4, 76, 206, 31, 13, 7, 9, 122, 5, 25, 13, 17], [521, 10, 47, 41], [59, 7, 20, 8, 121, 152, 81, 53, 11, 3, 278, 3, 28, 25], [282, 33, 30, 31, 8, 1, 45, 26, 68, 13, 6, 218, 60, 27], [406, 7, 97, 237, 34, 121, 270, 55, 36, 50, 907, 10, 187, 30, 4], [75, 366, 25, 49, 12, 62, 1, 45, 31, 8, 28, 137, 6, 160, 27, 244], [8, 528, 40, 68, 366, 25, 11, 53, 28, 69, 20, 1, 158], [45, 8, 529, 3, 5, 25, 4, 17, 115, 516, 74, 5, 89, 130, 19], [4, 349, 245, 208, 10, 14, 908, 36, 909, 910, 5, 25, 8], [5, 25, 3, 5, 25, 5, 25, 366, 25, 1, 25, 129, 34, 14, 181], [911, 2, 113, 3, 14, 3, 912, 23, 48, 8, 114, 24, 176], [19, 7, 16, 112], [265, 7, 173, 35, 530, 228, 53, 11, 6, 913, 7, 36], [5, 531, 25, 86, 100, 23, 287, 1, 532, 18, 23, 22], [14, 413, 8, 367, 11, 17, 50, 27, 914, 915, 38, 94, 916], [213, 17, 336, 30, 4, 75, 54, 917, 918, 320, 919, 85], [9, 1, 104, 920, 10, 14, 531, 533, 205, 1, 25, 474, 5], [363, 921, 31, 8, 1, 365, 2, 113, 3, 922, 22, 100, 23, 923], [51, 6, 195, 88, 288, 124, 11, 279, 268, 13, 4, 17, 433], [1, 109, 924, 36, 925, 6, 78, 485, 12, 262, 35, 47, 107], [29, 47, 107, 124, 1, 25, 10, 5, 926, 927, 233, 59], [12, 47, 107, 32, 12, 65, 42], [127, 265, 29, 16, 11, 10, 5, 928, 196, 171, 35, 20, 534], [40, 7, 2, 929, 6, 145, 6, 58, 535, 12, 197, 267, 87, 6, 60, 12, 198], [253, 5, 321, 3, 107, 32, 12, 58, 122, 56, 14, 4, 33, 74, 5, 64, 930], [89, 11, 52, 21, 149, 3, 41, 15, 4, 236, 931, 40, 10, 1], [137, 2, 4, 932, 933, 30, 934, 71, 1, 935, 936, 14, 536, 2], [937, 14, 346, 2, 4, 33, 74, 5, 215, 938, 89, 3, 537, 2, 4, 36], [74, 5, 284, 48, 18, 538, 325, 51, 6, 195, 88, 288, 124], [11, 54, 18, 28, 66, 1, 25, 9, 939, 22, 100, 2, 4], [184, 343, 7, 69, 20, 272, 238, 239, 172, 35, 237, 40, 14, 121], [123, 32, 12, 198, 181, 29], [239, 190, 124, 1, 25, 96, 9, 368, 26, 3, 1, 309, 8, 151], [285, 15, 32, 6, 59, 237, 21, 74, 5, 539, 197, 940, 369, 941], [107, 942, 186, 943, 93, 171, 35, 105, 42, 296, 1, 319, 54], [6, 172, 35, 190, 16, 11, 10, 5, 117, 339, 3, 519, 1, 539, 8], [944, 106, 12, 106, 12, 478, 8, 8, 370, 1, 25, 99, 29], [326, 30, 11, 52, 21, 371, 55, 33, 74, 5, 215, 24, 945, 259], [197, 156, 6, 218, 47, 3, 535, 12, 5, 24, 335, 946, 947, 12], [62, 23, 51, 74, 94, 948, 949, 505, 2, 7, 37, 372, 93, 46], [12, 950, 61, 2, 7, 37, 540, 43, 2, 195, 18, 114, 951, 2, 22, 508], [8, 93, 6, 97, 35, 219, 149, 8, 61, 2, 7, 952, 3, 5, 953], [12, 62, 2, 111, 541, 7, 36, 30, 542, 7, 36, 388, 5, 954, 955, 111], [541, 7, 956, 22, 1, 957, 2, 51, 64, 124, 11, 10, 5, 958], [196, 6, 77, 268, 6, 143, 238, 7, 54, 18, 1, 25, 9, 528], [165, 256, 14, 15, 501, 15, 7, 58, 82, 2, 387, 78, 5, 959, 10], [1, 137, 15, 7, 52], [30, 4, 199, 960, 102, 7, 25, 64, 49, 86, 110, 54, 2, 239], [172, 35, 237, 40, 107, 57, 370, 140, 32, 12, 171, 35, 47, 61, 46, 1], [25, 230, 28, 7, 220, 108, 2, 236, 303, 110, 3, 14, 114], [346, 9, 78, 961, 23, 962, 11, 53, 2, 7, 16, 10, 5, 186], [368, 233, 105, 373, 67, 3, 1, 543, 2, 79, 6, 37, 146, 12, 70], [367, 2, 12, 37, 530, 132, 7, 33, 6, 544, 107, 2, 370], [7, 9, 168, 66, 3, 82, 18, 1, 137, 9, 138, 78, 289, 23, 1], [200, 2, 545, 13, 17, 311, 72, 7, 55, 65, 5, 374, 2, 5, 90], [5, 201, 2, 133, 375, 2, 312, 221, 345, 963, 11, 447, 1], [45, 2, 1, 290, 240, 236, 3, 1, 543], [242, 964, 5, 376, 202, 2, 5, 94, 377], [39, 65, 190, 5, 234, 203, 240, 13, 965, 21, 1, 381, 1], [200, 23, 966, 967, 1, 545, 23, 180, 546, 968, 295], [3, 61, 2, 22, 969, 547, 970, 2, 971], [1, 104, 162, 8, 204, 9, 38, 3, 67, 164, 54, 39, 17, 5], [972, 40, 28, 2, 102, 5, 337, 467, 7, 113, 78, 391], [3, 11, 3, 150, 41, 182, 973, 23, 61, 15, 32, 4, 17], [974, 61, 22, 14, 356, 190, 4, 17, 78, 5, 94, 975, 23, 1], [201, 96, 34, 291, 220, 976, 2, 59, 122, 83, 6, 160, 977, 223], [12, 2, 69, 62, 548, 2, 28, 11, 59, 29, 978, 174], [979, 38, 292, 7, 9, 2, 15, 1, 201, 980, 981, 3, 146, 114], [504, 55, 9, 50, 123, 3, 20, 16], [34, 291, 1, 25, 96, 113, 3, 20, 5, 352, 8, 982, 261, 61], [199, 31, 540, 26, 22, 8, 12, 2, 414, 3, 42, 6, 37, 76, 212, 12], [164, 275, 39, 22, 226, 26, 34, 98, 10, 5, 153, 549, 23, 1, 25], [10, 1, 444, 11, 281, 14, 176, 276, 983, 21, 7, 18, 4, 184], [136, 4, 59, 425, 5, 520, 984, 32, 4, 99, 29, 67, 164, 27, 76], [985, 16, 1, 25, 23, 133, 986, 264, 106, 12, 22, 232, 28], [33, 1, 987, 89, 6, 62, 550, 22, 108, 32, 12, 217, 287], [1, 532, 988, 364, 9, 989, 71, 1, 990, 9, 76, 991], [3, 71, 1, 228, 96, 551, 992, 2, 17, 166, 8, 298, 91], [993, 3, 994, 2, 995, 552, 2, 553, 1, 554, 8], [555, 2, 556], [996, 16, 1, 201, 23, 5, 997], [6, 195, 88, 288, 16, 1, 25, 998, 19, 27, 999, 99], [29, 6, 16, 1, 201, 279], [6, 53, 12, 99, 16, 1, 25, 6, 1000, 552, 2, 553], [1, 554, 8, 555, 2, 556, 1001, 18, 293, 2, 258, 1002], [1, 1003, 1004, 8, 1005, 73, 7, 557], [73, 44, 16, 1, 374], [73, 7, 1, 25, 378, 181, 1006, 8, 204, 12, 62, 44], [6, 62, 44, 7, 1007, 127, 275, 46, 6, 150, 5, 89, 16, 1], [374, 7, 36, 348, 5, 1008, 57, 5, 1009, 1, 162, 33, 44, 99, 1], [1, 25, 99, 29, 1010, 28, 162, 19, 1011, 52, 21, 73], [7, 557, 3, 82, 23, 1012, 1013, 3, 558, 287, 2, 1014, 293, 1], [1015, 287, 36, 1016, 34, 104, 9, 1017, 19, 1, 1018, 8, 151], [1019, 38, 106, 12, 138, 21, 81, 70, 64, 7, 1020, 379], [3, 11, 15, 7, 421], [15, 547, 15, 119, 16, 11, 10, 5, 559, 196, 7, 173, 35, 316, 3], [164, 42, 34, 22], [10, 13, 524, 16, 1, 90, 560, 1021, 3, 114, 101, 6, 1022], [13, 1, 1023, 1024, 18, 1, 1025, 1026, 8, 123, 1027], [278, 228, 16, 1, 375, 6, 171, 35, 62, 1, 1028, 8, 149], [271, 94, 159, 2, 44, 36, 123, 6, 171, 35, 1029, 12, 49, 140, 2], [1, 375, 1030, 26, 114, 189, 3, 1031, 5, 1032, 177, 8, 1, 221, 200], [44, 6, 9, 116, 3, 83, 16, 1, 90, 10, 133, 238, 196, 9, 13], [1, 225, 89, 3, 67, 373, 164, 59, 20, 5, 376, 202], [44, 33, 5, 376, 202, 16, 11, 29, 13, 4, 551, 91, 3, 62], [19, 1, 90, 17, 1033, 15, 32, 7, 53, 13, 307, 299, 3, 278], [2, 50, 48, 266, 113, 1034, 3, 83, 213], [132, 16, 1, 90, 1, 225, 45, 3, 561, 7, 33, 3, 49, 7, 2, 15], [12, 161, 47, 3, 194, 1, 89, 494, 177, 1035, 250, 6, 191, 146], [12, 38, 1, 90, 403, 7], [104, 7, 224, 31, 5, 202, 204, 10, 5, 179, 8, 1036, 1, 1037], [1038, 173, 35, 428, 7, 16, 2, 79, 22, 1, 240, 65, 1039], [334, 1, 204, 68, 2, 55, 55, 9, 50, 48, 227, 331, 2], [165, 19, 39, 75, 562, 46, 39, 563, 2, 351, 80, 46, 39], [563, 30, 13, 7, 9, 29, 1040, 3, 62, 46, 1, 202, 9, 100, 135], [46, 39, 17, 166, 562, 149, 133, 1041, 57, 30, 2, 65, 78, 164, 54], [1, 90, 175, 199, 31, 1, 202, 33, 100, 2, 39, 22, 289], [108, 7, 1042, 2, 423, 19, 96, 564, 172], [28, 162, 1, 90, 58, 29, 326, 174, 5, 117, 357, 8, 53], [2, 7, 226, 18, 5, 94, 66, 23, 48, 341, 1043, 142, 114, 1044], [1, 1045, 10, 85, 12, 459, 56, 1046, 10, 1, 246], [8, 293, 192, 1, 1047, 466, 10, 550, 34, 291, 1, 90, 16], [1048, 564, 172, 2, 22, 69, 84, 241], [19, 96, 33, 3, 488, 1, 241, 78, 5, 565, 8, 1049, 1050], [132, 4, 8, 204, 16, 1, 90, 1051, 3, 11, 23, 48, 341], [2, 1, 290, 240, 34, 98, 289, 108, 14, 1052, 31, 10, 5, 1053], [45, 241, 241], [11, 17, 50, 260, 44, 3, 49, 2, 10, 1054, 4, 178, 14, 185, 10, 14], [251, 2, 1055, 31, 5, 350, 8, 566, 1056, 1, 522, 365, 17], [29, 118, 72, 7, 2, 1057, 61, 108, 15, 241, 55, 9, 1058, 48], [5, 1059, 22, 108], [19, 4, 69, 84, 5, 1060, 41, 12, 62, 16, 1, 25], [8, 204, 1, 90, 378, 27, 1061, 44, 266, 84, 12, 118, 10], [88, 251, 111, 52, 21, 379, 3, 11], [122, 5, 294, 16, 11, 443], [185, 7, 100, 68, 16, 1, 90], [79, 39, 22, 289, 108, 14, 98, 123, 192, 1, 90, 560], [1062, 1, 294, 183, 239, 195, 88, 1063, 8, 28, 1064], [294, 2, 46, 7, 17, 344, 28, 1065, 567, 39, 22, 1066], [11, 53, 1, 290, 89, 27, 1067, 19, 39, 22, 129, 30, 1068], [13, 4, 99, 29, 1069, 3, 1070, 2, 15, 4, 58, 29, 60, 8, 213], [3, 83, 4, 1071, 1072, 2, 139, 1, 294, 203, 15, 1073, 15, 4], [1, 210, 89, 9, 3, 134, 1, 566, 28, 568, 177, 1074, 2], [1075, 15, 1, 153, 200, 1076, 13, 39, 58, 29, 461], [1077, 2, 1, 167, 1078, 1079, 2, 17, 3, 20, 1080, 21, 1, 110], [135, 7, 9, 100, 34, 291, 2, 39, 226, 26, 54, 10, 5, 549, 2], [1081, 1, 25, 3, 146, 61, 286, 123], [12, 1082, 3, 146, 42, 88, 367, 12, 62, 16, 11, 2, 132], [7, 33, 12, 544, 1083, 2, 198, 4, 1084, 10, 5, 1085, 149, 268, 13, 7], [59, 20, 238, 54], [507, 33, 5, 94, 2, 5, 569, 377, 16, 1, 25, 379, 3, 11, 2], [7, 33, 5, 94, 285, 456, 16, 11, 203, 26, 23, 103, 34], [1, 25, 36, 285, 19, 132, 49, 12, 1086, 7, 569, 2, 4, 281, 21, 509], [40, 7, 192, 1, 25, 9, 529, 30, 13, 14, 260, 8, 1, 377, 9], [286, 47, 28], [570, 16, 3, 5], [25, 13, 111], [1087, 10, 1], [156], [105, 373], [1088, 82, 3], [1089, 6, 191], [1090], [12, 86], [6, 37, 253, 50], [1091, 239], [69, 84, 5], [571, 18], [272, 28], [502, 6, 143], [112], [3, 49], [16, 1], [25, 3, 1], [1092, 74], [5, 571], [64, 500], [23], [50, 572], [57, 573], [59, 20], [1093], [197], [1094], [6, 37, 20], [573, 6, 37], [20, 572], [16], [1095], [292, 570], [6, 37], [194, 1], [290], [364], [2], [1096], [12], [3], [1097], [12, 106, 29, 1098, 16, 1, 25, 3, 11, 475, 44, 106, 12], [6, 195, 88, 288, 16, 11, 27, 1099, 12, 17, 118, 3, 1, 1100], [1101, 6, 60], [6, 17, 29, 124, 1, 25, 471, 2, 27, 1102], [5, 1103, 16, 11, 369, 232, 3, 212, 41, 542, 2, 203], [276, 40, 14, 51, 49, 105, 42, 499, 3, 1104, 7], [6, 92, 49, 112, 8, 1, 179, 16, 1, 25, 138, 43, 2, 430], [165, 12, 1105, 42, 71, 182, 74, 491], [6, 262, 35, 1106, 7, 1107, 109, 11, 19, 12, 322, 30, 1108, 238], [1, 25, 122, 1109, 10, 1110], [217, 86, 110, 2, 574, 88, 575, 11, 199, 102, 7, 2, 1], [1111, 22, 1112, 10, 565, 415, 217, 49, 19, 1, 25, 122, 1113], [114, 189, 1114, 2, 442, 5, 24, 1115], [44, 5, 1116, 7, 404, 35, 361, 1117, 1, 201, 15, 76, 15, 7, 9, 78], [31, 8, 328, 2, 133, 292, 576, 139, 1, 412, 8, 183, 3, 14], [1118, 503, 70, 64, 105, 28, 20, 5, 533, 3, 12, 115, 3, 1119], [88, 1120, 458, 88, 1121, 320, 16, 1, 1122, 576, 5, 24], [1123, 12, 322, 275, 3, 194, 1, 1124, 8, 133, 1125], [6, 145, 6, 17, 197, 87, 68, 6, 62, 6, 49, 16, 11, 408, 1126], [577, 10, 1127, 4, 198, 76, 372, 7, 110], [2, 96, 33, 87, 32, 6, 161, 1128, 3, 263, 1, 162, 16, 1], [11, 378, 371, 18, 4, 9, 369, 232, 3, 237, 40, 14, 1129], [87, 36, 197, 267, 2, 4, 36, 74, 5, 284, 48, 18, 538, 325, 12], [97, 35, 60, 2, 51, 6, 145, 12, 58, 56, 14, 102, 1, 200, 132], [4, 37, 134, 5, 24, 1130, 15, 76, 15, 154, 34, 7], [28, 567, 568, 5, 390, 1131, 261, 1, 240, 177, 8, 1], [200, 252, 80, 34, 98, 48, 292, 1132, 75, 1133, 297, 43, 27], [1134, 1135, 6, 272, 69, 20, 138, 308, 1, 324, 264], [173, 35, 1136, 70, 1137, 2, 5, 1138, 199, 31, 10, 5, 368, 233, 3], [114, 274, 86, 165, 70, 487, 7, 36, 168, 66, 12, 65, 22, 10, 1139], [21, 1140, 1141, 39, 22, 1142, 80, 2, 11, 9, 76, 351, 515], [6, 145, 6, 526, 35, 1143, 87, 4, 16, 3, 41, 10, 5, 559], [196, 577, 360, 3, 47, 14, 26, 68, 2, 6, 77, 136, 4, 36, 1, 225], [267, 10, 1, 301, 51, 70, 64, 87, 6, 103, 32, 6, 92, 119, 56, 12], [121, 123, 2, 68, 109, 11, 75, 3, 493, 54, 18, 4, 184, 27], [1144, 2, 186, 1145, 10, 5, 24, 192, 135, 4, 54, 230], [5, 24, 496, 8, 1146, 10, 1, 315, 2, 4, 129, 43], [371, 149, 453, 13, 1, 25, 17, 235, 151, 248, 2, 9, 255], [110, 3, 574, 151, 575], [242, 1147, 1, 63, 1148, 10, 5, 24, 1149], [7, 9, 1, 128, 63, 497, 303, 110, 54, 2, 203], [276, 40, 15, 7, 52, 15, 32, 7, 17, 434, 286, 2, 4, 230], [7, 498, 3, 297, 1, 280, 1, 280, 51, 70, 64, 536, 51], [70, 546, 2, 437, 4, 37, 67, 42, 1150, 15, 136, 15, 578, 106], [578, 1151, 97, 6, 84, 358, 61, 6, 103, 11, 1152, 10, 5], [131, 13, 7, 9, 203, 18, 1, 126, 2, 1, 229, 8, 128, 231, 125], [2, 4, 27, 157, 1153, 75, 579, 40, 18, 61, 19, 39, 65], [1154, 3, 20, 208, 282, 113, 3, 84, 235, 1155, 14, 1156, 10], [1, 137, 2, 1, 117, 147, 23, 1, 222, 120, 2, 1, 24, 95], [17, 1157, 1158], [27, 76, 1, 63, 306, 11, 15, 4, 52, 579, 40, 2], [199, 31, 3, 14, 10, 133, 534, 196, 132, 580, 581, 44, 106, 12, 1159], [31, 68, 1160, 308, 28, 131, 2, 372, 42, 5, 229, 8, 125, 2, 5, 126], [1161, 81, 2, 11, 9, 30, 91, 518, 13, 4, 207, 80, 34, 98], [10, 1, 1162, 7, 1163, 3, 174, 187, 3, 561, 1, 1164, 7], [111, 139, 42, 18, 151, 1165, 4, 16, 3, 41, 15, 4, 207, 38], [277, 111, 37, 20, 46, 111, 1166, 31, 96, 6, 160, 19, 6, 198, 548, 253, 293], [151, 126, 2, 125, 13, 33, 32, 6, 97, 150, 61, 15, 4, 16, 28, 4], [163, 142, 5, 1167, 24, 156, 21, 1, 95, 8, 85, 9, 5, 335, 1168], [1169, 23, 1, 319, 1170, 63, 1171, 142, 7, 4, 52, 10, 174], [1172, 2, 252, 1173, 10, 117, 402, 1174, 4, 218, 558, 1], [1175, 580, 581, 2, 20, 220, 31, 8, 1, 156, 130, 4, 17, 73, 1], [126, 2, 125], [38, 234, 7, 360, 11, 16, 3, 41, 3, 20, 116, 582, 18], [5, 63, 6, 527, 87, 37, 20, 489, 42, 21, 582, 210, 2, 4], [75, 1176, 1, 179, 8, 89, 13, 59, 304, 424, 11, 86], [68, 1177, 2, 67, 232, 18, 88, 317, 255, 10, 5, 473], [537, 19, 6, 143, 118, 3, 56, 13, 1, 25, 173, 35, 67, 31, 122, 6, 171, 35], [60, 11, 52, 21, 13, 39, 198, 105, 87, 495, 10, 1, 156, 32, 7], [75, 1178, 216, 40, 47, 13], [71, 28, 66, 4, 17, 73, 14, 45, 72, 5, 1179, 24, 583, 23, 5, 120], [10, 1, 1180, 2, 21, 7, 15, 4, 17, 1181, 5, 126, 2, 227, 57, 331, 1182], [8, 445, 128, 231, 125, 4, 139, 43, 1, 126, 2, 5, 229, 8, 1, 125], [2, 9, 141, 116, 3, 472, 1, 583, 46, 14, 353, 209, 142, 5, 24]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "print((corpus))\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'and', 3: 'to', 4: 'she', 5: 'a', 6: 'i', 7: 'it', 8: 'of', 9: 'was', 10: 'in', 11: 'alice', 12: 'you', 13: 'that', 14: 'her', 15: 'as', 16: 'said', 17: 'had', 18: 'for', 19: 'but', 20: 'be', 21: 'on', 22: 'all', 23: 'with', 24: 'little', 25: 'mouse', 26: 'down', 27: 'very', 28: 'this', 29: 'not', 30: 'so', 31: 'out', 32: 'if', 33: 'is', 34: 'at', 35: 't', 36: 's', 37: 'll', 38: 'how', 39: 'they', 40: 'about', 41: 'herself', 42: 'me', 43: 'up', 44: 'what', 45: 'way', 46: 'when', 47: 'like', 48: 'one', 49: 'do', 50: 'no', 51: 'oh', 52: 'went', 53: 'thought', 54: 'again', 55: 'there', 56: 'see', 57: 'or', 58: 'could', 59: 'would', 60: 'think', 61: 'them', 62: 'know', 63: 'rabbit', 64: 'dear', 65: 'were', 66: 'time', 67: 'get', 68: 'here', 69: 'must', 70: 'my', 71: 'by', 72: 'into', 73: 'found', 74: 'such', 75: 'began', 76: 'soon', 77: 'm', 78: 'quite', 79: 'then', 80: 'off', 81: 'now', 82: 'go', 83: 'say', 84: 'have', 85: 'which', 86: 'come', 87: 'dinah', 88: 'your', 89: 'thing', 90: 'dodo', 91: 'much', 92: 'shall', 93: 'things', 94: 'long', 95: 'door', 96: 'who', 97: 'can', 98: 'once', 99: 'did', 100: 'over', 101: 'feet', 102: 'after', 103: 'wonder', 104: 'first', 105: 'let', 106: 'are', 107: 'cats', 108: 'round', 109: 'poor', 110: 'back', 111: 'he', 112: 'nothing', 113: 'seemed', 114: 'its', 115: 'never', 116: 'going', 117: 'great', 118: 'got', 119: 'ever', 120: 'table', 121: 'any', 122: 'only', 123: 'more', 124: 'cried', 125: 'gloves', 126: 'fan', 127: 'well', 128: 'white', 129: 'looked', 130: 'before', 131: 'moment', 132: 'why', 133: 'an', 134: 'eat', 135: 'however', 136: 'sure', 137: 'pool', 138: 'getting', 139: 'took', 140: 'either', 141: 'just', 142: 'upon', 143: 've', 144: 'through', 145: 'wish', 146: 'tell', 147: 'hall', 148: 'key', 149: 'half', 150: 'find', 151: 'his', 152: 'use', 153: 'large', 154: 'look', 155: 'too', 156: 'house', 157: 'good', 158: 'right', 159: 'words', 160: 'am', 161: 'might', 162: 'question', 163: 'came', 164: 'dry', 165: 'away', 166: 'been', 167: 'small', 168: 'high', 169: 'garden', 170: 'tears', 171: 'don', 172: 'won', 173: 'doesn', 174: 'without', 175: 'suddenly', 176: 'eyes', 177: 'some', 178: 'put', 179: 'sort', 180: 'their', 181: 'rather', 182: 'talking', 183: 'saying', 184: 'felt', 185: 'hand', 186: 'low', 187: 'trying', 188: 'golden', 189: 'head', 190: 'indeed', 191: 'will', 192: 'while', 193: 'same', 194: 'try', 195: 'beg', 196: 'tone', 197: 'our', 198: 'd', 199: 'called', 200: 'birds', 201: 'lory', 202: 'race', 203: 'looking', 204: 'course', 205: 'book', 206: 'made', 207: 'ran', 208: 'seen', 209: 'fell', 210: 'next', 211: 'tried', 212: 'make', 213: 'anything', 214: 'four', 215: 'nice', 216: 'people', 217: 'please', 218: 'should', 219: 'remember', 220: 'turned', 221: 'other', 222: 'glass', 223: 'than', 224: 'marked', 225: 'best', 226: 'sat', 227: 'two', 228: 'english', 229: 'pair', 230: 'heard', 231: 'kid', 232: 'ready', 233: 'voice', 234: 'queer', 235: 'changed', 236: 'swam', 237: 'talk', 238: 'offended', 239: 'we', 240: 'party', 241: 'prizes', 242: 'chapter', 243: 'hole', 244: 'tired', 245: 'having', 246: 'pictures', 247: 'own', 248: 'mind', 249: 'hot', 250: 'day', 251: 'pocket', 252: 'hurried', 253: 'take', 254: 'another', 255: 'coming', 256: 'from', 257: 'fall', 258: 'even', 259: 'near', 260: 'idea', 261: 'among', 262: 'didn', 263: 'ask', 264: 'air', 265: 'perhaps', 266: 'else', 267: 'cat', 268: 'afraid', 269: 'bats', 270: 'rate', 271: 'those', 272: 'really', 273: 'bottle', 274: 'children', 275: 'enough', 276: 'anxiously', 277: 'surprised', 278: 'speak', 279: 'hastily', 280: 'duchess', 281: 'kept', 282: 'everything', 283: 'mabel', 284: 'capital', 285: 'tail', 286: 'something', 287: 'william', 288: 'pardon', 289: 'crowded', 290: 'whole', 291: 'last', 292: 'old', 293: 'him', 294: 'thimble', 295: 'close', 296: 'hear', 297: 'itself', 298: 'late', 299: 'ought', 300: 'under', 301: 'world', 302: 'deep', 303: 'slowly', 304: 'happen', 305: 'dark', 306: 'noticed', 307: 'somebody', 308: 'home', 309: 'end', 310: 'many', 311: 'fallen', 312: 'several', 313: 'lessons', 314: 'still', 315: 'distance', 316: 'seem', 317: 'walk', 318: 'heads', 319: 'name', 320: 'ma', 321: 'fancy', 322: 're', 323: 'manage', 324: 'night', 325: 'mice', 326: 'answer', 327: 'passage', 328: 'sight', 329: 'behind', 330: 'every', 331: 'three', 332: 'alas', 333: 'inches', 334: 'along', 335: 'bright', 336: 'happened', 337: 'few', 338: 'drink', 339: 'hurry', 340: 'poison', 341: 'finger', 342: 'almost', 343: 'certain', 344: 'finished', 345: 'curious', 346: 'face', 347: 'candle', 348: 'generally', 349: 'remembered', 350: 'box', 351: 'left', 352: 'person', 353: 'eye', 354: 'lying', 355: 'cake', 356: 'life', 357: 'deal', 358: 'dropped', 359: 'times', 360: 'seems', 361: 'stay', 362: 'being', 363: 'sudden', 364: 'cause', 365: 'water', 366: 'o', 367: 'history', 368: 'trembling', 369: 'always', 370: 'dogs', 371: 'eagerly', 372: 'fetch', 373: 'us', 374: 'duck', 375: 'eaglet', 376: 'caucus', 377: 'tale', 378: 'replied', 379: 'turning', 380: 'sister', 381: 'bank', 382: 'considering', 383: 'feel', 384: 'sleepy', 385: 'stupid', 386: 'whether', 387: 'making', 388: 'worth', 389: 'trouble', 390: 'remarkable', 391: 'natural', 392: 'watch', 393: 'waistcoat', 394: 'started', 395: 'across', 396: 'falling', 397: 'filled', 398: 'cupboards', 399: 'shelves', 400: 'saw', 401: 'jar', 402: 'fear', 403: 'managed', 404: 'wouldn', 405: 'top', 406: 'likely', 407: 'miles', 408: 'aloud', 409: 'somewhere', 410: 'earth', 411: 'though', 412: 'opportunity', 413: 'knowledge', 414: 'listen', 415: 'yes', 416: 'latitude', 417: 'longitude', 418: 'funny', 419: 'glad', 420: 'new', 421: 'spoke', 422: 'girl', 423: 'asking', 424: 'miss', 425: 'catch', 426: 'bat', 427: 'sometimes', 428: 'matter', 429: 'begun', 430: 'walking', 431: 'thump', 432: 'bit', 433: 'hurt', 434: 'lost', 435: 'corner', 436: 'ears', 437: 'whiskers', 438: 'row', 439: 'roof', 440: 'doors', 441: 'side', 442: 'walked', 443: 'sadly', 444: 'middle', 445: 'tiny', 446: 'opened', 447: 'led', 448: 'larger', 449: 'shut', 450: 'telescope', 451: 'knew', 452: 'waiting', 453: 'hoping', 454: 'rules', 455: 'shutting', 456: 'certainly', 457: 'beautifully', 458: 'hold', 459: 'usually', 460: 'forgotten', 461: 'taste', 462: 'finding', 463: 'fact', 464: 'feeling', 465: 'size', 466: 'waited', 467: 'minutes', 468: 'altogether', 469: 'reach', 470: 'crying', 471: 'sharply', 472: 'leave', 473: 'minute', 474: 'gave', 475: 'severely', 476: 'against', 477: 'child', 478: 'fond', 479: 'makes', 480: 'grow', 481: 'happens', 482: 'holding', 483: 'growing', 484: 'curiouser', 485: 'forgot', 486: 'far', 487: 'dears', 488: 'give', 489: 'sending', 490: 'foot', 491: 'nonsense', 492: 'nine', 493: 'cry', 494: 'yourself', 495: 'stop', 496: 'pattering', 497: 'trotting', 498: 'muttering', 499: 'help', 500: 'sir', 501: 'hard', 502: 'morning', 503: 'ah', 504: 'age', 505: 'hair', 506: 'ringlets', 507: 'mine', 508: 'sorts', 509: 'puzzling', 510: 'used', 511: 'paris', 512: 'rome', 513: 'doth', 514: 'hands', 515: 'alone', 516: 'done', 517: 'shrinking', 518: 'frightened', 519: 'change', 520: 'bad', 521: 'slipped', 522: 'salt', 523: 'sea', 524: 'case', 525: 'railway', 526: 'hadn', 527: 'suppose', 528: 'swimming', 529: 'speaking', 530: 'understand', 531: 'french', 532: 'conqueror', 533: 'lesson', 534: 'angry', 535: 'show', 536: 'paws', 537: 'nurse', 538: 'catching', 539: 'subject', 540: 'sit', 541: 'says', 542: 'useful', 543: 'shore', 544: 'hate', 545: 'animals', 546: 'fur', 547: 'wet', 548: 'better', 549: 'ring', 550: 'silence', 551: 'wanted', 552: 'edwin', 553: 'morcar', 554: 'earls', 555: 'mercia', 556: 'northumbria', 557: 'advisable', 558: 'meet', 559: 'melancholy', 560: 'solemnly', 561: 'explain', 562: 'running', 563: 'liked', 564: 'has', 565: 'chorus', 566: 'comfits', 567: 'speech', 568: 'caused', 569: 'sad', 570: 'fury', 571: 'trial', 572: 'jury', 573: 'judge', 574: 'finish', 575: 'story', 576: 'crab', 577: 'nobody', 578: 'ferrets', 579: 'hunting', 580: 'mary', 581: 'ann', 582: 'messages', 583: 'room', 584: 'beginning', 585: 'sitting', 586: 'twice', 587: 'peeped', 588: 'reading', 589: 'conversations', 590: 'pleasure', 591: 'daisy', 592: 'chain', 593: 'picking', 594: 'daisies', 595: 'pink', 596: 'nor', 597: 'afterwards', 598: 'occurred', 599: 'wondered', 600: 'actually', 601: 'flashed', 602: 'burning', 603: 'curiosity', 604: 'field', 605: 'fortunately', 606: 'pop', 607: 'hedge', 608: 'straight', 609: 'tunnel', 610: 'dipped', 611: 'stopping', 612: 'plenty', 613: 'sides', 614: 'maps', 615: 'hung', 616: 'pegs', 617: 'passed', 618: 'labelled', 619: 'orange', 620: 'marmalade', 621: 'disappointment', 622: 'empty', 623: 'drop', 624: 'killing', 625: 'past', 626: 'tumbling', 627: 'stairs', 628: 'brave', 629: 'true', 630: 'centre', 631: 'thousand', 632: 'learnt', 633: 'schoolroom', 634: 'showing', 635: 'practice', 636: 'grand', 637: 'presently', 638: 'downward', 639: 'antipathies', 640: 'listening', 641: 'sound', 642: 'word', 643: 'country', 644: 'zealand', 645: 'australia', 646: 'curtsey', 647: 'curtseying', 648: 'ignorant', 649: 'written', 650: 'hope', 651: 'saucer', 652: 'milk', 653: 'tea', 654: 'dreamy', 655: 'couldn', 656: 'dozing', 657: 'dream', 658: 'earnestly', 659: 'truth', 660: 'heap', 661: 'sticks', 662: 'leaves', 663: 'jumped', 664: 'overhead', 665: 'hurrying', 666: 'wind', 667: 'longer', 668: 'lit', 669: 'lamps', 670: 'hanging', 671: 'locked', 672: 'wondering', 673: 'legged', 674: 'solid', 675: 'except', 676: 'belong', 677: 'locks', 678: 'open', 679: 'second', 680: 'curtain', 681: 'fifteen', 682: 'lock', 683: 'delight', 684: 'fitted', 685: 'rat', 686: 'knelt', 687: 'loveliest', 688: 'longed', 689: 'wander', 690: 'beds', 691: 'flowers', 692: 'cool', 693: 'fountains', 694: 'doorway', 695: 'shoulders', 696: 'begin', 697: 'lately', 698: 'telescopes', 699: 'neck', 700: 'paper', 701: 'label', 702: 'printed', 703: 'wise', 704: 'read', 705: 'histories', 706: 'burnt', 707: 'eaten', 708: 'wild', 709: 'beasts', 710: 'unpleasant', 711: 'because', 712: 'simple', 713: 'friends', 714: 'taught', 715: 'red', 716: 'poker', 717: 'burn', 718: 'cut', 719: 'deeply', 720: 'knife', 721: 'bleeds', 722: 'disagree', 723: 'sooner', 724: 'later', 725: 'ventured', 726: 'mixed', 727: 'flavour', 728: 'cherry', 729: 'tart', 730: 'custard', 731: 'pine', 732: 'apple', 733: 'roast', 734: 'turkey', 735: 'toffee', 736: 'buttered', 737: 'toast', 738: 'ten', 739: 'brightened', 740: 'lovely', 741: 'shrink', 742: 'further', 743: 'nervous', 744: 'flame', 745: 'blown', 746: 'decided', 747: 'possibly', 748: 'plainly', 749: 'climb', 750: 'legs', 751: 'slippery', 752: 'advise', 753: 'advice', 754: 'seldom', 755: 'followed', 756: 'scolded', 757: 'bring', 758: 'cheated', 759: 'game', 760: 'croquet', 761: 'playing', 762: 'pretending', 763: 'pretend', 764: 'hardly', 765: 'respectable', 766: 'currants', 767: 'smaller', 768: 'creep', 769: 'care', 770: 'ate', 771: 'remained', 772: 'eats', 773: 'expecting', 774: 'dull', 775: 'common', 776: 'set', 777: 'work', 778: 'ii', 779: 'opening', 780: 'largest', 781: 'bye', 782: 'shoes', 783: 'stockings', 784: 'shan', 785: 'able', 786: 'myself', 787: 'kind', 788: 'want', 789: 'boots', 790: 'christmas', 791: 'planning', 792: 'carrier', 793: 'presents', 794: 'odd', 795: 'directions', 796: 'esq', 797: 'hearthrug', 798: 'fender', 799: 'love', 800: 'struck', 801: 'hopeless', 802: 'ashamed', 803: 'shedding', 804: 'gallons', 805: 'until', 806: 'reaching', 807: 'dried', 808: 'returning', 809: 'splendidly', 810: 'dressed', 811: 'himself', 812: 'savage', 813: 'desperate', 814: 'timid', 815: 'violently', 816: 'skurried', 817: 'darkness', 818: 'fanning', 819: 'yesterday', 820: 'usual', 821: 'different', 822: 'puzzle', 823: 'thinking', 824: 'ada', 825: 'goes', 826: 'knows', 827: 'besides', 828: 'five', 829: 'twelve', 830: 'six', 831: 'thirteen', 832: 'seven', 833: 'twenty', 834: 'multiplication', 835: 'signify', 836: 'geography', 837: 'london', 838: 'wrong', 839: 'crossed', 840: 'lap', 841: 'repeat', 842: 'sounded', 843: 'hoarse', 844: 'strange', 845: 'crocodile', 846: 'improve', 847: 'shining', 848: 'pour', 849: 'waters', 850: 'nile', 851: 'scale', 852: 'cheerfully', 853: 'grin', 854: 'neatly', 855: 'spread', 856: 'claws', 857: 'welcome', 858: 'fishes', 859: 'gently', 860: 'smiling', 861: 'jaws', 862: 'live', 863: 'poky', 864: 'toys', 865: 'play', 866: 'learn', 867: 'putting', 868: 'till', 869: 'burst', 870: 'measure', 871: 'nearly', 872: 'guess', 873: 'rapidly', 874: 'avoid', 875: 'narrow', 876: 'escape', 877: 'existence', 878: 'speed', 879: 'worse', 880: 'declare', 881: 'these', 882: 'splash', 883: 'chin', 884: 'somehow', 885: 'seaside', 886: 'general', 887: 'conclusion', 888: 'wherever', 889: 'coast', 890: 'number', 891: 'bathing', 892: 'machines', 893: 'digging', 894: 'sand', 895: 'wooden', 896: 'spades', 897: 'lodging', 898: 'houses', 899: 'station', 900: 'wept', 901: 'punished', 902: 'drowned', 903: 'splashing', 904: 'nearer', 905: 'walrus', 906: 'hippopotamus', 907: 'harm', 908: 'brother', 909: 'latin', 910: 'grammar', 911: 'inquisitively', 912: 'wink', 913: 'daresay', 914: 'clear', 915: 'notion', 916: 'ago', 917: 'ou', 918: 'est', 919: 'chatte', 920: 'sentence', 921: 'leap', 922: 'quiver', 923: 'fright', 924: 'animal', 925: 'feelings', 926: 'shrill', 927: 'passionate', 928: 'soothing', 929: 'yet', 930: 'quiet', 931: 'lazily', 932: 'sits', 933: 'purring', 934: 'nicely', 935: 'fire', 936: 'licking', 937: 'washing', 938: 'soft', 939: 'bristling', 940: 'family', 941: 'hated', 942: 'nasty', 943: 'vulgar', 944: 'conversation', 945: 'dog', 946: 'eyed', 947: 'terrier', 948: 'curly', 949: 'brown', 950: 'throw', 951: 'dinner', 952: 'belongs', 953: 'farmer', 954: 'hundred', 955: 'pounds', 956: 'kills', 957: 'rats', 958: 'sorrowful', 959: 'commotion', 960: 'softly', 961: 'pale', 962: 'passion', 963: 'creatures', 964: 'iii', 965: 'assembled', 966: 'draggled', 967: 'feathers', 968: 'clinging', 969: 'dripping', 970: 'cross', 971: 'uncomfortable', 972: 'consultation', 973: 'familiarly', 974: 'known', 975: 'argument', 976: 'sulky', 977: 'older', 978: 'allow', 979: 'knowing', 980: 'positively', 981: 'refused', 982: 'authority', 983: 'fixed', 984: 'cold', 985: 'ahem', 986: 'important', 987: 'driest', 988: 'whose', 989: 'favoured', 990: 'pope', 991: 'submitted', 992: 'leaders', 993: 'accustomed', 994: 'usurpation', 995: 'conquest', 996: 'ugh', 997: 'shiver', 998: 'frowning', 999: 'politely', 1000: 'proceed', 1001: 'declared', 1002: 'stigand', 1003: 'patriotic', 1004: 'archbishop', 1005: 'canterbury', 1006: 'crossly', 1007: 'means', 1008: 'frog', 1009: 'worm', 1010: 'notice', 1011: 'hurriedly', 1012: 'edgar', 1013: 'atheling', 1014: 'offer', 1015: 'crown', 1016: 'conduct', 1017: 'moderate', 1018: 'insolence', 1019: 'normans', 1020: 'continued', 1021: 'rising', 1022: 'move', 1023: 'meeting', 1024: 'adjourn', 1025: 'immediate', 1026: 'adoption', 1027: 'energetic', 1028: 'meaning', 1029: 'believe', 1030: 'bent', 1031: 'hide', 1032: 'smile', 1033: 'paused', 1034: 'inclined', 1035: 'winter', 1036: 'circle', 1037: 'exact', 1038: 'shape', 1039: 'placed', 1040: 'easy', 1041: 'hour', 1042: 'panting', 1043: 'pressed', 1044: 'forehead', 1045: 'position', 1046: 'shakespeare', 1047: 'rest', 1048: 'everybody', 1049: 'voices', 1050: 'asked', 1051: 'pointing', 1052: 'calling', 1053: 'confused', 1054: 'despair', 1055: 'pulled', 1056: 'luckily', 1057: 'handed', 1058: 'exactly', 1059: 'piece', 1060: 'prize', 1061: 'gravely', 1062: 'presented', 1063: 'acceptance', 1064: 'elegant', 1065: 'short', 1066: 'cheered', 1067: 'absurd', 1068: 'grave', 1069: 'dare', 1070: 'laugh', 1071: 'simply', 1072: 'bowed', 1073: 'solemn', 1074: 'noise', 1075: 'confusion', 1076: 'complained', 1077: 'theirs', 1078: 'ones', 1079: 'choked', 1080: 'patted', 1081: 'begged', 1082: 'promised', 1083: 'c', 1084: 'added', 1085: 'whisper', 1086: 'call', 1087: 'met', 1088: 'both', 1089: 'law', 1090: 'prosecute', 1091: 'denial', 1092: 'cur', 1093: 'wasting', 1094: 'breath', 1095: 'cunning', 1096: 'condemn', 1097: 'death', 1098: 'attending', 1099: 'humbly', 1100: 'fifth', 1101: 'bend', 1102: 'angrily', 1103: 'knot', 1104: 'undo', 1105: 'insult', 1106: 'mean', 1107: 'pleaded', 1108: 'easily', 1109: 'growled', 1110: 'reply', 1111: 'others', 1112: 'joined', 1113: 'shook', 1114: 'impatiently', 1115: 'quicker', 1116: 'pity', 1117: 'sighed', 1118: 'daughter', 1119: 'lose', 1120: 'temper', 1121: 'tongue', 1122: 'young', 1123: 'snappishly', 1124: 'patience', 1125: 'oyster', 1126: 'addressing', 1127: 'particular', 1128: 'venture', 1129: 'pet', 1130: 'bird', 1131: 'sensation', 1132: 'magpie', 1133: 'wrapping', 1134: 'carefully', 1135: 'remarking', 1136: 'suit', 1137: 'throat', 1138: 'canary', 1139: 'bed', 1140: 'various', 1141: 'pretexts', 1142: 'moved', 1143: 'mentioned', 1144: 'lonely', 1145: 'spirited', 1146: 'footsteps', 1147: 'iv', 1148: 'sends', 1149: 'bill', 1150: 'executed', 1151: 'where', 1152: 'guessed', 1153: 'naturedly', 1154: 'nowhere', 1155: 'since', 1156: 'swim', 1157: 'vanished', 1158: 'completely', 1159: 'doing', 1160: 'run', 1161: 'quick', 1162: 'direction', 1163: 'pointed', 1164: 'mistake', 1165: 'housemaid', 1166: 'finds', 1167: 'neat', 1168: 'brass', 1169: 'plate', 1170: 'w', 1171: 'engraved', 1172: 'knocking', 1173: 'upstairs', 1174: 'lest', 1175: 'real', 1176: 'fancying', 1177: 'directly', 1178: 'ordering', 1179: 'tidy', 1180: 'window', 1181: 'hoped', 1182: 'pairs'}\n"
     ]
    }
   ],
   "source": [
    "# create inverted index to help recover the words from indexes\n",
    "inverted_index = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    inverted_index[i] = word\n",
    "    \n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "3\n",
      "[[0.33333333 0.66666667]]\n",
      "int64\n",
      "[[1 2]]\n",
      "7\n",
      "[[0.42857143 0.57142857]]\n",
      "int64\n",
      "[[3 4]]\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# helper snippet\n",
    "################################\n",
    "\n",
    "m = np.matrix([[1, 2], [3, 4]])\n",
    "print(m)\n",
    "(rows, columns) = m.shape\n",
    "for rowIdx in range(rows):\n",
    "    s = m[rowIdx]\n",
    "    total = s.sum()\n",
    "    print(s.sum())\n",
    "    print(np.divide(s, total))\n",
    "    print(s.dtype)\n",
    "    print(s.flatten())\n",
    "    \n",
    "    #for value in m[rowIdx, :]:\n",
    "    #    print(value)\n",
    "    #print(m[rowIdx, :])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[797]\n",
      "[1090]\n",
      "[1093]\n",
      "[1094]\n",
      "[1095]\n",
      "[1096]\n",
      "[1097]\n",
      "{'the': 1, 'and': 2, 'to': 3, 'she': 4, 'a': 5, 'i': 6, 'it': 7, 'of': 8, 'was': 9, 'in': 10, 'alice': 11, 'you': 12, 'that': 13, 'her': 14, 'as': 15, 'said': 16, 'had': 17, 'for': 18, 'but': 19, 'be': 20, 'on': 21, 'all': 22, 'with': 23, 'little': 24, 'mouse': 25, 'down': 26, 'very': 27, 'this': 28, 'not': 29, 'so': 30, 'out': 31, 'if': 32, 'is': 33, 'at': 34, 't': 35, 's': 36, 'll': 37, 'how': 38, 'they': 39, 'about': 40, 'herself': 41, 'me': 42, 'up': 43, 'what': 44, 'way': 45, 'when': 46, 'like': 47, 'one': 48, 'do': 49, 'no': 50, 'oh': 51, 'went': 52, 'thought': 53, 'again': 54, 'there': 55, 'see': 56, 'or': 57, 'could': 58, 'would': 59, 'think': 60, 'them': 61, 'know': 62, 'rabbit': 63, 'dear': 64, 'were': 65, 'time': 66, 'get': 67, 'here': 68, 'must': 69, 'my': 70, 'by': 71, 'into': 72, 'found': 73, 'such': 74, 'began': 75, 'soon': 76, 'm': 77, 'quite': 78, 'then': 79, 'off': 80, 'now': 81, 'go': 82, 'say': 83, 'have': 84, 'which': 85, 'come': 86, 'dinah': 87, 'your': 88, 'thing': 89, 'dodo': 90, 'much': 91, 'shall': 92, 'things': 93, 'long': 94, 'door': 95, 'who': 96, 'can': 97, 'once': 98, 'did': 99, 'over': 100, 'feet': 101, 'after': 102, 'wonder': 103, 'first': 104, 'let': 105, 'are': 106, 'cats': 107, 'round': 108, 'poor': 109, 'back': 110, 'he': 111, 'nothing': 112, 'seemed': 113, 'its': 114, 'never': 115, 'going': 116, 'great': 117, 'got': 118, 'ever': 119, 'table': 120, 'any': 121, 'only': 122, 'more': 123, 'cried': 124, 'gloves': 125, 'fan': 126, 'well': 127, 'white': 128, 'looked': 129, 'before': 130, 'moment': 131, 'why': 132, 'an': 133, 'eat': 134, 'however': 135, 'sure': 136, 'pool': 137, 'getting': 138, 'took': 139, 'either': 140, 'just': 141, 'upon': 142, 've': 143, 'through': 144, 'wish': 145, 'tell': 146, 'hall': 147, 'key': 148, 'half': 149, 'find': 150, 'his': 151, 'use': 152, 'large': 153, 'look': 154, 'too': 155, 'house': 156, 'good': 157, 'right': 158, 'words': 159, 'am': 160, 'might': 161, 'question': 162, 'came': 163, 'dry': 164, 'away': 165, 'been': 166, 'small': 167, 'high': 168, 'garden': 169, 'tears': 170, 'don': 171, 'won': 172, 'doesn': 173, 'without': 174, 'suddenly': 175, 'eyes': 176, 'some': 177, 'put': 178, 'sort': 179, 'their': 180, 'rather': 181, 'talking': 182, 'saying': 183, 'felt': 184, 'hand': 185, 'low': 186, 'trying': 187, 'golden': 188, 'head': 189, 'indeed': 190, 'will': 191, 'while': 192, 'same': 193, 'try': 194, 'beg': 195, 'tone': 196, 'our': 197, 'd': 198, 'called': 199, 'birds': 200, 'lory': 201, 'race': 202, 'looking': 203, 'course': 204, 'book': 205, 'made': 206, 'ran': 207, 'seen': 208, 'fell': 209, 'next': 210, 'tried': 211, 'make': 212, 'anything': 213, 'four': 214, 'nice': 215, 'people': 216, 'please': 217, 'should': 218, 'remember': 219, 'turned': 220, 'other': 221, 'glass': 222, 'than': 223, 'marked': 224, 'best': 225, 'sat': 226, 'two': 227, 'english': 228, 'pair': 229, 'heard': 230, 'kid': 231, 'ready': 232, 'voice': 233, 'queer': 234, 'changed': 235, 'swam': 236, 'talk': 237, 'offended': 238, 'we': 239, 'party': 240, 'prizes': 241, 'chapter': 242, 'hole': 243, 'tired': 244, 'having': 245, 'pictures': 246, 'own': 247, 'mind': 248, 'hot': 249, 'day': 250, 'pocket': 251, 'hurried': 252, 'take': 253, 'another': 254, 'coming': 255, 'from': 256, 'fall': 257, 'even': 258, 'near': 259, 'idea': 260, 'among': 261, 'didn': 262, 'ask': 263, 'air': 264, 'perhaps': 265, 'else': 266, 'cat': 267, 'afraid': 268, 'bats': 269, 'rate': 270, 'those': 271, 'really': 272, 'bottle': 273, 'children': 274, 'enough': 275, 'anxiously': 276, 'surprised': 277, 'speak': 278, 'hastily': 279, 'duchess': 280, 'kept': 281, 'everything': 282, 'mabel': 283, 'capital': 284, 'tail': 285, 'something': 286, 'william': 287, 'pardon': 288, 'crowded': 289, 'whole': 290, 'last': 291, 'old': 292, 'him': 293, 'thimble': 294, 'close': 295, 'hear': 296, 'itself': 297, 'late': 298, 'ought': 299, 'under': 300, 'world': 301, 'deep': 302, 'slowly': 303, 'happen': 304, 'dark': 305, 'noticed': 306, 'somebody': 307, 'home': 308, 'end': 309, 'many': 310, 'fallen': 311, 'several': 312, 'lessons': 313, 'still': 314, 'distance': 315, 'seem': 316, 'walk': 317, 'heads': 318, 'name': 319, 'ma': 320, 'fancy': 321, 're': 322, 'manage': 323, 'night': 324, 'mice': 325, 'answer': 326, 'passage': 327, 'sight': 328, 'behind': 329, 'every': 330, 'three': 331, 'alas': 332, 'inches': 333, 'along': 334, 'bright': 335, 'happened': 336, 'few': 337, 'drink': 338, 'hurry': 339, 'poison': 340, 'finger': 341, 'almost': 342, 'certain': 343, 'finished': 344, 'curious': 345, 'face': 346, 'candle': 347, 'generally': 348, 'remembered': 349, 'box': 350, 'left': 351, 'person': 352, 'eye': 353, 'lying': 354, 'cake': 355, 'life': 356, 'deal': 357, 'dropped': 358, 'times': 359, 'seems': 360, 'stay': 361, 'being': 362, 'sudden': 363, 'cause': 364, 'water': 365, 'o': 366, 'history': 367, 'trembling': 368, 'always': 369, 'dogs': 370, 'eagerly': 371, 'fetch': 372, 'us': 373, 'duck': 374, 'eaglet': 375, 'caucus': 376, 'tale': 377, 'replied': 378, 'turning': 379, 'sister': 380, 'bank': 381, 'considering': 382, 'feel': 383, 'sleepy': 384, 'stupid': 385, 'whether': 386, 'making': 387, 'worth': 388, 'trouble': 389, 'remarkable': 390, 'natural': 391, 'watch': 392, 'waistcoat': 393, 'started': 394, 'across': 395, 'falling': 396, 'filled': 397, 'cupboards': 398, 'shelves': 399, 'saw': 400, 'jar': 401, 'fear': 402, 'managed': 403, 'wouldn': 404, 'top': 405, 'likely': 406, 'miles': 407, 'aloud': 408, 'somewhere': 409, 'earth': 410, 'though': 411, 'opportunity': 412, 'knowledge': 413, 'listen': 414, 'yes': 415, 'latitude': 416, 'longitude': 417, 'funny': 418, 'glad': 419, 'new': 420, 'spoke': 421, 'girl': 422, 'asking': 423, 'miss': 424, 'catch': 425, 'bat': 426, 'sometimes': 427, 'matter': 428, 'begun': 429, 'walking': 430, 'thump': 431, 'bit': 432, 'hurt': 433, 'lost': 434, 'corner': 435, 'ears': 436, 'whiskers': 437, 'row': 438, 'roof': 439, 'doors': 440, 'side': 441, 'walked': 442, 'sadly': 443, 'middle': 444, 'tiny': 445, 'opened': 446, 'led': 447, 'larger': 448, 'shut': 449, 'telescope': 450, 'knew': 451, 'waiting': 452, 'hoping': 453, 'rules': 454, 'shutting': 455, 'certainly': 456, 'beautifully': 457, 'hold': 458, 'usually': 459, 'forgotten': 460, 'taste': 461, 'finding': 462, 'fact': 463, 'feeling': 464, 'size': 465, 'waited': 466, 'minutes': 467, 'altogether': 468, 'reach': 469, 'crying': 470, 'sharply': 471, 'leave': 472, 'minute': 473, 'gave': 474, 'severely': 475, 'against': 476, 'child': 477, 'fond': 478, 'makes': 479, 'grow': 480, 'happens': 481, 'holding': 482, 'growing': 483, 'curiouser': 484, 'forgot': 485, 'far': 486, 'dears': 487, 'give': 488, 'sending': 489, 'foot': 490, 'nonsense': 491, 'nine': 492, 'cry': 493, 'yourself': 494, 'stop': 495, 'pattering': 496, 'trotting': 497, 'muttering': 498, 'help': 499, 'sir': 500, 'hard': 501, 'morning': 502, 'ah': 503, 'age': 504, 'hair': 505, 'ringlets': 506, 'mine': 507, 'sorts': 508, 'puzzling': 509, 'used': 510, 'paris': 511, 'rome': 512, 'doth': 513, 'hands': 514, 'alone': 515, 'done': 516, 'shrinking': 517, 'frightened': 518, 'change': 519, 'bad': 520, 'slipped': 521, 'salt': 522, 'sea': 523, 'case': 524, 'railway': 525, 'hadn': 526, 'suppose': 527, 'swimming': 528, 'speaking': 529, 'understand': 530, 'french': 531, 'conqueror': 532, 'lesson': 533, 'angry': 534, 'show': 535, 'paws': 536, 'nurse': 537, 'catching': 538, 'subject': 539, 'sit': 540, 'says': 541, 'useful': 542, 'shore': 543, 'hate': 544, 'animals': 545, 'fur': 546, 'wet': 547, 'better': 548, 'ring': 549, 'silence': 550, 'wanted': 551, 'edwin': 552, 'morcar': 553, 'earls': 554, 'mercia': 555, 'northumbria': 556, 'advisable': 557, 'meet': 558, 'melancholy': 559, 'solemnly': 560, 'explain': 561, 'running': 562, 'liked': 563, 'has': 564, 'chorus': 565, 'comfits': 566, 'speech': 567, 'caused': 568, 'sad': 569, 'fury': 570, 'trial': 571, 'jury': 572, 'judge': 573, 'finish': 574, 'story': 575, 'crab': 576, 'nobody': 577, 'ferrets': 578, 'hunting': 579, 'mary': 580, 'ann': 581, 'messages': 582, 'room': 583, 'beginning': 584, 'sitting': 585, 'twice': 586, 'peeped': 587, 'reading': 588, 'conversations': 589, 'pleasure': 590, 'daisy': 591, 'chain': 592, 'picking': 593, 'daisies': 594, 'pink': 595, 'nor': 596, 'afterwards': 597, 'occurred': 598, 'wondered': 599, 'actually': 600, 'flashed': 601, 'burning': 602, 'curiosity': 603, 'field': 604, 'fortunately': 605, 'pop': 606, 'hedge': 607, 'straight': 608, 'tunnel': 609, 'dipped': 610, 'stopping': 611, 'plenty': 612, 'sides': 613, 'maps': 614, 'hung': 615, 'pegs': 616, 'passed': 617, 'labelled': 618, 'orange': 619, 'marmalade': 620, 'disappointment': 621, 'empty': 622, 'drop': 623, 'killing': 624, 'past': 625, 'tumbling': 626, 'stairs': 627, 'brave': 628, 'true': 629, 'centre': 630, 'thousand': 631, 'learnt': 632, 'schoolroom': 633, 'showing': 634, 'practice': 635, 'grand': 636, 'presently': 637, 'downward': 638, 'antipathies': 639, 'listening': 640, 'sound': 641, 'word': 642, 'country': 643, 'zealand': 644, 'australia': 645, 'curtsey': 646, 'curtseying': 647, 'ignorant': 648, 'written': 649, 'hope': 650, 'saucer': 651, 'milk': 652, 'tea': 653, 'dreamy': 654, 'couldn': 655, 'dozing': 656, 'dream': 657, 'earnestly': 658, 'truth': 659, 'heap': 660, 'sticks': 661, 'leaves': 662, 'jumped': 663, 'overhead': 664, 'hurrying': 665, 'wind': 666, 'longer': 667, 'lit': 668, 'lamps': 669, 'hanging': 670, 'locked': 671, 'wondering': 672, 'legged': 673, 'solid': 674, 'except': 675, 'belong': 676, 'locks': 677, 'open': 678, 'second': 679, 'curtain': 680, 'fifteen': 681, 'lock': 682, 'delight': 683, 'fitted': 684, 'rat': 685, 'knelt': 686, 'loveliest': 687, 'longed': 688, 'wander': 689, 'beds': 690, 'flowers': 691, 'cool': 692, 'fountains': 693, 'doorway': 694, 'shoulders': 695, 'begin': 696, 'lately': 697, 'telescopes': 698, 'neck': 699, 'paper': 700, 'label': 701, 'printed': 702, 'wise': 703, 'read': 704, 'histories': 705, 'burnt': 706, 'eaten': 707, 'wild': 708, 'beasts': 709, 'unpleasant': 710, 'because': 711, 'simple': 712, 'friends': 713, 'taught': 714, 'red': 715, 'poker': 716, 'burn': 717, 'cut': 718, 'deeply': 719, 'knife': 720, 'bleeds': 721, 'disagree': 722, 'sooner': 723, 'later': 724, 'ventured': 725, 'mixed': 726, 'flavour': 727, 'cherry': 728, 'tart': 729, 'custard': 730, 'pine': 731, 'apple': 732, 'roast': 733, 'turkey': 734, 'toffee': 735, 'buttered': 736, 'toast': 737, 'ten': 738, 'brightened': 739, 'lovely': 740, 'shrink': 741, 'further': 742, 'nervous': 743, 'flame': 744, 'blown': 745, 'decided': 746, 'possibly': 747, 'plainly': 748, 'climb': 749, 'legs': 750, 'slippery': 751, 'advise': 752, 'advice': 753, 'seldom': 754, 'followed': 755, 'scolded': 756, 'bring': 757, 'cheated': 758, 'game': 759, 'croquet': 760, 'playing': 761, 'pretending': 762, 'pretend': 763, 'hardly': 764, 'respectable': 765, 'currants': 766, 'smaller': 767, 'creep': 768, 'care': 769, 'ate': 770, 'remained': 771, 'eats': 772, 'expecting': 773, 'dull': 774, 'common': 775, 'set': 776, 'work': 777, 'ii': 778, 'opening': 779, 'largest': 780, 'bye': 781, 'shoes': 782, 'stockings': 783, 'shan': 784, 'able': 785, 'myself': 786, 'kind': 787, 'want': 788, 'boots': 789, 'christmas': 790, 'planning': 791, 'carrier': 792, 'presents': 793, 'odd': 794, 'directions': 795, 'esq': 796, 'hearthrug': 797, 'fender': 798, 'love': 799, 'struck': 800, 'hopeless': 801, 'ashamed': 802, 'shedding': 803, 'gallons': 804, 'until': 805, 'reaching': 806, 'dried': 807, 'returning': 808, 'splendidly': 809, 'dressed': 810, 'himself': 811, 'savage': 812, 'desperate': 813, 'timid': 814, 'violently': 815, 'skurried': 816, 'darkness': 817, 'fanning': 818, 'yesterday': 819, 'usual': 820, 'different': 821, 'puzzle': 822, 'thinking': 823, 'ada': 824, 'goes': 825, 'knows': 826, 'besides': 827, 'five': 828, 'twelve': 829, 'six': 830, 'thirteen': 831, 'seven': 832, 'twenty': 833, 'multiplication': 834, 'signify': 835, 'geography': 836, 'london': 837, 'wrong': 838, 'crossed': 839, 'lap': 840, 'repeat': 841, 'sounded': 842, 'hoarse': 843, 'strange': 844, 'crocodile': 845, 'improve': 846, 'shining': 847, 'pour': 848, 'waters': 849, 'nile': 850, 'scale': 851, 'cheerfully': 852, 'grin': 853, 'neatly': 854, 'spread': 855, 'claws': 856, 'welcome': 857, 'fishes': 858, 'gently': 859, 'smiling': 860, 'jaws': 861, 'live': 862, 'poky': 863, 'toys': 864, 'play': 865, 'learn': 866, 'putting': 867, 'till': 868, 'burst': 869, 'measure': 870, 'nearly': 871, 'guess': 872, 'rapidly': 873, 'avoid': 874, 'narrow': 875, 'escape': 876, 'existence': 877, 'speed': 878, 'worse': 879, 'declare': 880, 'these': 881, 'splash': 882, 'chin': 883, 'somehow': 884, 'seaside': 885, 'general': 886, 'conclusion': 887, 'wherever': 888, 'coast': 889, 'number': 890, 'bathing': 891, 'machines': 892, 'digging': 893, 'sand': 894, 'wooden': 895, 'spades': 896, 'lodging': 897, 'houses': 898, 'station': 899, 'wept': 900, 'punished': 901, 'drowned': 902, 'splashing': 903, 'nearer': 904, 'walrus': 905, 'hippopotamus': 906, 'harm': 907, 'brother': 908, 'latin': 909, 'grammar': 910, 'inquisitively': 911, 'wink': 912, 'daresay': 913, 'clear': 914, 'notion': 915, 'ago': 916, 'ou': 917, 'est': 918, 'chatte': 919, 'sentence': 920, 'leap': 921, 'quiver': 922, 'fright': 923, 'animal': 924, 'feelings': 925, 'shrill': 926, 'passionate': 927, 'soothing': 928, 'yet': 929, 'quiet': 930, 'lazily': 931, 'sits': 932, 'purring': 933, 'nicely': 934, 'fire': 935, 'licking': 936, 'washing': 937, 'soft': 938, 'bristling': 939, 'family': 940, 'hated': 941, 'nasty': 942, 'vulgar': 943, 'conversation': 944, 'dog': 945, 'eyed': 946, 'terrier': 947, 'curly': 948, 'brown': 949, 'throw': 950, 'dinner': 951, 'belongs': 952, 'farmer': 953, 'hundred': 954, 'pounds': 955, 'kills': 956, 'rats': 957, 'sorrowful': 958, 'commotion': 959, 'softly': 960, 'pale': 961, 'passion': 962, 'creatures': 963, 'iii': 964, 'assembled': 965, 'draggled': 966, 'feathers': 967, 'clinging': 968, 'dripping': 969, 'cross': 970, 'uncomfortable': 971, 'consultation': 972, 'familiarly': 973, 'known': 974, 'argument': 975, 'sulky': 976, 'older': 977, 'allow': 978, 'knowing': 979, 'positively': 980, 'refused': 981, 'authority': 982, 'fixed': 983, 'cold': 984, 'ahem': 985, 'important': 986, 'driest': 987, 'whose': 988, 'favoured': 989, 'pope': 990, 'submitted': 991, 'leaders': 992, 'accustomed': 993, 'usurpation': 994, 'conquest': 995, 'ugh': 996, 'shiver': 997, 'frowning': 998, 'politely': 999, 'proceed': 1000, 'declared': 1001, 'stigand': 1002, 'patriotic': 1003, 'archbishop': 1004, 'canterbury': 1005, 'crossly': 1006, 'means': 1007, 'frog': 1008, 'worm': 1009, 'notice': 1010, 'hurriedly': 1011, 'edgar': 1012, 'atheling': 1013, 'offer': 1014, 'crown': 1015, 'conduct': 1016, 'moderate': 1017, 'insolence': 1018, 'normans': 1019, 'continued': 1020, 'rising': 1021, 'move': 1022, 'meeting': 1023, 'adjourn': 1024, 'immediate': 1025, 'adoption': 1026, 'energetic': 1027, 'meaning': 1028, 'believe': 1029, 'bent': 1030, 'hide': 1031, 'smile': 1032, 'paused': 1033, 'inclined': 1034, 'winter': 1035, 'circle': 1036, 'exact': 1037, 'shape': 1038, 'placed': 1039, 'easy': 1040, 'hour': 1041, 'panting': 1042, 'pressed': 1043, 'forehead': 1044, 'position': 1045, 'shakespeare': 1046, 'rest': 1047, 'everybody': 1048, 'voices': 1049, 'asked': 1050, 'pointing': 1051, 'calling': 1052, 'confused': 1053, 'despair': 1054, 'pulled': 1055, 'luckily': 1056, 'handed': 1057, 'exactly': 1058, 'piece': 1059, 'prize': 1060, 'gravely': 1061, 'presented': 1062, 'acceptance': 1063, 'elegant': 1064, 'short': 1065, 'cheered': 1066, 'absurd': 1067, 'grave': 1068, 'dare': 1069, 'laugh': 1070, 'simply': 1071, 'bowed': 1072, 'solemn': 1073, 'noise': 1074, 'confusion': 1075, 'complained': 1076, 'theirs': 1077, 'ones': 1078, 'choked': 1079, 'patted': 1080, 'begged': 1081, 'promised': 1082, 'c': 1083, 'added': 1084, 'whisper': 1085, 'call': 1086, 'met': 1087, 'both': 1088, 'law': 1089, 'prosecute': 1090, 'denial': 1091, 'cur': 1092, 'wasting': 1093, 'breath': 1094, 'cunning': 1095, 'condemn': 1096, 'death': 1097, 'attending': 1098, 'humbly': 1099, 'fifth': 1100, 'bend': 1101, 'angrily': 1102, 'knot': 1103, 'undo': 1104, 'insult': 1105, 'mean': 1106, 'pleaded': 1107, 'easily': 1108, 'growled': 1109, 'reply': 1110, 'others': 1111, 'joined': 1112, 'shook': 1113, 'impatiently': 1114, 'quicker': 1115, 'pity': 1116, 'sighed': 1117, 'daughter': 1118, 'lose': 1119, 'temper': 1120, 'tongue': 1121, 'young': 1122, 'snappishly': 1123, 'patience': 1124, 'oyster': 1125, 'addressing': 1126, 'particular': 1127, 'venture': 1128, 'pet': 1129, 'bird': 1130, 'sensation': 1131, 'magpie': 1132, 'wrapping': 1133, 'carefully': 1134, 'remarking': 1135, 'suit': 1136, 'throat': 1137, 'canary': 1138, 'bed': 1139, 'various': 1140, 'pretexts': 1141, 'moved': 1142, 'mentioned': 1143, 'lonely': 1144, 'spirited': 1145, 'footsteps': 1146, 'iv': 1147, 'sends': 1148, 'bill': 1149, 'executed': 1150, 'where': 1151, 'guessed': 1152, 'naturedly': 1153, 'nowhere': 1154, 'since': 1155, 'swim': 1156, 'vanished': 1157, 'completely': 1158, 'doing': 1159, 'run': 1160, 'quick': 1161, 'direction': 1162, 'pointed': 1163, 'mistake': 1164, 'housemaid': 1165, 'finds': 1166, 'neat': 1167, 'brass': 1168, 'plate': 1169, 'w': 1170, 'engraved': 1171, 'knocking': 1172, 'upstairs': 1173, 'lest': 1174, 'real': 1175, 'fancying': 1176, 'directly': 1177, 'ordering': 1178, 'tidy': 1179, 'window': 1180, 'hoped': 1181, 'pairs': 1182}\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# helper snippet \n",
    "############################\n",
    "nan_indexes = [797, 1090, 1093, 1094, 1095, 1096, 1097]\n",
    "for sequence in corpus:\n",
    "    for nan_index in nan_indexes:\n",
    "        if nan_index in sequence:\n",
    "            print(sequence)\n",
    "            break\n",
    "    \n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create co-occurrence matrix\n",
    "#print(corpus[0])\n",
    "coMatrix = np.zeros((V, V))\n",
    "for sequence in corpus:\n",
    "    for idx, termId in enumerate(sequence):\n",
    "        sl = len(sequence)\n",
    "        ## select left window\n",
    "        leftw = sequence[max(idx - window_size_corpus, 0): idx]\n",
    "        ## select right window\n",
    "        rightw = sequence[idx + 1: min(idx + window_size_corpus + 1, sl)]\n",
    "        # update co-occurrence matrix\n",
    "        neighboors = leftw + rightw\n",
    "        for neighbor in neighboors:\n",
    "            coMatrix[termId, neighbor] += 1\n",
    "        \n",
    "\n",
    "# matrix normalization\n",
    "(rows, columns) = coMatrix.shape\n",
    "for rowIdx in range(rows):\n",
    "    # ignore first row with 0 entries everywhere\n",
    "    if (rowIdx > 0):\n",
    "        row = coMatrix[rowIdx]\n",
    "        total = row.sum()\n",
    "        # avoid division by zero in words that have no neighboors\n",
    "        if total > 0:\n",
    "            coMatrix[rowIdx] = np.divide(row, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02363823 0.0364851  ... 0.00051387 0.         0.        ]\n",
      " [0.         0.04752343 0.01204819 ... 0.00066934 0.00066934 0.00066934]\n",
      " ...\n",
      " [0.         0.16666667 0.16666667 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.125      ... 0.         0.         0.        ]\n",
      " [0.         0.         0.25       ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(coMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.02646503 0.03402647 ... 0.         0.         0.        ]]\n",
      "[[0.         0.03448276 0.03448276 ... 0.         0.         0.        ]]\n",
      "[[0.         0.13559322 0.00847458 ... 0.         0.         0.        ]]\n",
      "Cosine similarity between Alice and Dinah: [[0.39360011]]\n",
      "Cosine similarity between Alice and Rabbit: [[0.47890931]]\n",
      "Cosine similarity between Dinah and Rabbit: [[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "\n",
    "#find the word vectors for Alice, Dinah, and Rabbit\n",
    "aliceIdx = tokenizer.word_index['Alice'.lower()]\n",
    "dinahIdx = tokenizer.word_index['Dinah'.lower()]\n",
    "rabbitIdx = tokenizer.word_index['Rabbit'.lower()]\n",
    "\n",
    "aliceVector = coMatrix[aliceIdx].reshape(1, -1)\n",
    "dinahVector = coMatrix[dinahIdx].reshape(1, -1)\n",
    "rabbitVector = coMatrix[rabbitIdx].reshape(1, -1)\n",
    "print(aliceVector)\n",
    "print(dinahVector)\n",
    "print(rabbitVector)\n",
    "\n",
    "cosAD = cosine_similarity(aliceVector, dinahVector)\n",
    "print(\"Cosine similarity between Alice and Dinah: {}\".format(cosAD))\n",
    "cosAR = cosine_similarity(aliceVector, rabbitVector)\n",
    "print(\"Cosine similarity between Alice and Rabbit: {}\".format(cosAR))\n",
    "cosDR = cosine_similarity(dinahVector, rabbitVector)\n",
    "print(\"Cosine similarity between Dinah and Rabbit: {}\".format(cosDR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.18693993 0.22482059 0.25996004 0.26423806 0.2653115 ]]\n",
      "[[11  4  7  1  5 41]]\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "# create an array containing the cosine similarity values for alice and the rest\n",
    "nbrs = nn(n_neighbors=6, algorithm='brute', metric='cosine').fit(coMatrix)\n",
    "distances, indices = nbrs.kneighbors(aliceVector)\n",
    "print(distances)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the computed distances and the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we observe the six closest words to 'Alice'.\n",
    "\n",
    "Word with index 11 corresponds to 'Alice' itself. \n",
    "\n",
    "The words for the remaining indexes are:\n",
    "* 4: she\n",
    "* 7: it\n",
    "* 1: the\n",
    "* 5: a\n",
    "* 41: herself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dd8288ddb05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#     f.write(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     f.write(\" \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d38a4aacbe0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reopen your file as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mco_occurrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./vectors_co_occurrence.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242   6   1  63]\n",
      " [  6  26  63 243]\n",
      " [ 11   9   3  67]\n",
      " ...\n",
      " [ 46  14 209 142]\n",
      " [ 14 353 142   5]\n",
      " [353 209   5  24]] \n",
      " [[ 26.]\n",
      " [  1.]\n",
      " [584.]\n",
      " ...\n",
      " [353.]\n",
      " [209.]\n",
      " [142.]]\n"
     ]
    }
   ],
   "source": [
    "# prepare data for cbow\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def generate_data_cbow(corupus: list, window_size: int=2) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create CBOW based on a corpus.\n",
    "    Loop through the sentences and create bag of words based on the windows size.\n",
    "    Args:\n",
    "        corpus: list with words, split up by sentences in a nested list\n",
    "        window_size: size of the bag (1 = one word after and one word before)\n",
    "    \"\"\"\n",
    "    data = np.array([], dtype=np.int32)\n",
    "    target = np.array([])\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for x in range(0, len(sentence) - window_size*2):\n",
    "            append_data = sentence[x: x+(2*window_size)+1]\n",
    "            append_data.pop(window_size)\n",
    "            data = np.append(data, append_data)\n",
    "            target = np.append(target, sentence[x+window_size])\n",
    "    \n",
    "    data = np.resize(data, (int(data.size / (window_size*2)), window_size*2))\n",
    "    target = np.resize(target, (target.size, 1))\n",
    "    return data, target\n",
    "    \n",
    "data, target = generate_data_cbow(corpus, window_size)\n",
    "print(data, \"\\n\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242   6   1  63] [26.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76be21d32225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cbow window_size(2) losses: {} -> {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1068\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1427\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "iterations = 15\n",
    "\n",
    "for idx in range(iterations):\n",
    "    loss = 0.\n",
    "    for x, y in zip(data, target):\n",
    "        print(x, y)\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(\"cbow window_size(2) losses: {} -> {}\".format(idx, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "            \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # repeat the same word several times\n",
    "                    in_words.append([word])\n",
    "                    # add the context words\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                #print(in_words)\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "                #print(all_in)\n",
    "                #print(all_in[0].shape)\n",
    "                #print(all_out)\n",
    "                #print(all_out[0].shape)\n",
    "                #break\n",
    "    return (all_in,all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the preprocessed data of Skipgram\n",
    "def save_skipgram_data(filename, x, y):\n",
    "    f = open(filename ,'w')\n",
    "    for input,outcome  in zip(x,y):\n",
    "        input = np.concatenate(input)\n",
    "        f.write(\" \".join(map(str, list(input))))\n",
    "        f.write(\",\")\n",
    "        outcome = np.concatenate(outcome)\n",
    "        f.write(\" \".join(map(str,list(outcome))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file(filename):\n",
    "    f = open(filename ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPGRAM_2WORDS = \"data_skipgram_2words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,2,V)\n",
    "save_skipgram_data(SKIPGRAM_2WORDS, x, y)\n",
    "\n",
    "SKIPGRAM_4WORDS = \"data_skipgram_4words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,4,V)\n",
    "save_skipgram_data(SKIPGRAM_4WORDS, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models\n",
    "skipgram_2words_model = Sequential()\n",
    "skipgram_2words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_model.add(Reshape((dim, )))\n",
    "skipgram_2words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_model = Sequential()\n",
    "skipgram_4words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_model.add(Reshape((dim, )))\n",
    "skipgram_4words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgrams\n",
    "skipgram_2words_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#define loss function for Skipgram\n",
    "skipgram_4words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train Skipgram model\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_model.train_on_batch(x, y)\n",
    "    skipgram_2words_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_model.train_on_batch(x, y)\n",
    "    skipgram_4words_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_computed_vectors(filename, vectors):\n",
    "    f = open(filename ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_FILENAME = 'vectors_skipgram_2words.txt'\n",
    "skipgram_2words_model_vectors = skipgram_2words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_FILENAME, skipgram_2words_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_FILENAME = 'vectors_skipgram_4words.txt'\n",
    "skipgram_4words_model_vectors = skipgram_4words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_FILENAME, skipgram_4words_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "cbow_dense = Sequential()\n",
    "cbow_dense.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_dense.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_dense.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "cbow_dense.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models with additional dense layer\n",
    "\n",
    "skipgram_2words_modified_model = Sequential()\n",
    "skipgram_2words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_modified_model = Sequential()\n",
    "skipgram_4words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram_2words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "skipgram_4words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram 2words modified losses: 0 -> 40298.47370827198\n",
      "skipgram 2words modified losses: 1 -> 36718.10098481178\n",
      "skipgram 2words modified losses: 2 -> 35214.463769078255\n",
      "skipgram 2words modified losses: 3 -> 33928.63531982899\n",
      "skipgram 2words modified losses: 4 -> 32948.596044421196\n",
      "skipgram 2words modified losses: 5 -> 32201.258150160313\n",
      "skipgram 2words modified losses: 6 -> 31658.562999129295\n",
      "skipgram 2words modified losses: 7 -> 31182.39734441042\n",
      "skipgram 2words modified losses: 8 -> 30829.39433300495\n",
      "skipgram 2words modified losses: 9 -> 30590.332844138145\n",
      "skipgram 2words modified losses: 10 -> 30271.689700245857\n",
      "skipgram 2words modified losses: 11 -> 29975.5885976851\n",
      "skipgram 2words modified losses: 12 -> 29731.610060065985\n",
      "skipgram 2words modified losses: 13 -> 29555.208632495254\n",
      "skipgram 2words modified losses: 14 -> 29367.53787581995\n",
      "skipgram 4words modified losses: 0 -> 40128.40839910507\n",
      "skipgram 4words modified losses: 1 -> 37018.70784020424\n",
      "skipgram 4words modified losses: 2 -> 35872.23134851456\n",
      "skipgram 4words modified losses: 3 -> 34914.722341775894\n",
      "skipgram 4words modified losses: 4 -> 34179.64830648899\n",
      "skipgram 4words modified losses: 5 -> 33768.42542386055\n",
      "skipgram 4words modified losses: 6 -> 33500.59350526333\n",
      "skipgram 4words modified losses: 7 -> 33269.6083278656\n",
      "skipgram 4words modified losses: 8 -> 33245.30694997311\n",
      "skipgram 4words modified losses: 9 -> 33071.43763178587\n",
      "skipgram 4words modified losses: 10 -> 32703.92114406824\n",
      "skipgram 4words modified losses: 11 -> 32427.067891180515\n",
      "skipgram 4words modified losses: 12 -> 32212.412558317184\n",
      "skipgram 4words modified losses: 13 -> 32015.525918483734\n",
      "skipgram 4words modified losses: 14 -> 31841.553291022778\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_2words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words modified losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_4words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words modified losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME = 'vectors_skipgram_2words_modified.txt'\n",
    "skipgram_2words_modified_model_vectors = skipgram_2words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME, skipgram_2words_modified_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME = 'vectors_skipgram_4words_modified.txt'\n",
    "skipgram_4words_modified_model_vectors = skipgram_4words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME, skipgram_4words_modified_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 2,\n",
       " 'funny': 418,\n",
       " 'hippopotamus': 906,\n",
       " 'downward': 638,\n",
       " 'peeped': 587,\n",
       " 'hide': 1031,\n",
       " 'changed': 235,\n",
       " 'complained': 1076,\n",
       " 'gave': 474,\n",
       " 'histories': 705,\n",
       " 'lap': 840,\n",
       " 'conquest': 995,\n",
       " 'family': 940,\n",
       " 'softly': 960,\n",
       " 'sad': 569,\n",
       " 'his': 151,\n",
       " 'pale': 961,\n",
       " 'like': 47,\n",
       " 'sharply': 471,\n",
       " 'overhead': 664,\n",
       " 'book': 205,\n",
       " 'queer': 234,\n",
       " 'gloves': 125,\n",
       " 'yet': 929,\n",
       " 'exactly': 1058,\n",
       " 'mercia': 555,\n",
       " 'cool': 692,\n",
       " 'choked': 1079,\n",
       " 'low': 186,\n",
       " 'lodging': 897,\n",
       " 'terrier': 947,\n",
       " 'prize': 1060,\n",
       " 'northumbria': 556,\n",
       " 'crocodile': 845,\n",
       " 'being': 362,\n",
       " 'respectable': 765,\n",
       " 'naturedly': 1153,\n",
       " 'afterwards': 597,\n",
       " 'time': 66,\n",
       " 'paused': 1033,\n",
       " 'who': 96,\n",
       " 'purring': 933,\n",
       " 'whose': 988,\n",
       " 'opening': 779,\n",
       " 'right': 158,\n",
       " 'over': 100,\n",
       " 'generally': 348,\n",
       " 'considering': 382,\n",
       " 'red': 715,\n",
       " 'directions': 795,\n",
       " 'myself': 786,\n",
       " 'dear': 64,\n",
       " 'sentence': 920,\n",
       " 'or': 57,\n",
       " 'day': 250,\n",
       " 'rabbit': 63,\n",
       " 'heard': 230,\n",
       " 'confused': 1053,\n",
       " 'next': 210,\n",
       " 'chapter': 242,\n",
       " 'shedding': 803,\n",
       " 'carefully': 1134,\n",
       " 'either': 140,\n",
       " 'let': 105,\n",
       " 'familiarly': 973,\n",
       " 'flavour': 727,\n",
       " 'since': 1155,\n",
       " 'high': 168,\n",
       " 'quite': 78,\n",
       " 'with': 23,\n",
       " 'gallons': 804,\n",
       " 'feel': 383,\n",
       " 'see': 56,\n",
       " 'sleepy': 384,\n",
       " 'could': 58,\n",
       " 'mistake': 1164,\n",
       " 'might': 161,\n",
       " 'nile': 850,\n",
       " 'shrill': 926,\n",
       " 'hurt': 433,\n",
       " 'wept': 900,\n",
       " 'centre': 630,\n",
       " 'solid': 674,\n",
       " 'help': 499,\n",
       " 'general': 886,\n",
       " 'lessons': 313,\n",
       " 'drink': 338,\n",
       " 'almost': 342,\n",
       " 'improve': 846,\n",
       " 'yesterday': 819,\n",
       " 'such': 74,\n",
       " 'waters': 849,\n",
       " 'politely': 999,\n",
       " 'animals': 545,\n",
       " 'name': 319,\n",
       " 'paper': 700,\n",
       " 'after': 102,\n",
       " 'passion': 962,\n",
       " 'jaws': 861,\n",
       " 'hear': 296,\n",
       " 'has': 564,\n",
       " 'eats': 772,\n",
       " 'hands': 514,\n",
       " 'lying': 354,\n",
       " 'brown': 949,\n",
       " 'near': 259,\n",
       " 'added': 1084,\n",
       " 'voices': 1049,\n",
       " 'moved': 1142,\n",
       " 'showing': 634,\n",
       " 'conclusion': 887,\n",
       " 'crying': 470,\n",
       " 'four': 214,\n",
       " 'plenty': 612,\n",
       " 'answer': 326,\n",
       " 'glad': 419,\n",
       " 'ran': 207,\n",
       " 'ann': 581,\n",
       " 'mice': 325,\n",
       " 'stigand': 1002,\n",
       " 'did': 99,\n",
       " 'looking': 203,\n",
       " 'usual': 820,\n",
       " 'proceed': 1000,\n",
       " 'came': 163,\n",
       " 'thinking': 823,\n",
       " 'flame': 744,\n",
       " 'trial': 571,\n",
       " 'birds': 200,\n",
       " 'begged': 1081,\n",
       " 'white': 128,\n",
       " 'understand': 530,\n",
       " 'cat': 267,\n",
       " 'reading': 588,\n",
       " 'saw': 400,\n",
       " 'poor': 109,\n",
       " 'rapidly': 873,\n",
       " 'some': 177,\n",
       " 'followed': 755,\n",
       " 'smiling': 860,\n",
       " 'turning': 379,\n",
       " 'hall': 147,\n",
       " 'times': 359,\n",
       " 'calling': 1052,\n",
       " 'piece': 1059,\n",
       " 'stupid': 385,\n",
       " 'daughter': 1118,\n",
       " 'then': 79,\n",
       " 'great': 117,\n",
       " 'salt': 522,\n",
       " 'longed': 688,\n",
       " 'nervous': 743,\n",
       " 'kills': 956,\n",
       " 'golden': 188,\n",
       " 'fetch': 372,\n",
       " 'ordering': 1178,\n",
       " 'of': 8,\n",
       " 'thump': 431,\n",
       " 'if': 32,\n",
       " 'bright': 335,\n",
       " 'lock': 682,\n",
       " 'capital': 284,\n",
       " 'corner': 435,\n",
       " 'useful': 542,\n",
       " 'promised': 1082,\n",
       " 'glass': 222,\n",
       " 'creep': 768,\n",
       " 'burn': 717,\n",
       " 'thing': 89,\n",
       " 'declare': 880,\n",
       " 'hurrying': 665,\n",
       " 'sensation': 1131,\n",
       " 'on': 21,\n",
       " 'she': 4,\n",
       " 'quicker': 1115,\n",
       " 'wind': 666,\n",
       " 'leaders': 992,\n",
       " 'swimming': 528,\n",
       " 'nearly': 871,\n",
       " 'slippery': 751,\n",
       " 'come': 86,\n",
       " 'plate': 1169,\n",
       " 'door': 95,\n",
       " 'silence': 550,\n",
       " 'breath': 1094,\n",
       " 'an': 133,\n",
       " 'tears': 170,\n",
       " 'patriotic': 1003,\n",
       " 'splendidly': 809,\n",
       " 'particular': 1127,\n",
       " 'getting': 138,\n",
       " 'worse': 879,\n",
       " 'ahem': 985,\n",
       " 'houses': 898,\n",
       " 'curly': 948,\n",
       " 'nearer': 904,\n",
       " 'snappishly': 1123,\n",
       " 'please': 217,\n",
       " 'beg': 195,\n",
       " 'twenty': 833,\n",
       " 'every': 330,\n",
       " 'cold': 984,\n",
       " 'pine': 731,\n",
       " 'try': 194,\n",
       " 'ought': 299,\n",
       " 'past': 625,\n",
       " 'daisy': 591,\n",
       " 'pleaded': 1107,\n",
       " 'your': 88,\n",
       " 'tea': 653,\n",
       " 'custard': 730,\n",
       " 'really': 272,\n",
       " 'flowers': 691,\n",
       " 'frowning': 998,\n",
       " 'stop': 495,\n",
       " 'moment': 131,\n",
       " 'canary': 1138,\n",
       " 'currants': 766,\n",
       " 'trotting': 497,\n",
       " 'sends': 1148,\n",
       " 'railway': 525,\n",
       " 'row': 438,\n",
       " 'doing': 1159,\n",
       " 'savage': 812,\n",
       " 'meaning': 1028,\n",
       " 'sight': 328,\n",
       " 'fond': 478,\n",
       " 'mean': 1106,\n",
       " 'beds': 690,\n",
       " 'curiosity': 603,\n",
       " 'far': 486,\n",
       " 'perhaps': 265,\n",
       " 'shelves': 399,\n",
       " 'paws': 536,\n",
       " 'inches': 333,\n",
       " 'simple': 712,\n",
       " 'hair': 505,\n",
       " 'ready': 232,\n",
       " 'slowly': 303,\n",
       " 'stockings': 783,\n",
       " 'pop': 606,\n",
       " 'pair': 229,\n",
       " 'pity': 1116,\n",
       " 'advisable': 557,\n",
       " 'prizes': 241,\n",
       " 'voice': 233,\n",
       " 'hard': 501,\n",
       " 'would': 59,\n",
       " 'remembered': 349,\n",
       " 'lamps': 669,\n",
       " 'label': 701,\n",
       " 'took': 139,\n",
       " 'second': 679,\n",
       " 'rules': 454,\n",
       " 'better': 548,\n",
       " 'neat': 1167,\n",
       " 'dinah': 87,\n",
       " 'about': 40,\n",
       " 'earth': 410,\n",
       " 'run': 1160,\n",
       " 'forgot': 485,\n",
       " 'number': 890,\n",
       " 'caucus': 376,\n",
       " 'they': 39,\n",
       " 'end': 309,\n",
       " 'went': 52,\n",
       " 'pressed': 1043,\n",
       " 'though': 411,\n",
       " 'whiskers': 437,\n",
       " 'known': 974,\n",
       " 'shakespeare': 1046,\n",
       " 'too': 155,\n",
       " 'along': 334,\n",
       " 'neatly': 854,\n",
       " 'tart': 729,\n",
       " 'lovely': 740,\n",
       " 'ask': 263,\n",
       " 'bristling': 939,\n",
       " 'country': 643,\n",
       " 'impatiently': 1114,\n",
       " 'likely': 406,\n",
       " 'night': 324,\n",
       " 'larger': 448,\n",
       " 'head': 189,\n",
       " 'what': 44,\n",
       " 'ada': 824,\n",
       " 'nasty': 942,\n",
       " 'me': 42,\n",
       " 'kind': 787,\n",
       " 'grave': 1068,\n",
       " 'get': 67,\n",
       " 'angry': 534,\n",
       " 'learnt': 632,\n",
       " 'sister': 380,\n",
       " 'deeply': 719,\n",
       " 'fancying': 1176,\n",
       " 'knot': 1103,\n",
       " 'heap': 660,\n",
       " 'call': 1086,\n",
       " 'smile': 1032,\n",
       " 'gently': 859,\n",
       " 'pool': 137,\n",
       " 'climb': 749,\n",
       " 'thought': 53,\n",
       " 'any': 121,\n",
       " 'daresay': 913,\n",
       " 'cunning': 1095,\n",
       " 'nine': 492,\n",
       " 'hoping': 453,\n",
       " 'rest': 1047,\n",
       " 'garden': 169,\n",
       " 'desperate': 813,\n",
       " 'energetic': 1027,\n",
       " 'leave': 472,\n",
       " 'luckily': 1056,\n",
       " 'welcome': 857,\n",
       " 'other': 221,\n",
       " 'consultation': 972,\n",
       " 'tongue': 1121,\n",
       " 'offer': 1014,\n",
       " 'feelings': 925,\n",
       " 'cats': 107,\n",
       " 'mentioned': 1143,\n",
       " 'death': 1097,\n",
       " 'guess': 872,\n",
       " 'swim': 1156,\n",
       " 'esq': 796,\n",
       " 'clinging': 968,\n",
       " 'kid': 231,\n",
       " 'having': 245,\n",
       " 'usurpation': 994,\n",
       " 'moderate': 1017,\n",
       " 'remained': 771,\n",
       " 'knowledge': 413,\n",
       " 'position': 1045,\n",
       " 'sending': 489,\n",
       " 'box': 350,\n",
       " 'both': 1088,\n",
       " 'curtain': 680,\n",
       " 'marmalade': 620,\n",
       " 'eaten': 707,\n",
       " 'doth': 513,\n",
       " 'speaking': 529,\n",
       " 'fitted': 684,\n",
       " 'cried': 124,\n",
       " 'real': 1175,\n",
       " 'children': 274,\n",
       " 'possibly': 747,\n",
       " 'disagree': 722,\n",
       " 'sound': 641,\n",
       " 'trembling': 368,\n",
       " 'lonely': 1144,\n",
       " 'pounds': 955,\n",
       " 'humbly': 1099,\n",
       " 'roast': 733,\n",
       " 'thousand': 631,\n",
       " 'fanning': 818,\n",
       " 'licking': 936,\n",
       " 'inquisitively': 911,\n",
       " 'for': 18,\n",
       " 'hadn': 526,\n",
       " 'when': 46,\n",
       " 'grammar': 910,\n",
       " 'insult': 1105,\n",
       " 'farmer': 953,\n",
       " 'shook': 1113,\n",
       " 'swam': 236,\n",
       " 'longitude': 417,\n",
       " 'crown': 1015,\n",
       " 'wonder': 103,\n",
       " 'alone': 515,\n",
       " 'passage': 327,\n",
       " 'unpleasant': 710,\n",
       " 'telescopes': 698,\n",
       " 'why': 132,\n",
       " 'sand': 894,\n",
       " 'practice': 635,\n",
       " 'locks': 677,\n",
       " 'set': 776,\n",
       " 'gravely': 1061,\n",
       " 'iii': 964,\n",
       " 'spades': 896,\n",
       " 'listening': 640,\n",
       " 'live': 862,\n",
       " 'suddenly': 175,\n",
       " 'crossly': 1006,\n",
       " 'whole': 290,\n",
       " 'oyster': 1125,\n",
       " 'milk': 652,\n",
       " 'feet': 101,\n",
       " 'just': 141,\n",
       " 'solemn': 1073,\n",
       " 'certainly': 456,\n",
       " 'subject': 539,\n",
       " 'leap': 921,\n",
       " 'pointing': 1051,\n",
       " 'toys': 864,\n",
       " 'easy': 1040,\n",
       " 'disappointment': 621,\n",
       " 'ma': 320,\n",
       " 'dark': 305,\n",
       " 'question': 162,\n",
       " 'winter': 1035,\n",
       " 'digging': 893,\n",
       " 'pairs': 1182,\n",
       " 'before': 130,\n",
       " 'bank': 381,\n",
       " 'made': 206,\n",
       " 'dry': 164,\n",
       " 'against': 476,\n",
       " 'began': 75,\n",
       " 'fifth': 1100,\n",
       " 'noticed': 306,\n",
       " 'avoid': 874,\n",
       " 'law': 1089,\n",
       " 'dull': 774,\n",
       " 'three': 331,\n",
       " 'slipped': 521,\n",
       " 'are': 106,\n",
       " 'rome': 512,\n",
       " 'reaching': 806,\n",
       " 'tone': 196,\n",
       " 'nurse': 537,\n",
       " 'started': 394,\n",
       " 'conduct': 1016,\n",
       " 'fire': 935,\n",
       " 'spirited': 1145,\n",
       " 'kept': 281,\n",
       " 'reply': 1110,\n",
       " 'way': 45,\n",
       " 'somewhere': 409,\n",
       " 'course': 204,\n",
       " 'chatte': 919,\n",
       " 'true': 629,\n",
       " 'hated': 941,\n",
       " 'two': 227,\n",
       " 'reach': 469,\n",
       " 'skurried': 816,\n",
       " 'afraid': 268,\n",
       " 'station': 899,\n",
       " 'severely': 475,\n",
       " 'stairs': 627,\n",
       " 'making': 387,\n",
       " 'fan': 126,\n",
       " 'ah': 503,\n",
       " 'room': 583,\n",
       " 'sometimes': 427,\n",
       " 'dipped': 610,\n",
       " 'immediate': 1025,\n",
       " 'simply': 1071,\n",
       " 'engraved': 1171,\n",
       " 'fountains': 693,\n",
       " 'shape': 1038,\n",
       " 'adjourn': 1024,\n",
       " 'addressing': 1126,\n",
       " 'existence': 877,\n",
       " 'their': 180,\n",
       " 'belong': 676,\n",
       " 'soothing': 928,\n",
       " 'finger': 341,\n",
       " 'got': 118,\n",
       " 'circle': 1036,\n",
       " 'whisper': 1085,\n",
       " 'which': 85,\n",
       " 'age': 504,\n",
       " 'put': 178,\n",
       " 'paris': 511,\n",
       " 'leaves': 662,\n",
       " 'measure': 870,\n",
       " 'miles': 407,\n",
       " 'minute': 473,\n",
       " 'now': 81,\n",
       " 'est': 918,\n",
       " 'where': 1151,\n",
       " 'thimble': 294,\n",
       " 'fishes': 858,\n",
       " 'legs': 750,\n",
       " 'housemaid': 1165,\n",
       " 'watch': 392,\n",
       " 'shore': 543,\n",
       " 'many': 310,\n",
       " 'knows': 826,\n",
       " 'puzzle': 822,\n",
       " 'first': 104,\n",
       " 'grand': 636,\n",
       " 'acceptance': 1063,\n",
       " 'life': 356,\n",
       " 'take': 253,\n",
       " 'm': 77,\n",
       " 'wild': 708,\n",
       " 'brass': 1168,\n",
       " 'learn': 866,\n",
       " 'eagerly': 371,\n",
       " 'looked': 129,\n",
       " 'curious': 345,\n",
       " 'six': 830,\n",
       " 'later': 724,\n",
       " 'the': 1,\n",
       " 'latitude': 416,\n",
       " 'among': 261,\n",
       " 'care': 769,\n",
       " 'five': 828,\n",
       " 'will': 191,\n",
       " 'condemn': 1096,\n",
       " 'advise': 752,\n",
       " 'guessed': 1152,\n",
       " 'not': 29,\n",
       " 're': 322,\n",
       " 'ignorant': 648,\n",
       " 'ears': 436,\n",
       " 'legged': 673,\n",
       " 'sulky': 976,\n",
       " 'young': 1122,\n",
       " 'ou': 917,\n",
       " 'walked': 442,\n",
       " 'schoolroom': 633,\n",
       " 'matter': 428,\n",
       " 'side': 441,\n",
       " 'hope': 650,\n",
       " 'drowned': 902,\n",
       " 'minutes': 467,\n",
       " 'out': 31,\n",
       " 'talk': 237,\n",
       " 'new': 420,\n",
       " 'shutting': 455,\n",
       " 'meet': 558,\n",
       " 'somebody': 307,\n",
       " 'apple': 732,\n",
       " 'waited': 466,\n",
       " 'venture': 1128,\n",
       " 'jumped': 663,\n",
       " 'sudden': 363,\n",
       " 'until': 805,\n",
       " 'wrapping': 1133,\n",
       " 'words': 159,\n",
       " 'uncomfortable': 971,\n",
       " 'us': 373,\n",
       " 'allow': 978,\n",
       " 'bend': 1101,\n",
       " 'harm': 907,\n",
       " 'jury': 572,\n",
       " 'without': 174,\n",
       " 'deep': 302,\n",
       " 'dare': 1069,\n",
       " 'race': 202,\n",
       " 'found': 73,\n",
       " 'lately': 697,\n",
       " 'must': 69,\n",
       " 'forgotten': 460,\n",
       " 'means': 1007,\n",
       " 'stopping': 611,\n",
       " 'dripping': 969,\n",
       " 'splashing': 903,\n",
       " 'eyes': 176,\n",
       " 'advice': 753,\n",
       " 'because': 711,\n",
       " 'says': 541,\n",
       " 's': 36,\n",
       " 'd': 198,\n",
       " 'called': 199,\n",
       " 'twelve': 829,\n",
       " 'knife': 720,\n",
       " 'left': 351,\n",
       " 'meeting': 1023,\n",
       " 'him': 293,\n",
       " 'machines': 892,\n",
       " 'confusion': 1075,\n",
       " 'small': 167,\n",
       " 'canterbury': 1005,\n",
       " 'everything': 282,\n",
       " 'toffee': 735,\n",
       " 'shoes': 782,\n",
       " 'eye': 353,\n",
       " 'muttering': 498,\n",
       " 'shiver': 997,\n",
       " 'fright': 923,\n",
       " 'managed': 403,\n",
       " 'manage': 323,\n",
       " 'fact': 463,\n",
       " 'give': 488,\n",
       " 'ferrets': 578,\n",
       " 'pet': 1129,\n",
       " 'lory': 201,\n",
       " 'am': 160,\n",
       " 'atheling': 1013,\n",
       " 'labelled': 618,\n",
       " 'those': 271,\n",
       " 'seven': 832,\n",
       " 'longer': 667,\n",
       " 'seaside': 885,\n",
       " 'passed': 617,\n",
       " 'animal': 924,\n",
       " 'knew': 451,\n",
       " 'it': 7,\n",
       " 'nonsense': 491,\n",
       " 'home': 308,\n",
       " 'executed': 1150,\n",
       " 'few': 337,\n",
       " 'blown': 745,\n",
       " 'knowing': 979,\n",
       " 'pocket': 251,\n",
       " 'caused': 568,\n",
       " 'this': 28,\n",
       " 'frightened': 518,\n",
       " 'water': 365,\n",
       " 'asked': 1050,\n",
       " 'doorway': 694,\n",
       " 'field': 604,\n",
       " 'dogs': 370,\n",
       " 'commotion': 959,\n",
       " 'latin': 909,\n",
       " 'forehead': 1044,\n",
       " 'further': 742,\n",
       " 'poison': 340,\n",
       " 'bent': 1030,\n",
       " 'melancholy': 559,\n",
       " 'late': 298,\n",
       " 'shan': 784,\n",
       " 'were': 65,\n",
       " 'except': 675,\n",
       " 'hot': 249,\n",
       " 'laugh': 1070,\n",
       " 'bring': 757,\n",
       " 'sides': 613,\n",
       " 'there': 55,\n",
       " 'punished': 901,\n",
       " 'pour': 848,\n",
       " 'begin': 696,\n",
       " 'large': 153,\n",
       " 'elegant': 1064,\n",
       " 'waistcoat': 393,\n",
       " 'common': 775,\n",
       " 'presently': 637,\n",
       " 'be': 20,\n",
       " 'listen': 414,\n",
       " 'brightened': 739,\n",
       " 'bill': 1149,\n",
       " 'lesson': 533,\n",
       " 'don': 171,\n",
       " 'maps': 614,\n",
       " 'direction': 1162,\n",
       " 'adoption': 1026,\n",
       " 'deal': 357,\n",
       " 'sure': 136,\n",
       " 'quiver': 922,\n",
       " 'putting': 867,\n",
       " 'by': 71,\n",
       " 'rat': 685,\n",
       " 'frog': 1008,\n",
       " 'patted': 1080,\n",
       " 'play': 865,\n",
       " 'bleeds': 721,\n",
       " 'doesn': 173,\n",
       " 'absurd': 1067,\n",
       " 'lest': 1174,\n",
       " 'pointed': 1163,\n",
       " 'brave': 628,\n",
       " 'neck': 699,\n",
       " 'dried': 807,\n",
       " 'wise': 703,\n",
       " 'seldom': 754,\n",
       " 'notion': 915,\n",
       " 'wasting': 1093,\n",
       " 'smaller': 767,\n",
       " 'edwin': 552,\n",
       " 'important': 986,\n",
       " 'change': 519,\n",
       " 'favoured': 989,\n",
       " 'pleasure': 590,\n",
       " 'fallen': 311,\n",
       " 'normans': 1019,\n",
       " 'multiplication': 834,\n",
       " 'met': 1087,\n",
       " 'turkey': 734,\n",
       " 'happened': 336,\n",
       " 'said': 16,\n",
       " 'mixed': 726,\n",
       " 'falling': 396,\n",
       " 'sir': 500,\n",
       " 'through': 144,\n",
       " 'anxiously': 276,\n",
       " 'duchess': 280,\n",
       " 'hung': 615,\n",
       " 'begun': 429,\n",
       " 'doors': 440,\n",
       " 'know': 62,\n",
       " 'distance': 315,\n",
       " 'ringlets': 506,\n",
       " 'presented': 1062,\n",
       " 'running': 562,\n",
       " 'signify': 835,\n",
       " 'soon': 76,\n",
       " 'hanging': 670,\n",
       " 'once': 98,\n",
       " 'cut': 718,\n",
       " 'bad': 520,\n",
       " 'key': 148,\n",
       " 'telescope': 450,\n",
       " 'till': 868,\n",
       " 'placed': 1039,\n",
       " 'puzzling': 509,\n",
       " 'fall': 257,\n",
       " 'away': 165,\n",
       " 'positively': 980,\n",
       " 'her': 14,\n",
       " 'curtsey': 646,\n",
       " 'actually': 600,\n",
       " 'claws': 856,\n",
       " 'close': 295,\n",
       " 'upstairs': 1173,\n",
       " 'thirteen': 831,\n",
       " 'earnestly': 658,\n",
       " 'antipathies': 639,\n",
       " 'as': 15,\n",
       " 'history': 367,\n",
       " 'soft': 938,\n",
       " 'shrink': 741,\n",
       " 'bit': 432,\n",
       " 'expecting': 773,\n",
       " 'anything': 213,\n",
       " 'hopeless': 801,\n",
       " 'vulgar': 943,\n",
       " 'house': 156,\n",
       " 'wanted': 551,\n",
       " 'able': 785,\n",
       " 'rats': 957,\n",
       " 'curiouser': 484,\n",
       " 'besides': 827,\n",
       " 'tiny': 445,\n",
       " 'window': 1180,\n",
       " 'scale': 851,\n",
       " 'have': 84,\n",
       " 'goes': 825,\n",
       " 'remember': 219,\n",
       " 'feeling': 464,\n",
       " 'creatures': 963,\n",
       " 'zealand': 644,\n",
       " 'belongs': 952,\n",
       " 'make': 212,\n",
       " 'short': 1065,\n",
       " 'throat': 1137,\n",
       " 'fear': 402,\n",
       " 'show': 535,\n",
       " 'delight': 683,\n",
       " 'draggled': 966,\n",
       " 'french': 531,\n",
       " 'spread': 855,\n",
       " 'cross': 970,\n",
       " 'led': 447,\n",
       " 'ones': 1078,\n",
       " 'orange': 619,\n",
       " 'several': 312,\n",
       " 'conversation': 944,\n",
       " 'worm': 1009,\n",
       " 'knocking': 1172,\n",
       " 'denial': 1091,\n",
       " 'w': 1170,\n",
       " 'fender': 798,\n",
       " 'how': 38,\n",
       " 'sounded': 842,\n",
       " 'returning': 808,\n",
       " 'at': 34,\n",
       " 'judge': 573,\n",
       " 'patience': 1124,\n",
       " 'lose': 1119,\n",
       " 'cake': 355,\n",
       " 'hold': 458,\n",
       " 'struck': 800,\n",
       " 'say': 83,\n",
       " 'ventured': 725,\n",
       " 'sea': 523,\n",
       " 'mary': 580,\n",
       " 'candle': 347,\n",
       " 'hurriedly': 1011,\n",
       " 'comfits': 566,\n",
       " 'grow': 480,\n",
       " 'fancy': 321,\n",
       " 'throw': 950,\n",
       " 'ten': 738,\n",
       " 'himself': 811,\n",
       " 'footsteps': 1146,\n",
       " 'conqueror': 532,\n",
       " 'plainly': 748,\n",
       " 'planning': 791,\n",
       " 'word': 642,\n",
       " 'sort': 179,\n",
       " 'a': 5,\n",
       " 've': 143,\n",
       " 'rising': 1021,\n",
       " 'makes': 479,\n",
       " 'twice': 586,\n",
       " 'toast': 737,\n",
       " 'bottle': 273,\n",
       " 'crowded': 289,\n",
       " 'quick': 1161,\n",
       " 'pretending': 762,\n",
       " 'c': 1083,\n",
       " 'continued': 1020,\n",
       " 'solemnly': 560,\n",
       " 'jar': 401,\n",
       " 'geography': 836,\n",
       " 'only': 122,\n",
       " 'certain': 343,\n",
       " 'else': 266,\n",
       " 'no': 50,\n",
       " 'wondering': 672,\n",
       " 'hunting': 579,\n",
       " 'alice': 11,\n",
       " 'darkness': 817,\n",
       " 'face': 346,\n",
       " 'behind': 329,\n",
       " 'largest': 780,\n",
       " 'our': 197,\n",
       " 'poky': 863,\n",
       " 'bowed': 1072,\n",
       " 'half': 149,\n",
       " 'bats': 269,\n",
       " 'others': 1111,\n",
       " 'australia': 645,\n",
       " 'cheated': 758,\n",
       " 'usually': 459,\n",
       " 'been': 166,\n",
       " 'hundred': 954,\n",
       " 'daisies': 594,\n",
       " 'cupboards': 398,\n",
       " 'directly': 1177,\n",
       " 'chin': 883,\n",
       " 'mouse': 25,\n",
       " 'cheerfully': 852,\n",
       " 'sticks': 661,\n",
       " 'different': 821,\n",
       " 'mine': 507,\n",
       " 'oh': 51,\n",
       " 'speak': 278,\n",
       " 'assembled': 965,\n",
       " 'indeed': 190,\n",
       " 'wet': 547,\n",
       " 'tale': 377,\n",
       " 'angrily': 1102,\n",
       " 'shall': 92,\n",
       " 'beasts': 709,\n",
       " 'burning': 602,\n",
       " 'london': 837,\n",
       " 'people': 216,\n",
       " 'wooden': 895,\n",
       " 'herself': 41,\n",
       " 'edgar': 1012,\n",
       " 'fell': 209,\n",
       " 'buttered': 736,\n",
       " 'hole': 243,\n",
       " 'written': 649,\n",
       " 'little': 24,\n",
       " 'remarkable': 390,\n",
       " 'table': 120,\n",
       " 'morcar': 553,\n",
       " 'washing': 937,\n",
       " 'dressed': 810,\n",
       " 'magpie': 1132,\n",
       " 'cur': 1092,\n",
       " 'nicely': 934,\n",
       " 'hand': 185,\n",
       " 'brother': 908,\n",
       " 'finds': 1166,\n",
       " 'person': 352,\n",
       " 'ii': 778,\n",
       " 'last': 291,\n",
       " 'shoulders': 695,\n",
       " 'growing': 483,\n",
       " 'we': 239,\n",
       " 'should': 218,\n",
       " 'argument': 975,\n",
       " 'wondered': 599,\n",
       " 'insolence': 1018,\n",
       " 'marked': 224,\n",
       " 'cheered': 1066,\n",
       " 'finished': 344,\n",
       " 'read': 704,\n",
       " 'alas': 332,\n",
       " 'seems': 360,\n",
       " 'despair': 1054,\n",
       " 'very': 27,\n",
       " 'william': 287,\n",
       " 'happen': 304,\n",
       " 'long': 94,\n",
       " 'coming': 255,\n",
       " 'much': 91,\n",
       " 'lazily': 931,\n",
       " 'find': 150,\n",
       " 'vanished': 1157,\n",
       " 'done': 516,\n",
       " 'lit': 668,\n",
       " 'best': 225,\n",
       " 'again': 54,\n",
       " 'walk': 317,\n",
       " 'sat': 226,\n",
       " 'hastily': 279,\n",
       " 'fixed': 983,\n",
       " 'miss': 424,\n",
       " 'timid': 814,\n",
       " 'rather': 181,\n",
       " 'pattering': 496,\n",
       " 'undo': 1104,\n",
       " 'escape': 876,\n",
       " 'nor': 596,\n",
       " 'hour': 1041,\n",
       " 'in': 10,\n",
       " 'open': 678,\n",
       " 'prosecute': 1090,\n",
       " 'waiting': 452,\n",
       " 'sadly': 443,\n",
       " 'older': 977,\n",
       " 'completely': 1158,\n",
       " 'mind': 248,\n",
       " 'believe': 1029,\n",
       " 'altogether': 468,\n",
       " 'always': 369,\n",
       " 'opened': 446,\n",
       " 'crossed': 839,\n",
       " 'pegs': 616,\n",
       " 'dropped': 358,\n",
       " 'round': 108,\n",
       " 'printed': 702,\n",
       " 'morning': 502,\n",
       " 'go': 82,\n",
       " 'used': 510,\n",
       " 'that': 13,\n",
       " 'refused': 981,\n",
       " 'good': 157,\n",
       " 'down': 26,\n",
       " 'spoke': 421,\n",
       " 'suppose': 527,\n",
       " 'from': 256,\n",
       " 'tell': 146,\n",
       " 'll': 37,\n",
       " 'tidy': 1179,\n",
       " 'look': 154,\n",
       " 'occurred': 598,\n",
       " 'up': 43,\n",
       " 'pretend': 763,\n",
       " 'child': 477,\n",
       " 'hate': 544,\n",
       " 'nobody': 577,\n",
       " 'grin': 853,\n",
       " 'locked': 671,\n",
       " 'than': 223,\n",
       " 'lost': 434,\n",
       " 'curtseying': 647,\n",
       " 'hardly': 764,\n",
       " 'whether': 386,\n",
       " 'story': 575,\n",
       " 'tunnel': 609,\n",
       " 'off': 80,\n",
       " 'hurried': 252,\n",
       " 'pretexts': 1141,\n",
       " 'across': 395,\n",
       " 'catch': 425,\n",
       " 'cherry': 728,\n",
       " 'is': 33,\n",
       " 'aloud': 408,\n",
       " 'stay': 361,\n",
       " 'beginning': 584,\n",
       " 'taste': 461,\n",
       " 'attending': 1098,\n",
       " 'another': 254,\n",
       " 'happens': 481,\n",
       " 'sits': 932,\n",
       " 'tail': 285,\n",
       " 'o': 366,\n",
       " 'work': 777,\n",
       " 'wouldn': 404,\n",
       " 'yourself': 494,\n",
       " 'friends': 713,\n",
       " 'ate': 770,\n",
       " 'saying': 183,\n",
       " 'these': 881,\n",
       " 'seen': 208,\n",
       " 'natural': 391,\n",
       " 'beautifully': 457,\n",
       " 'can': 97,\n",
       " 'burst': 869,\n",
       " 'shining': 847,\n",
       " 'going': 116,\n",
       " 'air': 264,\n",
       " 'violently': 815,\n",
       " 'itself': 297,\n",
       " 'liked': 563,\n",
       " 'narrow': 875,\n",
       " 'you': 12,\n",
       " 'fifteen': 681,\n",
       " 'surprised': 277,\n",
       " 'walrus': 905,\n",
       " 'rate': 270,\n",
       " 'offended': 238,\n",
       " 'one': 48,\n",
       " 'decided': 746,\n",
       " 'girl': 422,\n",
       " 'growled': 1109,\n",
       " 'notice': 1010,\n",
       " 'wherever': 888,\n",
       " 'into': 72,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train nearest neighbors model\n",
    "nn_skipgram_2words = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_2words_model_vectors)\n",
    "nn_skipgram_4words = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_4words_model_vectors)\n",
    "nn_skipgram_2words_modified = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_2words_modified_model_vectors)\n",
    "nn_skipgram_4words_modified = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_4words_modified_model_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183, 100)\n",
      "1182\n"
     ]
    }
   ],
   "source": [
    "## --- SNIPPET USED TO UNDERSTAND SOME OPERATIONS ---- ##\n",
    "print(skipgram_2words_modified_model_vectors.shape)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "# implementation of the function argmax(w.(w3 + w2 - w1)), with w - w3 =' w2 - w1\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.neighbors import NearestNeighbors as nn\n",
    "#nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "\n",
    "def analogy(word1, word2, word3, word_vectors, nnmodel):\n",
    "    vector_w1 = word_vectors[tokenizer.word_index[word1]]\n",
    "    vector_w2 = word_vectors[tokenizer.word_index[word2]]\n",
    "    vector_w3 = word_vectors[tokenizer.word_index[word3]]\n",
    "    composed_vector = vector_w1 - vector_w2 + vector_w3\n",
    "    distances, indices = nnmodel.kneighbors(composed_vector.reshape(1, -1))\n",
    "    return (distances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the four different word embeddings\n",
    "\n",
    "Implement your own function to perform the analogy task with.\n",
    "    \n",
    "Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    \n",
    "Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.\n",
    "    \n",
    "Visualize your results and interpret your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analogy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b35b3de9be5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# I want to try with: melancholy - funny + smile = sad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m (distances_skipgram_2words, indices_skipgram_2words) = analogy(\"cry\", \"funny\", \"smile\", \n\u001b[0m\u001b[1;32m      4\u001b[0m                                                                \u001b[0mskipgram_2words_model_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                               nn_skipgram_2words)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analogy' is not defined"
     ]
    }
   ],
   "source": [
    "# I want to try with: melancholy - funny + smile = sad\n",
    "\n",
    "(distances_skipgram_2words, indices_skipgram_2words) = analogy(\"cry\", \"funny\", \"smile\", \n",
    "                                                               skipgram_2words_model_vectors, \n",
    "                                                              nn_skipgram_2words)\n",
    "\n",
    "(distances_skipgram_4words, indices_skipgram_4words) = analogy(\"cry\", \"funny\", \"smile\", \n",
    "                                                               skipgram_4words_model_vectors, \n",
    "                                                              nn_skipgram_4words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile\n",
      "melancholy\n",
      "game\n",
      "soothing\n",
      "patriotic\n"
     ]
    }
   ],
   "source": [
    "#print(distances_skipgram_2words)\n",
    "#print(indices_skipgram_2words)\n",
    "for index in indices_skipgram_2words[0]:\n",
    "    print(inverted_index[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile\n",
      "melancholy\n",
      "hide\n",
      "yourself\n",
      "circle\n"
     ]
    }
   ],
   "source": [
    "#print(distances_skipgram_4words)\n",
    "#print(indices_skipgram_4words)\n",
    "for index in indices_skipgram_4words[0]:\n",
    "    print(inverted_index[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
