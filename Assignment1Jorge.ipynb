{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.local/share/virtualenvs/RecommenderSystems-c5N1t04d/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'and', 3: 'to', 4: 'she', 5: 'a', 6: 'i', 7: 'it', 8: 'of', 9: 'was', 10: 'in', 11: 'alice', 12: 'you', 13: 'that', 14: 'her', 15: 'as', 16: 'said', 17: 'had', 18: 'for', 19: 'but', 20: 'be', 21: 'on', 22: 'all', 23: 'with', 24: 'little', 25: 'mouse', 26: 'down', 27: 'very', 28: 'this', 29: 'not', 30: 'so', 31: 'out', 32: 'if', 33: 'is', 34: 'at', 35: 't', 36: 's', 37: 'll', 38: 'how', 39: 'they', 40: 'about', 41: 'herself', 42: 'me', 43: 'up', 44: 'what', 45: 'way', 46: 'when', 47: 'like', 48: 'one', 49: 'do', 50: 'no', 51: 'oh', 52: 'went', 53: 'thought', 54: 'again', 55: 'there', 56: 'see', 57: 'or', 58: 'could', 59: 'would', 60: 'think', 61: 'them', 62: 'know', 63: 'rabbit', 64: 'dear', 65: 'were', 66: 'time', 67: 'get', 68: 'here', 69: 'must', 70: 'my', 71: 'by', 72: 'into', 73: 'found', 74: 'such', 75: 'began', 76: 'soon', 77: 'm', 78: 'quite', 79: 'then', 80: 'off', 81: 'now', 82: 'go', 83: 'say', 84: 'have', 85: 'which', 86: 'come', 87: 'dinah', 88: 'your', 89: 'thing', 90: 'dodo', 91: 'much', 92: 'shall', 93: 'things', 94: 'long', 95: 'door', 96: 'who', 97: 'can', 98: 'once', 99: 'did', 100: 'over', 101: 'feet', 102: 'after', 103: 'wonder', 104: 'first', 105: 'let', 106: 'are', 107: 'cats', 108: 'round', 109: 'poor', 110: 'back', 111: 'he', 112: 'nothing', 113: 'seemed', 114: 'its', 115: 'never', 116: 'going', 117: 'great', 118: 'got', 119: 'ever', 120: 'table', 121: 'any', 122: 'only', 123: 'more', 124: 'cried', 125: 'gloves', 126: 'fan', 127: 'well', 128: 'white', 129: 'looked', 130: 'before', 131: 'moment', 132: 'why', 133: 'an', 134: 'eat', 135: 'however', 136: 'sure', 137: 'pool', 138: 'getting', 139: 'took', 140: 'either', 141: 'just', 142: 'upon', 143: 've', 144: 'through', 145: 'wish', 146: 'tell', 147: 'hall', 148: 'key', 149: 'half', 150: 'find', 151: 'his', 152: 'use', 153: 'large', 154: 'look', 155: 'too', 156: 'house', 157: 'good', 158: 'right', 159: 'words', 160: 'am', 161: 'might', 162: 'question', 163: 'came', 164: 'dry', 165: 'away', 166: 'been', 167: 'small', 168: 'high', 169: 'garden', 170: 'tears', 171: 'don', 172: 'won', 173: 'doesn', 174: 'without', 175: 'suddenly', 176: 'eyes', 177: 'some', 178: 'put', 179: 'sort', 180: 'their', 181: 'rather', 182: 'talking', 183: 'saying', 184: 'felt', 185: 'hand', 186: 'low', 187: 'trying', 188: 'golden', 189: 'head', 190: 'indeed', 191: 'will', 192: 'while', 193: 'same', 194: 'try', 195: 'beg', 196: 'tone', 197: 'our', 198: 'd', 199: 'called', 200: 'birds', 201: 'lory', 202: 'race', 203: 'looking', 204: 'course', 205: 'book', 206: 'made', 207: 'ran', 208: 'seen', 209: 'fell', 210: 'next', 211: 'tried', 212: 'make', 213: 'anything', 214: 'four', 215: 'nice', 216: 'people', 217: 'please', 218: 'should', 219: 'remember', 220: 'turned', 221: 'other', 222: 'glass', 223: 'than', 224: 'marked', 225: 'best', 226: 'sat', 227: 'two', 228: 'english', 229: 'pair', 230: 'heard', 231: 'kid', 232: 'ready', 233: 'voice', 234: 'queer', 235: 'changed', 236: 'swam', 237: 'talk', 238: 'offended', 239: 'we', 240: 'party', 241: 'prizes', 242: 'chapter', 243: 'hole', 244: 'tired', 245: 'having', 246: 'pictures', 247: 'own', 248: 'mind', 249: 'hot', 250: 'day', 251: 'pocket', 252: 'hurried', 253: 'take', 254: 'another', 255: 'coming', 256: 'from', 257: 'fall', 258: 'even', 259: 'near', 260: 'idea', 261: 'among', 262: 'didn', 263: 'ask', 264: 'air', 265: 'perhaps', 266: 'else', 267: 'cat', 268: 'afraid', 269: 'bats', 270: 'rate', 271: 'those', 272: 'really', 273: 'bottle', 274: 'children', 275: 'enough', 276: 'anxiously', 277: 'surprised', 278: 'speak', 279: 'hastily', 280: 'duchess', 281: 'kept', 282: 'everything', 283: 'mabel', 284: 'capital', 285: 'tail', 286: 'something', 287: 'william', 288: 'pardon', 289: 'crowded', 290: 'whole', 291: 'last', 292: 'old', 293: 'him', 294: 'thimble', 295: 'close', 296: 'hear', 297: 'itself', 298: 'late', 299: 'ought', 300: 'under', 301: 'world', 302: 'deep', 303: 'slowly', 304: 'happen', 305: 'dark', 306: 'noticed', 307: 'somebody', 308: 'home', 309: 'end', 310: 'many', 311: 'fallen', 312: 'several', 313: 'lessons', 314: 'still', 315: 'distance', 316: 'seem', 317: 'walk', 318: 'heads', 319: 'name', 320: 'ma', 321: 'fancy', 322: 're', 323: 'manage', 324: 'night', 325: 'mice', 326: 'answer', 327: 'passage', 328: 'sight', 329: 'behind', 330: 'every', 331: 'three', 332: 'alas', 333: 'inches', 334: 'along', 335: 'bright', 336: 'happened', 337: 'few', 338: 'drink', 339: 'hurry', 340: 'poison', 341: 'finger', 342: 'almost', 343: 'certain', 344: 'finished', 345: 'curious', 346: 'face', 347: 'candle', 348: 'generally', 349: 'remembered', 350: 'box', 351: 'left', 352: 'person', 353: 'eye', 354: 'lying', 355: 'cake', 356: 'life', 357: 'deal', 358: 'dropped', 359: 'times', 360: 'seems', 361: 'stay', 362: 'being', 363: 'sudden', 364: 'cause', 365: 'water', 366: 'o', 367: 'history', 368: 'trembling', 369: 'always', 370: 'dogs', 371: 'eagerly', 372: 'fetch', 373: 'us', 374: 'duck', 375: 'eaglet', 376: 'caucus', 377: 'tale', 378: 'replied', 379: 'turning', 380: 'sister', 381: 'bank', 382: 'considering', 383: 'feel', 384: 'sleepy', 385: 'stupid', 386: 'whether', 387: 'making', 388: 'worth', 389: 'trouble', 390: 'remarkable', 391: 'natural', 392: 'watch', 393: 'waistcoat', 394: 'started', 395: 'across', 396: 'falling', 397: 'filled', 398: 'cupboards', 399: 'shelves', 400: 'saw', 401: 'jar', 402: 'fear', 403: 'managed', 404: 'wouldn', 405: 'top', 406: 'likely', 407: 'miles', 408: 'aloud', 409: 'somewhere', 410: 'earth', 411: 'though', 412: 'opportunity', 413: 'knowledge', 414: 'listen', 415: 'yes', 416: 'latitude', 417: 'longitude', 418: 'funny', 419: 'glad', 420: 'new', 421: 'spoke', 422: 'girl', 423: 'asking', 424: 'miss', 425: 'catch', 426: 'bat', 427: 'sometimes', 428: 'matter', 429: 'begun', 430: 'walking', 431: 'thump', 432: 'bit', 433: 'hurt', 434: 'lost', 435: 'corner', 436: 'ears', 437: 'whiskers', 438: 'row', 439: 'roof', 440: 'doors', 441: 'side', 442: 'walked', 443: 'sadly', 444: 'middle', 445: 'tiny', 446: 'opened', 447: 'led', 448: 'larger', 449: 'shut', 450: 'telescope', 451: 'knew', 452: 'waiting', 453: 'hoping', 454: 'rules', 455: 'shutting', 456: 'certainly', 457: 'beautifully', 458: 'hold', 459: 'usually', 460: 'forgotten', 461: 'taste', 462: 'finding', 463: 'fact', 464: 'feeling', 465: 'size', 466: 'waited', 467: 'minutes', 468: 'altogether', 469: 'reach', 470: 'crying', 471: 'sharply', 472: 'leave', 473: 'minute', 474: 'gave', 475: 'severely', 476: 'against', 477: 'child', 478: 'fond', 479: 'makes', 480: 'grow', 481: 'happens', 482: 'holding', 483: 'growing', 484: 'curiouser', 485: 'forgot', 486: 'far', 487: 'dears', 488: 'give', 489: 'sending', 490: 'foot', 491: 'nonsense', 492: 'nine', 493: 'cry', 494: 'yourself', 495: 'stop', 496: 'pattering', 497: 'trotting', 498: 'muttering', 499: 'help', 500: 'sir', 501: 'hard', 502: 'morning', 503: 'ah', 504: 'age', 505: 'hair', 506: 'ringlets', 507: 'mine', 508: 'sorts', 509: 'puzzling', 510: 'used', 511: 'paris', 512: 'rome', 513: 'doth', 514: 'hands', 515: 'alone', 516: 'done', 517: 'shrinking', 518: 'frightened', 519: 'change', 520: 'bad', 521: 'slipped', 522: 'salt', 523: 'sea', 524: 'case', 525: 'railway', 526: 'hadn', 527: 'suppose', 528: 'swimming', 529: 'speaking', 530: 'understand', 531: 'french', 532: 'conqueror', 533: 'lesson', 534: 'angry', 535: 'show', 536: 'paws', 537: 'nurse', 538: 'catching', 539: 'subject', 540: 'sit', 541: 'says', 542: 'useful', 543: 'shore', 544: 'hate', 545: 'animals', 546: 'fur', 547: 'wet', 548: 'better', 549: 'ring', 550: 'silence', 551: 'wanted', 552: 'edwin', 553: 'morcar', 554: 'earls', 555: 'mercia', 556: 'northumbria', 557: 'advisable', 558: 'meet', 559: 'melancholy', 560: 'solemnly', 561: 'explain', 562: 'running', 563: 'liked', 564: 'has', 565: 'chorus', 566: 'comfits', 567: 'speech', 568: 'caused', 569: 'sad', 570: 'fury', 571: 'trial', 572: 'jury', 573: 'judge', 574: 'finish', 575: 'story', 576: 'crab', 577: 'nobody', 578: 'ferrets', 579: 'hunting', 580: 'mary', 581: 'ann', 582: 'messages', 583: 'room', 584: 'beginning', 585: 'sitting', 586: 'twice', 587: 'peeped', 588: 'reading', 589: 'conversations', 590: 'pleasure', 591: 'daisy', 592: 'chain', 593: 'picking', 594: 'daisies', 595: 'pink', 596: 'nor', 597: 'afterwards', 598: 'occurred', 599: 'wondered', 600: 'actually', 601: 'flashed', 602: 'burning', 603: 'curiosity', 604: 'field', 605: 'fortunately', 606: 'pop', 607: 'hedge', 608: 'straight', 609: 'tunnel', 610: 'dipped', 611: 'stopping', 612: 'plenty', 613: 'sides', 614: 'maps', 615: 'hung', 616: 'pegs', 617: 'passed', 618: 'labelled', 619: 'orange', 620: 'marmalade', 621: 'disappointment', 622: 'empty', 623: 'drop', 624: 'killing', 625: 'past', 626: 'tumbling', 627: 'stairs', 628: 'brave', 629: 'true', 630: 'centre', 631: 'thousand', 632: 'learnt', 633: 'schoolroom', 634: 'showing', 635: 'practice', 636: 'grand', 637: 'presently', 638: 'downward', 639: 'antipathies', 640: 'listening', 641: 'sound', 642: 'word', 643: 'country', 644: 'zealand', 645: 'australia', 646: 'curtsey', 647: 'curtseying', 648: 'ignorant', 649: 'written', 650: 'hope', 651: 'saucer', 652: 'milk', 653: 'tea', 654: 'dreamy', 655: 'couldn', 656: 'dozing', 657: 'dream', 658: 'earnestly', 659: 'truth', 660: 'heap', 661: 'sticks', 662: 'leaves', 663: 'jumped', 664: 'overhead', 665: 'hurrying', 666: 'wind', 667: 'longer', 668: 'lit', 669: 'lamps', 670: 'hanging', 671: 'locked', 672: 'wondering', 673: 'legged', 674: 'solid', 675: 'except', 676: 'belong', 677: 'locks', 678: 'open', 679: 'second', 680: 'curtain', 681: 'fifteen', 682: 'lock', 683: 'delight', 684: 'fitted', 685: 'rat', 686: 'knelt', 687: 'loveliest', 688: 'longed', 689: 'wander', 690: 'beds', 691: 'flowers', 692: 'cool', 693: 'fountains', 694: 'doorway', 695: 'shoulders', 696: 'begin', 697: 'lately', 698: 'telescopes', 699: 'neck', 700: 'paper', 701: 'label', 702: 'printed', 703: 'wise', 704: 'read', 705: 'histories', 706: 'burnt', 707: 'eaten', 708: 'wild', 709: 'beasts', 710: 'unpleasant', 711: 'because', 712: 'simple', 713: 'friends', 714: 'taught', 715: 'red', 716: 'poker', 717: 'burn', 718: 'cut', 719: 'deeply', 720: 'knife', 721: 'bleeds', 722: 'disagree', 723: 'sooner', 724: 'later', 725: 'ventured', 726: 'mixed', 727: 'flavour', 728: 'cherry', 729: 'tart', 730: 'custard', 731: 'pine', 732: 'apple', 733: 'roast', 734: 'turkey', 735: 'toffee', 736: 'buttered', 737: 'toast', 738: 'ten', 739: 'brightened', 740: 'lovely', 741: 'shrink', 742: 'further', 743: 'nervous', 744: 'flame', 745: 'blown', 746: 'decided', 747: 'possibly', 748: 'plainly', 749: 'climb', 750: 'legs', 751: 'slippery', 752: 'advise', 753: 'advice', 754: 'seldom', 755: 'followed', 756: 'scolded', 757: 'bring', 758: 'cheated', 759: 'game', 760: 'croquet', 761: 'playing', 762: 'pretending', 763: 'pretend', 764: 'hardly', 765: 'respectable', 766: 'currants', 767: 'smaller', 768: 'creep', 769: 'care', 770: 'ate', 771: 'remained', 772: 'eats', 773: 'expecting', 774: 'dull', 775: 'common', 776: 'set', 777: 'work', 778: 'ii', 779: 'opening', 780: 'largest', 781: 'bye', 782: 'shoes', 783: 'stockings', 784: 'shan', 785: 'able', 786: 'myself', 787: 'kind', 788: 'want', 789: 'boots', 790: 'christmas', 791: 'planning', 792: 'carrier', 793: 'presents', 794: 'odd', 795: 'directions', 796: 'esq', 797: 'hearthrug', 798: 'fender', 799: 'love', 800: 'struck', 801: 'hopeless', 802: 'ashamed', 803: 'shedding', 804: 'gallons', 805: 'until', 806: 'reaching', 807: 'dried', 808: 'returning', 809: 'splendidly', 810: 'dressed', 811: 'himself', 812: 'savage', 813: 'desperate', 814: 'timid', 815: 'violently', 816: 'skurried', 817: 'darkness', 818: 'fanning', 819: 'yesterday', 820: 'usual', 821: 'different', 822: 'puzzle', 823: 'thinking', 824: 'ada', 825: 'goes', 826: 'knows', 827: 'besides', 828: 'five', 829: 'twelve', 830: 'six', 831: 'thirteen', 832: 'seven', 833: 'twenty', 834: 'multiplication', 835: 'signify', 836: 'geography', 837: 'london', 838: 'wrong', 839: 'crossed', 840: 'lap', 841: 'repeat', 842: 'sounded', 843: 'hoarse', 844: 'strange', 845: 'crocodile', 846: 'improve', 847: 'shining', 848: 'pour', 849: 'waters', 850: 'nile', 851: 'scale', 852: 'cheerfully', 853: 'grin', 854: 'neatly', 855: 'spread', 856: 'claws', 857: 'welcome', 858: 'fishes', 859: 'gently', 860: 'smiling', 861: 'jaws', 862: 'live', 863: 'poky', 864: 'toys', 865: 'play', 866: 'learn', 867: 'putting', 868: 'till', 869: 'burst', 870: 'measure', 871: 'nearly', 872: 'guess', 873: 'rapidly', 874: 'avoid', 875: 'narrow', 876: 'escape', 877: 'existence', 878: 'speed', 879: 'worse', 880: 'declare', 881: 'these', 882: 'splash', 883: 'chin', 884: 'somehow', 885: 'seaside', 886: 'general', 887: 'conclusion', 888: 'wherever', 889: 'coast', 890: 'number', 891: 'bathing', 892: 'machines', 893: 'digging', 894: 'sand', 895: 'wooden', 896: 'spades', 897: 'lodging', 898: 'houses', 899: 'station', 900: 'wept', 901: 'punished', 902: 'drowned', 903: 'splashing', 904: 'nearer', 905: 'walrus', 906: 'hippopotamus', 907: 'harm', 908: 'brother', 909: 'latin', 910: 'grammar', 911: 'inquisitively', 912: 'wink', 913: 'daresay', 914: 'clear', 915: 'notion', 916: 'ago', 917: 'ou', 918: 'est', 919: 'chatte', 920: 'sentence', 921: 'leap', 922: 'quiver', 923: 'fright', 924: 'animal', 925: 'feelings', 926: 'shrill', 927: 'passionate', 928: 'soothing', 929: 'yet', 930: 'quiet', 931: 'lazily', 932: 'sits', 933: 'purring', 934: 'nicely', 935: 'fire', 936: 'licking', 937: 'washing', 938: 'soft', 939: 'bristling', 940: 'family', 941: 'hated', 942: 'nasty', 943: 'vulgar', 944: 'conversation', 945: 'dog', 946: 'eyed', 947: 'terrier', 948: 'curly', 949: 'brown', 950: 'throw', 951: 'dinner', 952: 'belongs', 953: 'farmer', 954: 'hundred', 955: 'pounds', 956: 'kills', 957: 'rats', 958: 'sorrowful', 959: 'commotion', 960: 'softly', 961: 'pale', 962: 'passion', 963: 'creatures', 964: 'iii', 965: 'assembled', 966: 'draggled', 967: 'feathers', 968: 'clinging', 969: 'dripping', 970: 'cross', 971: 'uncomfortable', 972: 'consultation', 973: 'familiarly', 974: 'known', 975: 'argument', 976: 'sulky', 977: 'older', 978: 'allow', 979: 'knowing', 980: 'positively', 981: 'refused', 982: 'authority', 983: 'fixed', 984: 'cold', 985: 'ahem', 986: 'important', 987: 'driest', 988: 'whose', 989: 'favoured', 990: 'pope', 991: 'submitted', 992: 'leaders', 993: 'accustomed', 994: 'usurpation', 995: 'conquest', 996: 'ugh', 997: 'shiver', 998: 'frowning', 999: 'politely', 1000: 'proceed', 1001: 'declared', 1002: 'stigand', 1003: 'patriotic', 1004: 'archbishop', 1005: 'canterbury', 1006: 'crossly', 1007: 'means', 1008: 'frog', 1009: 'worm', 1010: 'notice', 1011: 'hurriedly', 1012: 'edgar', 1013: 'atheling', 1014: 'offer', 1015: 'crown', 1016: 'conduct', 1017: 'moderate', 1018: 'insolence', 1019: 'normans', 1020: 'continued', 1021: 'rising', 1022: 'move', 1023: 'meeting', 1024: 'adjourn', 1025: 'immediate', 1026: 'adoption', 1027: 'energetic', 1028: 'meaning', 1029: 'believe', 1030: 'bent', 1031: 'hide', 1032: 'smile', 1033: 'paused', 1034: 'inclined', 1035: 'winter', 1036: 'circle', 1037: 'exact', 1038: 'shape', 1039: 'placed', 1040: 'easy', 1041: 'hour', 1042: 'panting', 1043: 'pressed', 1044: 'forehead', 1045: 'position', 1046: 'shakespeare', 1047: 'rest', 1048: 'everybody', 1049: 'voices', 1050: 'asked', 1051: 'pointing', 1052: 'calling', 1053: 'confused', 1054: 'despair', 1055: 'pulled', 1056: 'luckily', 1057: 'handed', 1058: 'exactly', 1059: 'piece', 1060: 'prize', 1061: 'gravely', 1062: 'presented', 1063: 'acceptance', 1064: 'elegant', 1065: 'short', 1066: 'cheered', 1067: 'absurd', 1068: 'grave', 1069: 'dare', 1070: 'laugh', 1071: 'simply', 1072: 'bowed', 1073: 'solemn', 1074: 'noise', 1075: 'confusion', 1076: 'complained', 1077: 'theirs', 1078: 'ones', 1079: 'choked', 1080: 'patted', 1081: 'begged', 1082: 'promised', 1083: 'c', 1084: 'added', 1085: 'whisper', 1086: 'call', 1087: 'met', 1088: 'both', 1089: 'law', 1090: 'prosecute', 1091: 'denial', 1092: 'cur', 1093: 'wasting', 1094: 'breath', 1095: 'cunning', 1096: 'condemn', 1097: 'death', 1098: 'attending', 1099: 'humbly', 1100: 'fifth', 1101: 'bend', 1102: 'angrily', 1103: 'knot', 1104: 'undo', 1105: 'insult', 1106: 'mean', 1107: 'pleaded', 1108: 'easily', 1109: 'growled', 1110: 'reply', 1111: 'others', 1112: 'joined', 1113: 'shook', 1114: 'impatiently', 1115: 'quicker', 1116: 'pity', 1117: 'sighed', 1118: 'daughter', 1119: 'lose', 1120: 'temper', 1121: 'tongue', 1122: 'young', 1123: 'snappishly', 1124: 'patience', 1125: 'oyster', 1126: 'addressing', 1127: 'particular', 1128: 'venture', 1129: 'pet', 1130: 'bird', 1131: 'sensation', 1132: 'magpie', 1133: 'wrapping', 1134: 'carefully', 1135: 'remarking', 1136: 'suit', 1137: 'throat', 1138: 'canary', 1139: 'bed', 1140: 'various', 1141: 'pretexts', 1142: 'moved', 1143: 'mentioned', 1144: 'lonely', 1145: 'spirited', 1146: 'footsteps', 1147: 'iv', 1148: 'sends', 1149: 'bill', 1150: 'executed', 1151: 'where', 1152: 'guessed', 1153: 'naturedly', 1154: 'nowhere', 1155: 'since', 1156: 'swim', 1157: 'vanished', 1158: 'completely', 1159: 'doing', 1160: 'run', 1161: 'quick', 1162: 'direction', 1163: 'pointed', 1164: 'mistake', 1165: 'housemaid', 1166: 'finds', 1167: 'neat', 1168: 'brass', 1169: 'plate', 1170: 'w', 1171: 'engraved', 1172: 'knocking', 1173: 'upstairs', 1174: 'lest', 1175: 'real', 1176: 'fancying', 1177: 'directly', 1178: 'ordering', 1179: 'tidy', 1180: 'window', 1181: 'hoped', 1182: 'pairs'}\n"
     ]
    }
   ],
   "source": [
    "# create inverted index to help recover the words from indexes\n",
    "inverted_index = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    inverted_index[i] = word\n",
    "    \n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "3\n",
      "[[0.33333333 0.66666667]]\n",
      "int64\n",
      "[[1 2]]\n",
      "7\n",
      "[[0.42857143 0.57142857]]\n",
      "int64\n",
      "[[3 4]]\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# helper snippet\n",
    "################################\n",
    "\n",
    "m = np.matrix([[1, 2], [3, 4]])\n",
    "print(m)\n",
    "(rows, columns) = m.shape\n",
    "for rowIdx in range(rows):\n",
    "    s = m[rowIdx]\n",
    "    total = s.sum()\n",
    "    print(s.sum())\n",
    "    print(np.divide(s, total))\n",
    "    print(s.dtype)\n",
    "    print(s.flatten())\n",
    "    \n",
    "    #for value in m[rowIdx, :]:\n",
    "    #    print(value)\n",
    "    #print(m[rowIdx, :])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[797]\n",
      "[1090]\n",
      "[1093]\n",
      "[1094]\n",
      "[1095]\n",
      "[1096]\n",
      "[1097]\n",
      "{'duck': 374, 'begged': 1081, 'nearly': 871, 'advise': 752, 'have': 84, 'fear': 402, 'heap': 660, 'beasts': 709, 'drink': 338, 'met': 1087, 'choked': 1079, 'chorus': 565, 'pairs': 1182, 'simply': 1071, 'w': 1170, 'way': 45, 'longed': 688, 'leap': 921, 'politely': 999, 'then': 79, 'odd': 794, 'neatly': 854, 'key': 148, 'gave': 474, 'red': 715, 'shrill': 926, 'swim': 1156, 'growled': 1109, 'english': 228, 'hour': 1041, 'continued': 1020, 'nicely': 934, 'side': 441, 'these': 881, 'country': 643, 'shrinking': 517, 'claws': 856, 'jar': 401, 'stairs': 627, 'bed': 1139, 'hippopotamus': 906, 'i': 6, 'bathing': 891, 'hanging': 670, 'despair': 1054, 'conversation': 944, 'times': 359, 'remember': 219, 'sharply': 471, 'miles': 407, 'fur': 546, 'find': 150, 'australia': 645, 'playing': 761, 'getting': 138, 'dark': 305, 'harm': 907, 'various': 1140, 'sight': 328, 'nothing': 112, 'trying': 187, 'four': 214, 'boots': 789, 'child': 477, 'speech': 567, 'things': 93, 'hear': 296, 'made': 206, 'ashamed': 802, 'ii': 778, 'crossly': 1006, 'indeed': 190, 'found': 73, 'asked': 1050, 'piece': 1059, 'conversations': 589, 'seem': 316, 'fell': 209, 'book': 205, 'normans': 1019, 'sleepy': 384, 'right': 158, 'authority': 982, 'course': 204, 'else': 266, 'alone': 515, 'hall': 147, 'cherry': 728, 'short': 1065, 'sir': 500, 'ah': 503, 'condemn': 1096, 'ever': 119, 'shining': 847, 'escape': 876, 'fond': 478, 'dull': 774, 'toys': 864, 'tired': 244, 'shakespeare': 1046, 'hide': 1031, 'chapter': 242, 'carrier': 792, 'iv': 1147, 'go': 82, 'tart': 729, 'sits': 932, 'temper': 1120, 'wander': 689, 'voices': 1049, 'puzzling': 509, 'brown': 949, 'timid': 814, 'others': 1111, 'likely': 406, 'quiver': 922, 'shelves': 399, 'pardon': 288, 'led': 447, 'yourself': 494, 'fishes': 858, 'mentioned': 1143, 'wondering': 672, 'patience': 1124, 'means': 1007, 'far': 486, 'somebody': 307, 'legged': 673, 'offer': 1014, 'jaws': 861, 'she': 4, 'crowded': 289, 'eyes': 176, 'cats': 107, 'delight': 683, 'passage': 327, 'winter': 1035, 'fixed': 983, 'toast': 737, 'pattering': 496, 'geography': 836, 'sentence': 920, 'some': 177, 'cool': 692, 'added': 1084, 'burning': 602, 'size': 465, 'splashing': 903, 'capital': 284, 'maps': 614, 'he': 111, 'twenty': 833, 'foot': 490, 'completely': 1158, 'finding': 462, 'really': 272, 'fifth': 1100, 'paper': 700, 'carefully': 1134, 'am': 160, 'sorts': 508, 'seen': 208, 'sit': 540, 'sand': 894, 'canary': 1138, 'commotion': 959, 'cheered': 1066, 'herself': 41, 'conclusion': 887, 'conduct': 1016, 'pine': 731, 'is': 33, 'behind': 329, 'curiosity': 603, 'pet': 1129, 'mice': 325, 'either': 140, 'bring': 757, 'prosecute': 1090, 'downward': 638, 'stupid': 385, 'only': 122, 'falling': 396, 'bank': 381, 'known': 974, 'refused': 981, 'jumped': 663, 'knew': 451, 'was': 9, 'pressed': 1043, 'offended': 238, 'once': 98, 'cut': 718, 'yesterday': 819, 'older': 977, 'signify': 835, 'lost': 434, 'pale': 961, 'denial': 1091, 'sound': 641, 'please': 217, 'housemaid': 1165, 'atheling': 1013, 'answer': 326, 'rest': 1047, 'again': 54, 'kept': 281, 'being': 362, 'across': 395, 'sadly': 443, 'use': 152, 'earnestly': 658, 'eat': 134, 'earth': 410, 'insolence': 1018, 'clear': 914, 'able': 785, 'belongs': 952, 'seldom': 754, 'slowly': 303, 'duchess': 280, 'seaside': 885, 'nile': 850, 'afterwards': 597, 'saying': 183, 'grave': 1068, 'said': 16, 'age': 504, 'consultation': 972, 'dare': 1069, 'did': 99, 'five': 828, 'are': 106, 'high': 168, 'elegant': 1064, 'lazily': 931, 'reading': 588, 'history': 367, 'we': 239, 'saucer': 651, 'brother': 908, 'usual': 820, 'one': 48, 'calling': 1052, 'started': 394, 'curtain': 680, 't': 35, 'got': 118, 'kid': 231, 'addressing': 1126, 'close': 295, 'turkey': 734, 'hair': 505, 'dry': 164, 'strange': 844, 'thinking': 823, 'let': 105, 'hoarse': 843, 'back': 110, 'new': 420, 'golden': 188, 'opportunity': 412, 'ears': 436, 'sea': 523, 'solid': 674, 'speed': 878, 'number': 890, 'comfits': 566, 'animals': 545, 'hurried': 252, 'grow': 480, 'knowing': 979, 'natural': 391, 'knocking': 1172, 'larger': 448, 'ou': 917, 'struck': 800, 'hot': 249, 'adoption': 1026, 'many': 310, 'hope': 650, 'life': 356, 'burn': 717, 'sister': 380, 'daresay': 913, 'custard': 730, 'crocodile': 845, 'eaglet': 375, 'about': 40, 'lory': 201, 'farmer': 953, 'suit': 1136, 'glass': 222, 'curiouser': 484, 'simple': 712, 'shook': 1113, 'game': 759, 'leaves': 662, 'if': 32, 'beg': 195, 'table': 120, 'dinah': 87, 'play': 865, 'lamps': 669, 'upon': 142, 'pink': 595, 'think': 60, 'sensation': 1131, 'rats': 957, 'flavour': 727, 'night': 324, 'purring': 933, 'notion': 915, 'centre': 630, 'latitude': 416, 'largest': 780, 'enough': 275, 'field': 604, 'be': 20, 'creep': 768, 'young': 1122, 'cold': 984, 'immediate': 1025, 'bye': 781, 'considering': 382, 'bill': 1149, 'believe': 1029, 'twelve': 829, 'managed': 403, 'watch': 392, 'middle': 444, 'sorrowful': 958, 'argument': 975, 'holding': 482, 'sitting': 585, 'help': 499, 'pretending': 762, 'crown': 1015, 'possibly': 747, 'tears': 170, 'd': 198, 'hopeless': 801, 'even': 258, 'walking': 430, 'over': 100, 'crossed': 839, 'were': 65, 'put': 178, 'muttering': 498, 'flowers': 691, 'trouble': 389, 'esq': 796, 'throw': 950, 'cause': 364, 'death': 1097, 'anxiously': 276, 'few': 337, 'station': 899, 'except': 675, 'hurt': 433, 'catch': 425, 'his': 151, 'hand': 185, 'more': 123, 'angry': 534, 'loveliest': 687, 'hoping': 453, 'edgar': 1012, 'patted': 1080, 'wet': 547, 'turned': 220, 'of': 8, 'c': 1083, 'chin': 883, 'glad': 419, 'ago': 916, 'those': 271, 'won': 172, 'shape': 1038, 'schoolroom': 633, 'curly': 948, 'fanning': 818, 'directions': 795, 'pictures': 246, 'understand': 530, 'saw': 400, 'very': 27, 'legs': 750, 'heard': 230, 'throat': 1137, 'deep': 302, 'real': 1175, 'lose': 1119, 'such': 74, 'fall': 257, 'dripping': 969, 'us': 373, 'whisper': 1085, 'tidy': 1179, 'wrapping': 1133, 'low': 186, 'moment': 131, 'through': 144, 'dreamy': 654, 'smiling': 860, 'judge': 573, 'ignorant': 648, 'generally': 348, 'ones': 1078, 'every': 330, 'says': 541, 'end': 309, 'altogether': 468, 'important': 986, 'own': 247, 'wish': 145, 'curtseying': 647, 'race': 202, 'fury': 570, 'archbishop': 1004, 'impatiently': 1114, 'inquisitively': 911, 'nearer': 904, 'used': 510, 'driest': 987, 'row': 438, 'stopping': 611, 'whiskers': 437, 'dressed': 810, 'overhead': 664, 'see': 56, 'vulgar': 943, 'while': 192, 'confusion': 1075, 'planning': 791, 'sat': 226, 'home': 308, 'slippery': 751, 'happen': 304, 'common': 775, 'tell': 146, 'lock': 682, 'plenty': 612, 'adjourn': 1024, 'moderate': 1017, 'histories': 705, 'too': 155, 'old': 292, 'advisable': 557, 'repeat': 841, 'executed': 1150, 'better': 548, 'tongue': 1121, 'label': 701, 'whether': 386, 'daughter': 1118, 'mary': 580, 'shore': 543, 'thimble': 294, 'finished': 344, 'presented': 1062, 'grin': 853, 'frightened': 518, 'eaten': 707, 'feel': 383, 'happened': 336, 'down': 26, 'fancying': 1176, 'alice': 11, 'past': 625, 'head': 189, 'picking': 593, 'lovely': 740, 'since': 1155, 'liked': 563, 'nine': 492, 'm': 77, 'mean': 1106, 'hearthrug': 797, 'naturedly': 1153, 'care': 769, 'for': 18, 'thousand': 631, 'half': 149, 'happens': 481, 'paws': 536, 'caucus': 376, 'pour': 848, 'curious': 345, 'time': 66, 'girl': 422, 'much': 91, 'severely': 475, 'earls': 554, 'off': 80, 'roof': 439, 'soft': 938, 'undo': 1104, 'nasty': 942, 'waiting': 452, 'subject': 539, 'skurried': 816, 'burst': 869, 'showing': 634, 'longitude': 417, 'telescopes': 698, 'what': 44, 'bent': 1030, 'cheated': 758, 'solemn': 1073, 'occurred': 598, 'left': 351, 'brightened': 739, 'railway': 525, 'morcar': 553, 'plainly': 748, 'almost': 342, 'followed': 755, 'ten': 738, 'miss': 424, 'nor': 596, 'desperate': 813, 'all': 22, 'them': 61, 'make': 212, 'sounded': 842, 'doesn': 173, 'rome': 512, 'the': 1, 'written': 649, 'read': 704, 'pleaded': 1107, 'where': 1151, 'presently': 637, 'hadn': 526, 'northumbria': 556, 'nice': 215, 'house': 156, 'must': 69, 'usually': 459, 'stay': 361, 'handed': 1057, 'shrink': 741, 'ask': 263, 'words': 159, 'reply': 1110, 'funny': 418, 'machines': 892, 'lest': 1174, 'friends': 713, 'salt': 522, 'noise': 1074, 'brave': 628, 'besides': 827, 'rate': 270, 'washing': 937, 'existence': 877, 'cheerfully': 852, 'pop': 606, 'by': 71, 'meeting': 1023, 'measure': 870, 'dogs': 370, 'tiny': 445, 'pointed': 1163, 'christmas': 790, 'thing': 89, 'true': 629, 'wrong': 838, 'easy': 1040, 'poor': 109, 'roast': 733, 'suddenly': 175, 'somewhere': 409, 'certainly': 456, 'walrus': 905, 'soothing': 928, 'story': 575, 'digging': 893, 'can': 97, 'everybody': 1048, 'which': 85, 'clinging': 968, 'seven': 832, 'shall': 92, 'notice': 1010, 'useful': 542, 'always': 369, 'among': 261, 'currants': 766, 'insult': 1105, 'ann': 581, 'passed': 617, 'three': 331, 'chatte': 919, 'world': 301, 'quiet': 930, 'walk': 317, 'chain': 592, 'ventured': 725, 'hardly': 764, 'took': 139, 'your': 88, 'sure': 136, 'solemnly': 560, 'minutes': 467, 'brass': 1168, 'dozing': 656, 'bad': 520, 'hated': 941, 'further': 742, 'why': 132, 'because': 711, 'pegs': 616, 'might': 161, 'everything': 282, 'but': 19, 're': 322, 'run': 1160, 'pope': 990, 'neck': 699, 'candle': 347, 'you': 12, 'lesson': 533, 'distance': 315, 'forgotten': 460, 'narrow': 875, 'printed': 702, 'into': 72, 'wasting': 1093, 'hard': 501, 'good': 157, 'corner': 435, 'goes': 825, 'empty': 622, 'that': 13, 'begin': 696, 'advice': 753, 'ate': 770, 'rising': 1021, 've': 143, 'an': 133, 'window': 1180, 'work': 777, 'till': 868, 'name': 319, 'drowned': 902, 'spoke': 421, 'fire': 935, 'seems': 360, 'beds': 690, 'punished': 901, 'ferrets': 578, 'wherever': 888, 'returning': 808, 'buttered': 736, 'door': 95, 'breath': 1094, 'next': 210, 'seemed': 113, 'our': 197, 'than': 223, 'splash': 882, 'joined': 1112, 'allow': 978, 'shedding': 803, 's': 36, 'cry': 493, 'guessed': 1152, 'flashed': 601, 'poker': 716, 'turning': 379, 'opening': 779, 'puzzle': 822, 'tail': 285, 'dear': 64, 'waited': 466, 'my': 70, 'began': 75, 'dipped': 610, 'me': 42, 'frowning': 998, 'suppose': 527, 'caused': 568, 'however': 135, 'tea': 653, 'noticed': 306, 'wind': 666, 'engraved': 1171, 'william': 287, 'any': 121, 'deal': 357, 'cat': 267, 'rules': 454, 'placed': 1039, 'running': 562, 'bird': 1130, 'peeped': 587, 'sad': 569, 'multiplication': 834, 'guess': 872, 'truth': 659, 'set': 776, 'hastily': 279, 'filled': 397, 'came': 163, 'waistcoat': 393, 'went': 52, 'rather': 181, 'sudden': 363, 'open': 678, 'fitted': 684, 'paused': 1033, 'patriotic': 1003, 'accustomed': 993, 'feeling': 464, 'late': 298, 'her': 14, 'hurriedly': 1011, 'party': 240, 'when': 46, 'softly': 960, 'marked': 224, 'white': 128, 'exactly': 1058, 'directly': 1177, 'explain': 561, 'children': 274, 'matter': 428, 'curtsey': 646, 'live': 862, 'animal': 924, 'dog': 945, 'speaking': 529, 'crying': 470, 'soon': 76, 'without': 174, 'try': 194, 'vanished': 1157, 'practice': 635, 'second': 679, 'get': 67, 'doing': 1159, 'large': 153, 'mixed': 726, 'trotting': 497, 'panting': 1042, 'do': 49, 'another': 254, 'luckily': 1056, 'swam': 236, 'edwin': 552, 'climb': 749, 'how': 38, 'him': 293, 'begun': 429, 'antipathies': 639, 'here': 68, 'flame': 744, 'daisy': 591, 'canterbury': 1005, 'never': 115, 'hands': 514, 'cunning': 1095, 'sort': 179, 'll': 37, 'usurpation': 994, 'hedge': 607, 'deeply': 719, 'hoped': 1181, 'poison': 340, 'called': 199, 'killing': 624, 'fright': 923, 'rat': 685, 'up': 43, 'gently': 859, 'reaching': 806, 'no': 50, 'pair': 229, 'couldn': 655, 'drop': 623, 'ada': 824, 'hate': 544, 'inclined': 1034, 'a': 5, 'different': 821, 'trial': 571, 'draggled': 966, 'lessons': 313, 'would': 59, 'iii': 964, 'latin': 909, 'yet': 929, 'bend': 1101, 'nurse': 537, 'say': 83, 'it': 7, 'learnt': 632, 'and': 2, 'humbly': 1099, 'eye': 353, 'poky': 863, 'quite': 78, 'walked': 442, 'hung': 615, 'assembled': 965, 'itself': 297, 'thirteen': 831, 'anything': 213, 'knelt': 686, 'word': 642, 'wink': 912, 'oyster': 1125, 'making': 387, 'rabbit': 63, 'looking': 203, 'nervous': 743, 'zealand': 644, 'jury': 572, 'proceed': 1000, 'feet': 101, 'knife': 720, 'ma': 320, 'bowed': 1072, 'position': 1045, 'twice': 586, 'top': 405, 'orange': 619, 'eyed': 946, 'ran': 207, 'to': 3, 'stigand': 1002, 'listening': 640, 'eagerly': 371, 'favoured': 989, 'passionate': 927, 'footsteps': 1146, 'daisies': 594, 'positively': 980, 'doth': 513, 'heads': 318, 'upstairs': 1173, 'pretend': 763, 'both': 1088, 'tale': 377, 'taste': 461, 'near': 259, 'same': 193, 'pointing': 1051, 'until': 805, 'first': 104, 'dried': 807, 'has': 564, 'show': 535, 'looked': 129, 'who': 96, 'mabel': 283, 'leaders': 992, 'fan': 126, 'thought': 53, 'disagree': 722, 'shutting': 455, 'love': 799, 'with': 23, 'uncomfortable': 971, 'tried': 211, 'waters': 849, 'growing': 483, 'beautifully': 457, 'moved': 1142, 'trembling': 368, 'case': 524, 'finds': 1166, 'afraid': 268, 'easily': 1108, 'felt': 184, 'shoulders': 695, 'small': 167, 'forgot': 485, 'whole': 290, 'spades': 896, 'lonely': 1144, 'promised': 1082, 'on': 21, 'move': 1022, 'bats': 269, 'stockings': 783, 'wanted': 551, 'inches': 333, 'frog': 1008, 'sending': 489, 'welcome': 857, 'energetic': 1027, 'several': 312, 'want': 788, 'familiarly': 973, 'particular': 1127, 'could': 58, 'sometimes': 427, 'talking': 182, 'sends': 1148, 'labelled': 618, 'grand': 636, 'wise': 703, 'worth': 388, 'smile': 1032, 'oh': 51, 'manage': 323, 'ought': 299, 'pity': 1116, 'wonder': 103, 'improve': 846, 'wept': 900, 'slipped': 521, 'toffee': 735, 'sticks': 661, 'spirited': 1145, 'six': 830, 'remarking': 1135, 'quick': 1161, 'under': 300, 'tone': 196, 'opened': 446, 'shoes': 782, 'idea': 260, 'call': 1086, 'grammar': 910, 'passion': 962, 'marmalade': 620, 'along': 334, 'alas': 332, 'minute': 473, 'two': 227, 'fancy': 321, 'question': 162, 'scale': 851, 'nobody': 577, 'doorway': 694, 'beginning': 584, 'creatures': 963, 'lying': 354, 'shiver': 997, 'fact': 463, 'rapidly': 873, 'ringlets': 506, 'knot': 1103, 'hurrying': 665, 'gloves': 125, 'unpleasant': 710, 'coming': 255, 'out': 31, 'their': 180, 'spread': 855, 'other': 221, 'nonsense': 491, 'aloud': 408, 'milk': 652, 'doors': 440, 'long': 94, 'learn': 866, 'quicker': 1115, 'little': 24, 'had': 17, 'ready': 232, 'direction': 1162, 'bit': 432, 'feathers': 967, 'magpie': 1132, 'hundred': 954, 'at': 34, 'sides': 613, 'bat': 426, 'look': 154, 'declared': 1001, 'fifteen': 681, 'surprised': 277, 'fetch': 372, 'cupboards': 398, 'having': 245, 'asking': 423, 'absurd': 1067, 'ugh': 996, 'kind': 787, 'ahem': 985, 'laugh': 1070, 'bristling': 939, 'not': 29, 'in': 10, 'or': 57, 'sulky': 976, 'neat': 1167, 'himself': 811, 'know': 62, 'face': 346, 'est': 918, 'shan': 784, 'longer': 667, 'well': 127, 'ordering': 1178, 'savage': 812, 'venture': 1128, 'stop': 495, 'perhaps': 265, 'nowhere': 1154, 'hole': 243, 'later': 724, 'family': 940, 'morning': 502, 'smaller': 767, 'remained': 771, 'knows': 826, 'certain': 343, 'o': 366, 'wild': 708, 'cake': 355, 'garden': 169, 'theirs': 1077, 'houses': 898, 'burnt': 706, 'conqueror': 532, 'they': 39, 'remembered': 349, 'this': 28, 'pocket': 251, 'dodo': 90, 'snappishly': 1123, 'locked': 671, 'as': 15, 'expecting': 773, 'straight': 608, 'hurry': 339, 'terrier': 947, 'presents': 793, 'cried': 124, 'london': 837, 'mine': 507, 'attending': 1098, 'myself': 786, 'meet': 558, 'last': 291, 'catching': 538, 'didn': 262, 'don': 171, 'lodging': 897, 'kills': 956, 'though': 411, 'pool': 137, 'cur': 1092, 'round': 108, 'going': 116, 'meaning': 1028, 'something': 286, 'coast': 889, 'dream': 657, 'should': 218, 'crab': 576, 'ring': 549, 'licking': 936, 'like': 47, 'eats': 772, 'blown': 745, 'fender': 798, 'mind': 248, 'worse': 879, 'air': 264, 'from': 256, 'dinner': 951, 'take': 253, 'putting': 867, 'fortunately': 605, 'law': 1089, 'taught': 714, 'against': 476, 'come': 86, 'violently': 815, 'somehow': 884, 'decided': 746, 'lit': 668, 'leave': 472, 'people': 216, 'will': 191, 'replied': 378, 'acceptance': 1063, 'submitted': 991, 'lap': 840, 'birds': 200, 'before': 130, 'gallons': 804, 'gravely': 1061, 'listen': 414, 'plate': 1169, 'bottle': 273, 'hunting': 579, 'disappointment': 621, 'lately': 697, 'its': 114, 'changed': 235, 'away': 165, 'tumbling': 626, 'water': 365, 'whose': 988, 'circle': 1036, 'worm': 1009, 'prize': 1060, 'feelings': 925, 'locks': 677, 'melancholy': 559, 'pleasure': 590, 'wouldn': 404, 'cross': 970, 'remarkable': 390, 'general': 886, 'finger': 341, 'swimming': 528, 'talk': 237, 'fallen': 311, 'scolded': 756, 'give': 488, 'knowledge': 413, 'dropped': 358, 'conquest': 995, 'respectable': 765, 'telescope': 450, 'hold': 458, 'there': 55, 'makes': 479, 'darkness': 817, 'fountains': 693, 'paris': 511, 'box': 350, 'tunnel': 609, 'dears': 487, 'thump': 431, 'sighed': 1117, 'been': 166, 'now': 81, 'person': 352, 'prizes': 241, 'silence': 550, 'exact': 1037, 'change': 519, 'messages': 582, 'actually': 600, 'great': 117, 'done': 516, 'room': 583, 'confused': 1053, 'shut': 449, 'pounds': 955, 'wondered': 599, 'speak': 278, 'after': 102, 'french': 531, 'just': 141, 'sooner': 723, 'complained': 1076, 'day': 250, 'avoid': 874, 'angrily': 1102, 'pulled': 1055, 'still': 314, 'bleeds': 721, 'splendidly': 809, 'best': 225, 'reach': 469, 'pretexts': 1141, 'finish': 574, 'voice': 233, 'forehead': 1044, 'apple': 732, 'yes': 415, 'wooden': 895, 'mistake': 1164, 'mercia': 555, 'so': 30, 'mouse': 25, 'croquet': 760, 'belong': 676, 'queer': 234, 'declare': 880, 'bright': 335}\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# helper snippet \n",
    "############################\n",
    "nan_indexes = [797, 1090, 1093, 1094, 1095, 1096, 1097]\n",
    "for sequence in corpus:\n",
    "    for nan_index in nan_indexes:\n",
    "        if nan_index in sequence:\n",
    "            print(sequence)\n",
    "            break\n",
    "    \n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create co-occurrence matrix\n",
    "#print(corpus[0])\n",
    "coMatrix = np.zeros((V, V))\n",
    "for sequence in corpus:\n",
    "    for idx, termId in enumerate(sequence):\n",
    "        sl = len(sequence)\n",
    "        ## select left window\n",
    "        leftw = sequence[max(idx - window_size_corpus, 0): idx]\n",
    "        ## select right window\n",
    "        rightw = sequence[idx + 1: min(idx + window_size_corpus + 1, sl)]\n",
    "        # update co-occurrence matrix\n",
    "        neighboors = leftw + rightw\n",
    "        for neighbor in neighboors:\n",
    "            coMatrix[termId, neighbor] += 1\n",
    "        \n",
    "\n",
    "# matrix normalization\n",
    "# TODO: check this normalization\n",
    "(rows, columns) = coMatrix.shape\n",
    "for rowIdx in range(rows):\n",
    "    # ignore first row with 0 entries everywhere\n",
    "    if (rowIdx > 0):\n",
    "        row = coMatrix[rowIdx]\n",
    "        total = row.sum()\n",
    "        # avoid division by zero in words that have no neighboors\n",
    "        if total > 0:\n",
    "            coMatrix[rowIdx] = np.divide(row, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02363823 0.0364851  ... 0.00051387 0.         0.        ]\n",
      " [0.         0.04752343 0.01204819 ... 0.00066934 0.00066934 0.00066934]\n",
      " ...\n",
      " [0.         0.16666667 0.16666667 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.125      ... 0.         0.         0.        ]\n",
      " [0.         0.         0.25       ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(coMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.02646503 0.03402647 ... 0.         0.         0.        ]]\n",
      "[[0.         0.03448276 0.03448276 ... 0.         0.         0.        ]]\n",
      "[[0.         0.13559322 0.00847458 ... 0.         0.         0.        ]]\n",
      "Cosine similarity between Alice and Dinah: [[0.39360011]]\n",
      "Cosine similarity between Alice and Rabbit: [[0.47890931]]\n",
      "Cosine similarity between Dinah and Rabbit: [[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "\n",
    "#find the word vectors for Alice, Dinah, and Rabbit\n",
    "aliceIdx = tokenizer.word_index['Alice'.lower()]\n",
    "dinahIdx = tokenizer.word_index['Dinah'.lower()]\n",
    "rabbitIdx = tokenizer.word_index['Rabbit'.lower()]\n",
    "\n",
    "aliceVector = coMatrix[aliceIdx].reshape(1, -1)\n",
    "dinahVector = coMatrix[dinahIdx].reshape(1, -1)\n",
    "rabbitVector = coMatrix[rabbitIdx].reshape(1, -1)\n",
    "print(aliceVector)\n",
    "print(dinahVector)\n",
    "print(rabbitVector)\n",
    "\n",
    "cosAD = cosine_similarity(aliceVector, dinahVector)\n",
    "print(\"Cosine similarity between Alice and Dinah: {}\".format(cosAD))\n",
    "cosAR = cosine_similarity(aliceVector, rabbitVector)\n",
    "print(\"Cosine similarity between Alice and Rabbit: {}\".format(cosAR))\n",
    "cosDR = cosine_similarity(dinahVector, rabbitVector)\n",
    "print(\"Cosine similarity between Dinah and Rabbit: {}\".format(cosDR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.18693993 0.22482059 0.25996004 0.26423806 0.2653115 ]]\n",
      "[[11  4  7  1  5 41]]\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "# create an array containing the cosine similarity values for alice and the rest\n",
    "nbrs = nn(n_neighbors=6, algorithm='brute', metric='cosine').fit(coMatrix)\n",
    "distances, indices = nbrs.kneighbors(aliceVector)\n",
    "print(distances)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we observe the six closest words to 'Alice'.\n",
    "\n",
    "The first word corresponds to 'Alice' and the next five words are the closest words according to the NearestNeighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\n",
      "she\n",
      "it\n",
      "the\n",
      "a\n",
      "herself\n"
     ]
    }
   ],
   "source": [
    "for index in indices[0]:\n",
    "    print(inverted_index[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(coMatrix[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "            \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # repeat the same word several times\n",
    "                    in_words.append([word])\n",
    "                    # add the context words\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                #print(in_words)\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "                #print(all_in)\n",
    "                #print(all_in[0].shape)\n",
    "                #print(all_out)\n",
    "                #print(all_out[0].shape)\n",
    "                #break\n",
    "    return (all_in,all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the preprocessed data of Skipgram\n",
    "def save_skipgram_data(filename, x, y):\n",
    "    f = open(filename ,'w')\n",
    "    for input,outcome  in zip(x,y):\n",
    "        input = np.concatenate(input)\n",
    "        f.write(\" \".join(map(str, list(input))))\n",
    "        f.write(\",\")\n",
    "        outcome = np.concatenate(outcome)\n",
    "        f.write(\" \".join(map(str,list(outcome))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file(filename):\n",
    "    f = open(filename ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPGRAM_2WORDS = \"data_skipgram_2words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,2,V)\n",
    "save_skipgram_data(SKIPGRAM_2WORDS, x, y)\n",
    "\n",
    "SKIPGRAM_4WORDS = \"data_skipgram_4words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,4,V)\n",
    "save_skipgram_data(SKIPGRAM_4WORDS, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models\n",
    "skipgram_2words_model = Sequential()\n",
    "skipgram_2words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_model.add(Reshape((dim, )))\n",
    "skipgram_2words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_model = Sequential()\n",
    "skipgram_4words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_model.add(Reshape((dim, )))\n",
    "skipgram_4words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgrams\n",
    "skipgram_2words_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#define loss function for Skipgram\n",
    "skipgram_4words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram 2words losses: 0 -> 41073.176288604736\n",
      "skipgram 2words losses: 1 -> 36825.96047639847\n",
      "skipgram 2words losses: 2 -> 35294.80758178234\n",
      "skipgram 2words losses: 3 -> 33950.175908088684\n",
      "skipgram 2words losses: 4 -> 32746.51278924942\n",
      "skipgram 2words losses: 5 -> 31693.647832155228\n",
      "skipgram 2words losses: 6 -> 30794.318407058716\n",
      "skipgram 2words losses: 7 -> 30038.185254216194\n",
      "skipgram 2words losses: 8 -> 29411.423499822617\n",
      "skipgram 2words losses: 9 -> 28896.811560720205\n",
      "skipgram 2words losses: 10 -> 28479.537357762456\n",
      "skipgram 2words losses: 11 -> 28141.477943971753\n",
      "skipgram 2words losses: 12 -> 27870.50633222796\n",
      "skipgram 2words losses: 13 -> 27656.828305175528\n",
      "skipgram 2words losses: 14 -> 27488.104597038357\n",
      "skipgram 4words losses: 0 -> 40954.526681900024\n",
      "skipgram 4words losses: 1 -> 37519.69016981125\n",
      "skipgram 4words losses: 2 -> 36512.428622961044\n",
      "skipgram 4words losses: 3 -> 35587.4751098156\n",
      "skipgram 4words losses: 4 -> 34722.041709423065\n",
      "skipgram 4words losses: 5 -> 33930.63242530823\n",
      "skipgram 4words losses: 6 -> 33226.26936471462\n",
      "skipgram 4words losses: 7 -> 32609.874802917242\n",
      "skipgram 4words losses: 8 -> 32078.007990717888\n",
      "skipgram 4words losses: 9 -> 31623.392564158887\n",
      "skipgram 4words losses: 10 -> 31240.817594990134\n",
      "skipgram 4words losses: 11 -> 30922.063619391993\n",
      "skipgram 4words losses: 12 -> 30659.794172184542\n",
      "skipgram 4words losses: 13 -> 30445.170634314418\n",
      "skipgram 4words losses: 14 -> 30271.415667047433\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram model\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_model.train_on_batch(x, y)\n",
    "    skipgram_2words_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_model.train_on_batch(x, y)\n",
    "    skipgram_4words_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save computed vectors\n",
    "def save_computed_vectors(filename, vectors):\n",
    "    f = open(filename ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_FILENAME = 'vectors_skipgram_2words.txt'\n",
    "skipgram_2words_model_vectors = skipgram_2words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_FILENAME, skipgram_2words_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_FILENAME = 'vectors_skipgram_4words.txt'\n",
    "skipgram_4words_model_vectors = skipgram_4words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_FILENAME, skipgram_4words_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load computed vectors\n",
    "def load_word_vectors(filename):\n",
    "    f = open(filename, 'r')\n",
    "    # read first line to get the dimensions\n",
    "    dim_rows = 0\n",
    "    dim_columns = 0\n",
    "    word_vectors = None\n",
    "    for line in f.readlines():\n",
    "        input_values = line.strip().split(\" \")\n",
    "        if len(input_values) == 2:\n",
    "            dim_rows = int(input_values[0])\n",
    "            dim_columns = int(input_values[1])\n",
    "            word_vectors = np.zeros((dim_rows+1, dim_columns))\n",
    "            #print(\"rows: {}, columns: {}\".format(dim_rows, dim_columns))\n",
    "        else:\n",
    "            word = input_values[0]\n",
    "            vector_values = np.fromstring(\" \".join(input_values[1:]), dtype=float, sep=' ')\n",
    "            word_vectors[tokenizer.word_index[word]] = vector_values\n",
    "            \n",
    "    f.close()\n",
    "    return word_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_skipgram_2words_model_vectors = load_word_vectors(\"vectors_skipgram_2words.txt\")\n",
    "loaded_skipgram_4words_model_vectors = load_word_vectors(\"vectors_skipgram_4words.txt\")\n",
    "\n",
    "#print(np.allclose(loaded_skipgram_2words_model_vectors[1:], skipgram_2words_model_vectors[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models with additional dense layer\n",
    "\n",
    "skipgram_2words_modified_model = Sequential()\n",
    "skipgram_2words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_modified_model = Sequential()\n",
    "skipgram_4words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram_2words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "skipgram_4words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram 2words modified losses: 0 -> 40279.82591700554\n",
      "skipgram 2words modified losses: 1 -> 36782.553721904755\n",
      "skipgram 2words modified losses: 2 -> 35266.73841714859\n",
      "skipgram 2words modified losses: 3 -> 33938.88986003399\n",
      "skipgram 2words modified losses: 4 -> 32794.06656730175\n",
      "skipgram 2words modified losses: 5 -> 32115.513432741165\n",
      "skipgram 2words modified losses: 6 -> 31553.072211384773\n",
      "skipgram 2words modified losses: 7 -> 30978.873185694218\n",
      "skipgram 2words modified losses: 8 -> 30618.791428744793\n",
      "skipgram 2words modified losses: 9 -> 30316.65439903736\n",
      "skipgram 2words modified losses: 10 -> 30106.736092984676\n",
      "skipgram 2words modified losses: 11 -> 29835.97156509757\n",
      "skipgram 2words modified losses: 12 -> 29774.77802425623\n",
      "skipgram 2words modified losses: 13 -> 29411.00211817026\n",
      "skipgram 2words modified losses: 14 -> 29170.537762254477\n",
      "skipgram 4words modified losses: 0 -> 40154.90250015259\n",
      "skipgram 4words modified losses: 1 -> 37087.002388477325\n",
      "skipgram 4words modified losses: 2 -> 35919.926533937454\n",
      "skipgram 4words modified losses: 3 -> 35032.66031074524\n",
      "skipgram 4words modified losses: 4 -> 34387.44694805145\n",
      "skipgram 4words modified losses: 5 -> 33829.36461555958\n",
      "skipgram 4words modified losses: 6 -> 33615.44137322903\n",
      "skipgram 4words modified losses: 7 -> 33508.89103913307\n",
      "skipgram 4words modified losses: 8 -> 33436.74237692356\n",
      "skipgram 4words modified losses: 9 -> 33323.8696680069\n",
      "skipgram 4words modified losses: 10 -> 33122.14578521252\n",
      "skipgram 4words modified losses: 11 -> 32804.208872914314\n",
      "skipgram 4words modified losses: 12 -> 32665.952882409096\n",
      "skipgram 4words modified losses: 13 -> 32501.424136161804\n",
      "skipgram 4words modified losses: 14 -> 32380.574867486954\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_2words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words modified losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_4words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words modified losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME = 'vectors_skipgram_2words_modified.txt'\n",
    "skipgram_2words_modified_model_vectors = skipgram_2words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME, skipgram_2words_modified_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME = 'vectors_skipgram_4words_modified.txt'\n",
    "skipgram_4words_modified_model_vectors = skipgram_4words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME, skipgram_4words_modified_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train nearest neighbors model\n",
    "nn_skipgram_2words = nn(n_neighbors=21, algorithm='brute', metric='cosine').fit(skipgram_2words_model_vectors)\n",
    "nn_skipgram_4words = nn(n_neighbors=21, algorithm='brute', metric='cosine').fit(skipgram_4words_model_vectors)\n",
    "nn_skipgram_2words_modified = nn(n_neighbors=21, algorithm='brute', metric='cosine').fit(skipgram_2words_modified_model_vectors)\n",
    "nn_skipgram_4words_modified = nn(n_neighbors=21, algorithm='brute', metric='cosine').fit(skipgram_4words_modified_model_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_skipgram_2words, indices_skipgram_2words = nn_skipgram_2words.kneighbors(skipgram_2words_model_vectors)\n",
    "distances_skipgram_4words, indices_skipgram_4words = nn_skipgram_4words.kneighbors(skipgram_4words_model_vectors)\n",
    "distances_skipgram_2words_modified, indices_skipgram_2words_modified = nn_skipgram_2words_modified.kneighbors(skipgram_2words_modified_model_vectors)\n",
    "distances_skipgram_4words_modified, indices_skipgram_4words_modified = nn_skipgram_4words_modified.kneighbors(skipgram_4words_modified_model_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_neighbors(filename, neighbors):\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    for idrow, matrix_row in enumerate(neighbors):\n",
    "        words = []\n",
    "        if idrow > 0:\n",
    "            for idc, column in enumerate(matrix_row):\n",
    "                if column > 0 :\n",
    "                    words.append(inverted_index[column])\n",
    "            f.write(\", \".join(words))\n",
    "            f.write(\"\\n\")\n",
    "    f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_neighbors(\"skipgram_2words_neighbors.txt\", indices_skipgram_2words)\n",
    "save_neighbors(\"skipgram_4words_neighbors.txt\", indices_skipgram_4words)\n",
    "save_neighbors(\"skipgram_2words_modified_neighbors.txt\", indices_skipgram_2words_modified)\n",
    "save_neighbors(\"skipgram_4words_modified_neighbors.txt\", indices_skipgram_4words_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "# implementation of the function argmax(w.(w3 + w2 - w1)), with w - w3 =' w2 - w1\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.neighbors import NearestNeighbors as nn\n",
    "#nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "\n",
    "# computes: word1 is a word2 as word3 is to target\n",
    "# returns true if target is found in the closest 10 neighbors\n",
    "def analogy(word1, word2, word3, target, word_vectors, nnmodel):\n",
    "    vector_w1 = word_vectors[tokenizer.word_index[word1]]\n",
    "    vector_w2 = word_vectors[tokenizer.word_index[word2]]\n",
    "    vector_w3 = word_vectors[tokenizer.word_index[word3]]\n",
    "    composed_vector = vector_w1 - vector_w2 + vector_w3\n",
    "    distances, indices = nnmodel.kneighbors(composed_vector.reshape(1, -1))\n",
    "    return tokenizer.word_index[target] in indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the analogy function\n",
    "\n",
    "The following sentences are used to test our analogy function:\n",
    "* __rome__ is to __paris__ as  __neck__ is to __legs__\n",
    "* __tears__ is to __smiling__ as __smaller__ is to __larger__\n",
    "* __rome__ is to __paris__ as __apple__ is to __cherry__\n",
    "* __orange__ is to __marmalade__ as __apple__ is to __tart__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = analogy('rome', 'paris', 'neck', 'legs', skipgram_2words_model_vectors, nn_skipgram_2words)\n",
    "result2 = analogy('tears', 'smiling', 'smaller', 'larger', skipgram_2words_model_vectors, nn_skipgram_2words)\n",
    "result3 = analogy('rome', 'paris', 'apple', 'cherry', skipgram_2words_model_vectors, nn_skipgram_2words)\n",
    "result4 = analogy('orange', 'marmalade', 'apple', 'tart', skipgram_2words_model_vectors, nn_skipgram_2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the four different word embeddings\n",
    "\n",
    "Implement your own function to perform the analogy task with.\n",
    "    \n",
    "Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    \n",
    "Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.\n",
    "    \n",
    "Visualize your results and interpret your results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
