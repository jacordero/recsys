{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.local/share/virtualenvs/RecommenderSystems-c5N1t04d/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'and', 3: 'to', 4: 'she', 5: 'a', 6: 'i', 7: 'it', 8: 'of', 9: 'was', 10: 'in', 11: 'alice', 12: 'you', 13: 'that', 14: 'her', 15: 'as', 16: 'said', 17: 'had', 18: 'for', 19: 'but', 20: 'be', 21: 'on', 22: 'all', 23: 'with', 24: 'little', 25: 'mouse', 26: 'down', 27: 'very', 28: 'this', 29: 'not', 30: 'so', 31: 'out', 32: 'if', 33: 'is', 34: 'at', 35: 't', 36: 's', 37: 'll', 38: 'how', 39: 'they', 40: 'about', 41: 'herself', 42: 'me', 43: 'up', 44: 'what', 45: 'way', 46: 'when', 47: 'like', 48: 'one', 49: 'do', 50: 'no', 51: 'oh', 52: 'went', 53: 'thought', 54: 'again', 55: 'there', 56: 'see', 57: 'or', 58: 'could', 59: 'would', 60: 'think', 61: 'them', 62: 'know', 63: 'rabbit', 64: 'dear', 65: 'were', 66: 'time', 67: 'get', 68: 'here', 69: 'must', 70: 'my', 71: 'by', 72: 'into', 73: 'found', 74: 'such', 75: 'began', 76: 'soon', 77: 'm', 78: 'quite', 79: 'then', 80: 'off', 81: 'now', 82: 'go', 83: 'say', 84: 'have', 85: 'which', 86: 'come', 87: 'dinah', 88: 'your', 89: 'thing', 90: 'dodo', 91: 'much', 92: 'shall', 93: 'things', 94: 'long', 95: 'door', 96: 'who', 97: 'can', 98: 'once', 99: 'did', 100: 'over', 101: 'feet', 102: 'after', 103: 'wonder', 104: 'first', 105: 'let', 106: 'are', 107: 'cats', 108: 'round', 109: 'poor', 110: 'back', 111: 'he', 112: 'nothing', 113: 'seemed', 114: 'its', 115: 'never', 116: 'going', 117: 'great', 118: 'got', 119: 'ever', 120: 'table', 121: 'any', 122: 'only', 123: 'more', 124: 'cried', 125: 'gloves', 126: 'fan', 127: 'well', 128: 'white', 129: 'looked', 130: 'before', 131: 'moment', 132: 'why', 133: 'an', 134: 'eat', 135: 'however', 136: 'sure', 137: 'pool', 138: 'getting', 139: 'took', 140: 'either', 141: 'just', 142: 'upon', 143: 've', 144: 'through', 145: 'wish', 146: 'tell', 147: 'hall', 148: 'key', 149: 'half', 150: 'find', 151: 'his', 152: 'use', 153: 'large', 154: 'look', 155: 'too', 156: 'house', 157: 'good', 158: 'right', 159: 'words', 160: 'am', 161: 'might', 162: 'question', 163: 'came', 164: 'dry', 165: 'away', 166: 'been', 167: 'small', 168: 'high', 169: 'garden', 170: 'tears', 171: 'don', 172: 'won', 173: 'doesn', 174: 'without', 175: 'suddenly', 176: 'eyes', 177: 'some', 178: 'put', 179: 'sort', 180: 'their', 181: 'rather', 182: 'talking', 183: 'saying', 184: 'felt', 185: 'hand', 186: 'low', 187: 'trying', 188: 'golden', 189: 'head', 190: 'indeed', 191: 'will', 192: 'while', 193: 'same', 194: 'try', 195: 'beg', 196: 'tone', 197: 'our', 198: 'd', 199: 'called', 200: 'birds', 201: 'lory', 202: 'race', 203: 'looking', 204: 'course', 205: 'book', 206: 'made', 207: 'ran', 208: 'seen', 209: 'fell', 210: 'next', 211: 'tried', 212: 'make', 213: 'anything', 214: 'four', 215: 'nice', 216: 'people', 217: 'please', 218: 'should', 219: 'remember', 220: 'turned', 221: 'other', 222: 'glass', 223: 'than', 224: 'marked', 225: 'best', 226: 'sat', 227: 'two', 228: 'english', 229: 'pair', 230: 'heard', 231: 'kid', 232: 'ready', 233: 'voice', 234: 'queer', 235: 'changed', 236: 'swam', 237: 'talk', 238: 'offended', 239: 'we', 240: 'party', 241: 'prizes', 242: 'chapter', 243: 'hole', 244: 'tired', 245: 'having', 246: 'pictures', 247: 'own', 248: 'mind', 249: 'hot', 250: 'day', 251: 'pocket', 252: 'hurried', 253: 'take', 254: 'another', 255: 'coming', 256: 'from', 257: 'fall', 258: 'even', 259: 'near', 260: 'idea', 261: 'among', 262: 'didn', 263: 'ask', 264: 'air', 265: 'perhaps', 266: 'else', 267: 'cat', 268: 'afraid', 269: 'bats', 270: 'rate', 271: 'those', 272: 'really', 273: 'bottle', 274: 'children', 275: 'enough', 276: 'anxiously', 277: 'surprised', 278: 'speak', 279: 'hastily', 280: 'duchess', 281: 'kept', 282: 'everything', 283: 'mabel', 284: 'capital', 285: 'tail', 286: 'something', 287: 'william', 288: 'pardon', 289: 'crowded', 290: 'whole', 291: 'last', 292: 'old', 293: 'him', 294: 'thimble', 295: 'close', 296: 'hear', 297: 'itself', 298: 'late', 299: 'ought', 300: 'under', 301: 'world', 302: 'deep', 303: 'slowly', 304: 'happen', 305: 'dark', 306: 'noticed', 307: 'somebody', 308: 'home', 309: 'end', 310: 'many', 311: 'fallen', 312: 'several', 313: 'lessons', 314: 'still', 315: 'distance', 316: 'seem', 317: 'walk', 318: 'heads', 319: 'name', 320: 'ma', 321: 'fancy', 322: 're', 323: 'manage', 324: 'night', 325: 'mice', 326: 'answer', 327: 'passage', 328: 'sight', 329: 'behind', 330: 'every', 331: 'three', 332: 'alas', 333: 'inches', 334: 'along', 335: 'bright', 336: 'happened', 337: 'few', 338: 'drink', 339: 'hurry', 340: 'poison', 341: 'finger', 342: 'almost', 343: 'certain', 344: 'finished', 345: 'curious', 346: 'face', 347: 'candle', 348: 'generally', 349: 'remembered', 350: 'box', 351: 'left', 352: 'person', 353: 'eye', 354: 'lying', 355: 'cake', 356: 'life', 357: 'deal', 358: 'dropped', 359: 'times', 360: 'seems', 361: 'stay', 362: 'being', 363: 'sudden', 364: 'cause', 365: 'water', 366: 'o', 367: 'history', 368: 'trembling', 369: 'always', 370: 'dogs', 371: 'eagerly', 372: 'fetch', 373: 'us', 374: 'duck', 375: 'eaglet', 376: 'caucus', 377: 'tale', 378: 'replied', 379: 'turning', 380: 'sister', 381: 'bank', 382: 'considering', 383: 'feel', 384: 'sleepy', 385: 'stupid', 386: 'whether', 387: 'making', 388: 'worth', 389: 'trouble', 390: 'remarkable', 391: 'natural', 392: 'watch', 393: 'waistcoat', 394: 'started', 395: 'across', 396: 'falling', 397: 'filled', 398: 'cupboards', 399: 'shelves', 400: 'saw', 401: 'jar', 402: 'fear', 403: 'managed', 404: 'wouldn', 405: 'top', 406: 'likely', 407: 'miles', 408: 'aloud', 409: 'somewhere', 410: 'earth', 411: 'though', 412: 'opportunity', 413: 'knowledge', 414: 'listen', 415: 'yes', 416: 'latitude', 417: 'longitude', 418: 'funny', 419: 'glad', 420: 'new', 421: 'spoke', 422: 'girl', 423: 'asking', 424: 'miss', 425: 'catch', 426: 'bat', 427: 'sometimes', 428: 'matter', 429: 'begun', 430: 'walking', 431: 'thump', 432: 'bit', 433: 'hurt', 434: 'lost', 435: 'corner', 436: 'ears', 437: 'whiskers', 438: 'row', 439: 'roof', 440: 'doors', 441: 'side', 442: 'walked', 443: 'sadly', 444: 'middle', 445: 'tiny', 446: 'opened', 447: 'led', 448: 'larger', 449: 'shut', 450: 'telescope', 451: 'knew', 452: 'waiting', 453: 'hoping', 454: 'rules', 455: 'shutting', 456: 'certainly', 457: 'beautifully', 458: 'hold', 459: 'usually', 460: 'forgotten', 461: 'taste', 462: 'finding', 463: 'fact', 464: 'feeling', 465: 'size', 466: 'waited', 467: 'minutes', 468: 'altogether', 469: 'reach', 470: 'crying', 471: 'sharply', 472: 'leave', 473: 'minute', 474: 'gave', 475: 'severely', 476: 'against', 477: 'child', 478: 'fond', 479: 'makes', 480: 'grow', 481: 'happens', 482: 'holding', 483: 'growing', 484: 'curiouser', 485: 'forgot', 486: 'far', 487: 'dears', 488: 'give', 489: 'sending', 490: 'foot', 491: 'nonsense', 492: 'nine', 493: 'cry', 494: 'yourself', 495: 'stop', 496: 'pattering', 497: 'trotting', 498: 'muttering', 499: 'help', 500: 'sir', 501: 'hard', 502: 'morning', 503: 'ah', 504: 'age', 505: 'hair', 506: 'ringlets', 507: 'mine', 508: 'sorts', 509: 'puzzling', 510: 'used', 511: 'paris', 512: 'rome', 513: 'doth', 514: 'hands', 515: 'alone', 516: 'done', 517: 'shrinking', 518: 'frightened', 519: 'change', 520: 'bad', 521: 'slipped', 522: 'salt', 523: 'sea', 524: 'case', 525: 'railway', 526: 'hadn', 527: 'suppose', 528: 'swimming', 529: 'speaking', 530: 'understand', 531: 'french', 532: 'conqueror', 533: 'lesson', 534: 'angry', 535: 'show', 536: 'paws', 537: 'nurse', 538: 'catching', 539: 'subject', 540: 'sit', 541: 'says', 542: 'useful', 543: 'shore', 544: 'hate', 545: 'animals', 546: 'fur', 547: 'wet', 548: 'better', 549: 'ring', 550: 'silence', 551: 'wanted', 552: 'edwin', 553: 'morcar', 554: 'earls', 555: 'mercia', 556: 'northumbria', 557: 'advisable', 558: 'meet', 559: 'melancholy', 560: 'solemnly', 561: 'explain', 562: 'running', 563: 'liked', 564: 'has', 565: 'chorus', 566: 'comfits', 567: 'speech', 568: 'caused', 569: 'sad', 570: 'fury', 571: 'trial', 572: 'jury', 573: 'judge', 574: 'finish', 575: 'story', 576: 'crab', 577: 'nobody', 578: 'ferrets', 579: 'hunting', 580: 'mary', 581: 'ann', 582: 'messages', 583: 'room', 584: 'beginning', 585: 'sitting', 586: 'twice', 587: 'peeped', 588: 'reading', 589: 'conversations', 590: 'pleasure', 591: 'daisy', 592: 'chain', 593: 'picking', 594: 'daisies', 595: 'pink', 596: 'nor', 597: 'afterwards', 598: 'occurred', 599: 'wondered', 600: 'actually', 601: 'flashed', 602: 'burning', 603: 'curiosity', 604: 'field', 605: 'fortunately', 606: 'pop', 607: 'hedge', 608: 'straight', 609: 'tunnel', 610: 'dipped', 611: 'stopping', 612: 'plenty', 613: 'sides', 614: 'maps', 615: 'hung', 616: 'pegs', 617: 'passed', 618: 'labelled', 619: 'orange', 620: 'marmalade', 621: 'disappointment', 622: 'empty', 623: 'drop', 624: 'killing', 625: 'past', 626: 'tumbling', 627: 'stairs', 628: 'brave', 629: 'true', 630: 'centre', 631: 'thousand', 632: 'learnt', 633: 'schoolroom', 634: 'showing', 635: 'practice', 636: 'grand', 637: 'presently', 638: 'downward', 639: 'antipathies', 640: 'listening', 641: 'sound', 642: 'word', 643: 'country', 644: 'zealand', 645: 'australia', 646: 'curtsey', 647: 'curtseying', 648: 'ignorant', 649: 'written', 650: 'hope', 651: 'saucer', 652: 'milk', 653: 'tea', 654: 'dreamy', 655: 'couldn', 656: 'dozing', 657: 'dream', 658: 'earnestly', 659: 'truth', 660: 'heap', 661: 'sticks', 662: 'leaves', 663: 'jumped', 664: 'overhead', 665: 'hurrying', 666: 'wind', 667: 'longer', 668: 'lit', 669: 'lamps', 670: 'hanging', 671: 'locked', 672: 'wondering', 673: 'legged', 674: 'solid', 675: 'except', 676: 'belong', 677: 'locks', 678: 'open', 679: 'second', 680: 'curtain', 681: 'fifteen', 682: 'lock', 683: 'delight', 684: 'fitted', 685: 'rat', 686: 'knelt', 687: 'loveliest', 688: 'longed', 689: 'wander', 690: 'beds', 691: 'flowers', 692: 'cool', 693: 'fountains', 694: 'doorway', 695: 'shoulders', 696: 'begin', 697: 'lately', 698: 'telescopes', 699: 'neck', 700: 'paper', 701: 'label', 702: 'printed', 703: 'wise', 704: 'read', 705: 'histories', 706: 'burnt', 707: 'eaten', 708: 'wild', 709: 'beasts', 710: 'unpleasant', 711: 'because', 712: 'simple', 713: 'friends', 714: 'taught', 715: 'red', 716: 'poker', 717: 'burn', 718: 'cut', 719: 'deeply', 720: 'knife', 721: 'bleeds', 722: 'disagree', 723: 'sooner', 724: 'later', 725: 'ventured', 726: 'mixed', 727: 'flavour', 728: 'cherry', 729: 'tart', 730: 'custard', 731: 'pine', 732: 'apple', 733: 'roast', 734: 'turkey', 735: 'toffee', 736: 'buttered', 737: 'toast', 738: 'ten', 739: 'brightened', 740: 'lovely', 741: 'shrink', 742: 'further', 743: 'nervous', 744: 'flame', 745: 'blown', 746: 'decided', 747: 'possibly', 748: 'plainly', 749: 'climb', 750: 'legs', 751: 'slippery', 752: 'advise', 753: 'advice', 754: 'seldom', 755: 'followed', 756: 'scolded', 757: 'bring', 758: 'cheated', 759: 'game', 760: 'croquet', 761: 'playing', 762: 'pretending', 763: 'pretend', 764: 'hardly', 765: 'respectable', 766: 'currants', 767: 'smaller', 768: 'creep', 769: 'care', 770: 'ate', 771: 'remained', 772: 'eats', 773: 'expecting', 774: 'dull', 775: 'common', 776: 'set', 777: 'work', 778: 'ii', 779: 'opening', 780: 'largest', 781: 'bye', 782: 'shoes', 783: 'stockings', 784: 'shan', 785: 'able', 786: 'myself', 787: 'kind', 788: 'want', 789: 'boots', 790: 'christmas', 791: 'planning', 792: 'carrier', 793: 'presents', 794: 'odd', 795: 'directions', 796: 'esq', 797: 'hearthrug', 798: 'fender', 799: 'love', 800: 'struck', 801: 'hopeless', 802: 'ashamed', 803: 'shedding', 804: 'gallons', 805: 'until', 806: 'reaching', 807: 'dried', 808: 'returning', 809: 'splendidly', 810: 'dressed', 811: 'himself', 812: 'savage', 813: 'desperate', 814: 'timid', 815: 'violently', 816: 'skurried', 817: 'darkness', 818: 'fanning', 819: 'yesterday', 820: 'usual', 821: 'different', 822: 'puzzle', 823: 'thinking', 824: 'ada', 825: 'goes', 826: 'knows', 827: 'besides', 828: 'five', 829: 'twelve', 830: 'six', 831: 'thirteen', 832: 'seven', 833: 'twenty', 834: 'multiplication', 835: 'signify', 836: 'geography', 837: 'london', 838: 'wrong', 839: 'crossed', 840: 'lap', 841: 'repeat', 842: 'sounded', 843: 'hoarse', 844: 'strange', 845: 'crocodile', 846: 'improve', 847: 'shining', 848: 'pour', 849: 'waters', 850: 'nile', 851: 'scale', 852: 'cheerfully', 853: 'grin', 854: 'neatly', 855: 'spread', 856: 'claws', 857: 'welcome', 858: 'fishes', 859: 'gently', 860: 'smiling', 861: 'jaws', 862: 'live', 863: 'poky', 864: 'toys', 865: 'play', 866: 'learn', 867: 'putting', 868: 'till', 869: 'burst', 870: 'measure', 871: 'nearly', 872: 'guess', 873: 'rapidly', 874: 'avoid', 875: 'narrow', 876: 'escape', 877: 'existence', 878: 'speed', 879: 'worse', 880: 'declare', 881: 'these', 882: 'splash', 883: 'chin', 884: 'somehow', 885: 'seaside', 886: 'general', 887: 'conclusion', 888: 'wherever', 889: 'coast', 890: 'number', 891: 'bathing', 892: 'machines', 893: 'digging', 894: 'sand', 895: 'wooden', 896: 'spades', 897: 'lodging', 898: 'houses', 899: 'station', 900: 'wept', 901: 'punished', 902: 'drowned', 903: 'splashing', 904: 'nearer', 905: 'walrus', 906: 'hippopotamus', 907: 'harm', 908: 'brother', 909: 'latin', 910: 'grammar', 911: 'inquisitively', 912: 'wink', 913: 'daresay', 914: 'clear', 915: 'notion', 916: 'ago', 917: 'ou', 918: 'est', 919: 'chatte', 920: 'sentence', 921: 'leap', 922: 'quiver', 923: 'fright', 924: 'animal', 925: 'feelings', 926: 'shrill', 927: 'passionate', 928: 'soothing', 929: 'yet', 930: 'quiet', 931: 'lazily', 932: 'sits', 933: 'purring', 934: 'nicely', 935: 'fire', 936: 'licking', 937: 'washing', 938: 'soft', 939: 'bristling', 940: 'family', 941: 'hated', 942: 'nasty', 943: 'vulgar', 944: 'conversation', 945: 'dog', 946: 'eyed', 947: 'terrier', 948: 'curly', 949: 'brown', 950: 'throw', 951: 'dinner', 952: 'belongs', 953: 'farmer', 954: 'hundred', 955: 'pounds', 956: 'kills', 957: 'rats', 958: 'sorrowful', 959: 'commotion', 960: 'softly', 961: 'pale', 962: 'passion', 963: 'creatures', 964: 'iii', 965: 'assembled', 966: 'draggled', 967: 'feathers', 968: 'clinging', 969: 'dripping', 970: 'cross', 971: 'uncomfortable', 972: 'consultation', 973: 'familiarly', 974: 'known', 975: 'argument', 976: 'sulky', 977: 'older', 978: 'allow', 979: 'knowing', 980: 'positively', 981: 'refused', 982: 'authority', 983: 'fixed', 984: 'cold', 985: 'ahem', 986: 'important', 987: 'driest', 988: 'whose', 989: 'favoured', 990: 'pope', 991: 'submitted', 992: 'leaders', 993: 'accustomed', 994: 'usurpation', 995: 'conquest', 996: 'ugh', 997: 'shiver', 998: 'frowning', 999: 'politely', 1000: 'proceed', 1001: 'declared', 1002: 'stigand', 1003: 'patriotic', 1004: 'archbishop', 1005: 'canterbury', 1006: 'crossly', 1007: 'means', 1008: 'frog', 1009: 'worm', 1010: 'notice', 1011: 'hurriedly', 1012: 'edgar', 1013: 'atheling', 1014: 'offer', 1015: 'crown', 1016: 'conduct', 1017: 'moderate', 1018: 'insolence', 1019: 'normans', 1020: 'continued', 1021: 'rising', 1022: 'move', 1023: 'meeting', 1024: 'adjourn', 1025: 'immediate', 1026: 'adoption', 1027: 'energetic', 1028: 'meaning', 1029: 'believe', 1030: 'bent', 1031: 'hide', 1032: 'smile', 1033: 'paused', 1034: 'inclined', 1035: 'winter', 1036: 'circle', 1037: 'exact', 1038: 'shape', 1039: 'placed', 1040: 'easy', 1041: 'hour', 1042: 'panting', 1043: 'pressed', 1044: 'forehead', 1045: 'position', 1046: 'shakespeare', 1047: 'rest', 1048: 'everybody', 1049: 'voices', 1050: 'asked', 1051: 'pointing', 1052: 'calling', 1053: 'confused', 1054: 'despair', 1055: 'pulled', 1056: 'luckily', 1057: 'handed', 1058: 'exactly', 1059: 'piece', 1060: 'prize', 1061: 'gravely', 1062: 'presented', 1063: 'acceptance', 1064: 'elegant', 1065: 'short', 1066: 'cheered', 1067: 'absurd', 1068: 'grave', 1069: 'dare', 1070: 'laugh', 1071: 'simply', 1072: 'bowed', 1073: 'solemn', 1074: 'noise', 1075: 'confusion', 1076: 'complained', 1077: 'theirs', 1078: 'ones', 1079: 'choked', 1080: 'patted', 1081: 'begged', 1082: 'promised', 1083: 'c', 1084: 'added', 1085: 'whisper', 1086: 'call', 1087: 'met', 1088: 'both', 1089: 'law', 1090: 'prosecute', 1091: 'denial', 1092: 'cur', 1093: 'wasting', 1094: 'breath', 1095: 'cunning', 1096: 'condemn', 1097: 'death', 1098: 'attending', 1099: 'humbly', 1100: 'fifth', 1101: 'bend', 1102: 'angrily', 1103: 'knot', 1104: 'undo', 1105: 'insult', 1106: 'mean', 1107: 'pleaded', 1108: 'easily', 1109: 'growled', 1110: 'reply', 1111: 'others', 1112: 'joined', 1113: 'shook', 1114: 'impatiently', 1115: 'quicker', 1116: 'pity', 1117: 'sighed', 1118: 'daughter', 1119: 'lose', 1120: 'temper', 1121: 'tongue', 1122: 'young', 1123: 'snappishly', 1124: 'patience', 1125: 'oyster', 1126: 'addressing', 1127: 'particular', 1128: 'venture', 1129: 'pet', 1130: 'bird', 1131: 'sensation', 1132: 'magpie', 1133: 'wrapping', 1134: 'carefully', 1135: 'remarking', 1136: 'suit', 1137: 'throat', 1138: 'canary', 1139: 'bed', 1140: 'various', 1141: 'pretexts', 1142: 'moved', 1143: 'mentioned', 1144: 'lonely', 1145: 'spirited', 1146: 'footsteps', 1147: 'iv', 1148: 'sends', 1149: 'bill', 1150: 'executed', 1151: 'where', 1152: 'guessed', 1153: 'naturedly', 1154: 'nowhere', 1155: 'since', 1156: 'swim', 1157: 'vanished', 1158: 'completely', 1159: 'doing', 1160: 'run', 1161: 'quick', 1162: 'direction', 1163: 'pointed', 1164: 'mistake', 1165: 'housemaid', 1166: 'finds', 1167: 'neat', 1168: 'brass', 1169: 'plate', 1170: 'w', 1171: 'engraved', 1172: 'knocking', 1173: 'upstairs', 1174: 'lest', 1175: 'real', 1176: 'fancying', 1177: 'directly', 1178: 'ordering', 1179: 'tidy', 1180: 'window', 1181: 'hoped', 1182: 'pairs'}\n"
     ]
    }
   ],
   "source": [
    "# create inverted index to help recover the words from indexes\n",
    "inverted_index = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    inverted_index[i] = word\n",
    "    \n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "3\n",
      "[[0.33333333 0.66666667]]\n",
      "int64\n",
      "[[1 2]]\n",
      "7\n",
      "[[0.42857143 0.57142857]]\n",
      "int64\n",
      "[[3 4]]\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# helper snippet\n",
    "################################\n",
    "\n",
    "m = np.matrix([[1, 2], [3, 4]])\n",
    "print(m)\n",
    "(rows, columns) = m.shape\n",
    "for rowIdx in range(rows):\n",
    "    s = m[rowIdx]\n",
    "    total = s.sum()\n",
    "    print(s.sum())\n",
    "    print(np.divide(s, total))\n",
    "    print(s.dtype)\n",
    "    print(s.flatten())\n",
    "    \n",
    "    #for value in m[rowIdx, :]:\n",
    "    #    print(value)\n",
    "    #print(m[rowIdx, :])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[797]\n",
      "[1090]\n",
      "[1093]\n",
      "[1094]\n",
      "[1095]\n",
      "[1096]\n",
      "[1097]\n",
      "{'duck': 374, 'begged': 1081, 'nearly': 871, 'advise': 752, 'have': 84, 'fear': 402, 'heap': 660, 'beasts': 709, 'drink': 338, 'met': 1087, 'choked': 1079, 'chorus': 565, 'pairs': 1182, 'simply': 1071, 'w': 1170, 'way': 45, 'longed': 688, 'leap': 921, 'politely': 999, 'then': 79, 'odd': 794, 'neatly': 854, 'key': 148, 'gave': 474, 'red': 715, 'shrill': 926, 'swim': 1156, 'growled': 1109, 'english': 228, 'hour': 1041, 'continued': 1020, 'nicely': 934, 'side': 441, 'these': 881, 'country': 643, 'shrinking': 517, 'claws': 856, 'jar': 401, 'stairs': 627, 'bed': 1139, 'hippopotamus': 906, 'i': 6, 'bathing': 891, 'hanging': 670, 'despair': 1054, 'conversation': 944, 'times': 359, 'remember': 219, 'sharply': 471, 'miles': 407, 'fur': 546, 'find': 150, 'australia': 645, 'playing': 761, 'getting': 138, 'dark': 305, 'harm': 907, 'various': 1140, 'sight': 328, 'nothing': 112, 'trying': 187, 'four': 214, 'boots': 789, 'child': 477, 'speech': 567, 'things': 93, 'hear': 296, 'made': 206, 'ashamed': 802, 'ii': 778, 'crossly': 1006, 'indeed': 190, 'found': 73, 'asked': 1050, 'piece': 1059, 'conversations': 589, 'seem': 316, 'fell': 209, 'book': 205, 'normans': 1019, 'sleepy': 384, 'right': 158, 'authority': 982, 'course': 204, 'else': 266, 'alone': 515, 'hall': 147, 'cherry': 728, 'short': 1065, 'sir': 500, 'ah': 503, 'condemn': 1096, 'ever': 119, 'shining': 847, 'escape': 876, 'fond': 478, 'dull': 774, 'toys': 864, 'tired': 244, 'shakespeare': 1046, 'hide': 1031, 'chapter': 242, 'carrier': 792, 'iv': 1147, 'go': 82, 'tart': 729, 'sits': 932, 'temper': 1120, 'wander': 689, 'voices': 1049, 'puzzling': 509, 'brown': 949, 'timid': 814, 'others': 1111, 'likely': 406, 'quiver': 922, 'shelves': 399, 'pardon': 288, 'led': 447, 'yourself': 494, 'fishes': 858, 'mentioned': 1143, 'wondering': 672, 'patience': 1124, 'means': 1007, 'far': 486, 'somebody': 307, 'legged': 673, 'offer': 1014, 'jaws': 861, 'she': 4, 'crowded': 289, 'eyes': 176, 'cats': 107, 'delight': 683, 'passage': 327, 'winter': 1035, 'fixed': 983, 'toast': 737, 'pattering': 496, 'geography': 836, 'sentence': 920, 'some': 177, 'cool': 692, 'added': 1084, 'burning': 602, 'size': 465, 'splashing': 903, 'capital': 284, 'maps': 614, 'he': 111, 'twenty': 833, 'foot': 490, 'completely': 1158, 'finding': 462, 'really': 272, 'fifth': 1100, 'paper': 700, 'carefully': 1134, 'am': 160, 'sorts': 508, 'seen': 208, 'sit': 540, 'sand': 894, 'canary': 1138, 'commotion': 959, 'cheered': 1066, 'herself': 41, 'conclusion': 887, 'conduct': 1016, 'pine': 731, 'is': 33, 'behind': 329, 'curiosity': 603, 'pet': 1129, 'mice': 325, 'either': 140, 'bring': 757, 'prosecute': 1090, 'downward': 638, 'stupid': 385, 'only': 122, 'falling': 396, 'bank': 381, 'known': 974, 'refused': 981, 'jumped': 663, 'knew': 451, 'was': 9, 'pressed': 1043, 'offended': 238, 'once': 98, 'cut': 718, 'yesterday': 819, 'older': 977, 'signify': 835, 'lost': 434, 'pale': 961, 'denial': 1091, 'sound': 641, 'please': 217, 'housemaid': 1165, 'atheling': 1013, 'answer': 326, 'rest': 1047, 'again': 54, 'kept': 281, 'being': 362, 'across': 395, 'sadly': 443, 'use': 152, 'earnestly': 658, 'eat': 134, 'earth': 410, 'insolence': 1018, 'clear': 914, 'able': 785, 'belongs': 952, 'seldom': 754, 'slowly': 303, 'duchess': 280, 'seaside': 885, 'nile': 850, 'afterwards': 597, 'saying': 183, 'grave': 1068, 'said': 16, 'age': 504, 'consultation': 972, 'dare': 1069, 'did': 99, 'five': 828, 'are': 106, 'high': 168, 'elegant': 1064, 'lazily': 931, 'reading': 588, 'history': 367, 'we': 239, 'saucer': 651, 'brother': 908, 'usual': 820, 'one': 48, 'calling': 1052, 'started': 394, 'curtain': 680, 't': 35, 'got': 118, 'kid': 231, 'addressing': 1126, 'close': 295, 'turkey': 734, 'hair': 505, 'dry': 164, 'strange': 844, 'thinking': 823, 'let': 105, 'hoarse': 843, 'back': 110, 'new': 420, 'golden': 188, 'opportunity': 412, 'ears': 436, 'sea': 523, 'solid': 674, 'speed': 878, 'number': 890, 'comfits': 566, 'animals': 545, 'hurried': 252, 'grow': 480, 'knowing': 979, 'natural': 391, 'knocking': 1172, 'larger': 448, 'ou': 917, 'struck': 800, 'hot': 249, 'adoption': 1026, 'many': 310, 'hope': 650, 'life': 356, 'burn': 717, 'sister': 380, 'daresay': 913, 'custard': 730, 'crocodile': 845, 'eaglet': 375, 'about': 40, 'lory': 201, 'farmer': 953, 'suit': 1136, 'glass': 222, 'curiouser': 484, 'simple': 712, 'shook': 1113, 'game': 759, 'leaves': 662, 'if': 32, 'beg': 195, 'table': 120, 'dinah': 87, 'play': 865, 'lamps': 669, 'upon': 142, 'pink': 595, 'think': 60, 'sensation': 1131, 'rats': 957, 'flavour': 727, 'night': 324, 'purring': 933, 'notion': 915, 'centre': 630, 'latitude': 416, 'largest': 780, 'enough': 275, 'field': 604, 'be': 20, 'creep': 768, 'young': 1122, 'cold': 984, 'immediate': 1025, 'bye': 781, 'considering': 382, 'bill': 1149, 'believe': 1029, 'twelve': 829, 'managed': 403, 'watch': 392, 'middle': 444, 'sorrowful': 958, 'argument': 975, 'holding': 482, 'sitting': 585, 'help': 499, 'pretending': 762, 'crown': 1015, 'possibly': 747, 'tears': 170, 'd': 198, 'hopeless': 801, 'even': 258, 'walking': 430, 'over': 100, 'crossed': 839, 'were': 65, 'put': 178, 'muttering': 498, 'flowers': 691, 'trouble': 389, 'esq': 796, 'throw': 950, 'cause': 364, 'death': 1097, 'anxiously': 276, 'few': 337, 'station': 899, 'except': 675, 'hurt': 433, 'catch': 425, 'his': 151, 'hand': 185, 'more': 123, 'angry': 534, 'loveliest': 687, 'hoping': 453, 'edgar': 1012, 'patted': 1080, 'wet': 547, 'turned': 220, 'of': 8, 'c': 1083, 'chin': 883, 'glad': 419, 'ago': 916, 'those': 271, 'won': 172, 'shape': 1038, 'schoolroom': 633, 'curly': 948, 'fanning': 818, 'directions': 795, 'pictures': 246, 'understand': 530, 'saw': 400, 'very': 27, 'legs': 750, 'heard': 230, 'throat': 1137, 'deep': 302, 'real': 1175, 'lose': 1119, 'such': 74, 'fall': 257, 'dripping': 969, 'us': 373, 'whisper': 1085, 'tidy': 1179, 'wrapping': 1133, 'low': 186, 'moment': 131, 'through': 144, 'dreamy': 654, 'smiling': 860, 'judge': 573, 'ignorant': 648, 'generally': 348, 'ones': 1078, 'every': 330, 'says': 541, 'end': 309, 'altogether': 468, 'important': 986, 'own': 247, 'wish': 145, 'curtseying': 647, 'race': 202, 'fury': 570, 'archbishop': 1004, 'impatiently': 1114, 'inquisitively': 911, 'nearer': 904, 'used': 510, 'driest': 987, 'row': 438, 'stopping': 611, 'whiskers': 437, 'dressed': 810, 'overhead': 664, 'see': 56, 'vulgar': 943, 'while': 192, 'confusion': 1075, 'planning': 791, 'sat': 226, 'home': 308, 'slippery': 751, 'happen': 304, 'common': 775, 'tell': 146, 'lock': 682, 'plenty': 612, 'adjourn': 1024, 'moderate': 1017, 'histories': 705, 'too': 155, 'old': 292, 'advisable': 557, 'repeat': 841, 'executed': 1150, 'better': 548, 'tongue': 1121, 'label': 701, 'whether': 386, 'daughter': 1118, 'mary': 580, 'shore': 543, 'thimble': 294, 'finished': 344, 'presented': 1062, 'grin': 853, 'frightened': 518, 'eaten': 707, 'feel': 383, 'happened': 336, 'down': 26, 'fancying': 1176, 'alice': 11, 'past': 625, 'head': 189, 'picking': 593, 'lovely': 740, 'since': 1155, 'liked': 563, 'nine': 492, 'm': 77, 'mean': 1106, 'hearthrug': 797, 'naturedly': 1153, 'care': 769, 'for': 18, 'thousand': 631, 'half': 149, 'happens': 481, 'paws': 536, 'caucus': 376, 'pour': 848, 'curious': 345, 'time': 66, 'girl': 422, 'much': 91, 'severely': 475, 'earls': 554, 'off': 80, 'roof': 439, 'soft': 938, 'undo': 1104, 'nasty': 942, 'waiting': 452, 'subject': 539, 'skurried': 816, 'burst': 869, 'showing': 634, 'longitude': 417, 'telescopes': 698, 'what': 44, 'bent': 1030, 'cheated': 758, 'solemn': 1073, 'occurred': 598, 'left': 351, 'brightened': 739, 'railway': 525, 'morcar': 553, 'plainly': 748, 'almost': 342, 'followed': 755, 'ten': 738, 'miss': 424, 'nor': 596, 'desperate': 813, 'all': 22, 'them': 61, 'make': 212, 'sounded': 842, 'doesn': 173, 'rome': 512, 'the': 1, 'written': 649, 'read': 704, 'pleaded': 1107, 'where': 1151, 'presently': 637, 'hadn': 526, 'northumbria': 556, 'nice': 215, 'house': 156, 'must': 69, 'usually': 459, 'stay': 361, 'handed': 1057, 'shrink': 741, 'ask': 263, 'words': 159, 'reply': 1110, 'funny': 418, 'machines': 892, 'lest': 1174, 'friends': 713, 'salt': 522, 'noise': 1074, 'brave': 628, 'besides': 827, 'rate': 270, 'washing': 937, 'existence': 877, 'cheerfully': 852, 'pop': 606, 'by': 71, 'meeting': 1023, 'measure': 870, 'dogs': 370, 'tiny': 445, 'pointed': 1163, 'christmas': 790, 'thing': 89, 'true': 629, 'wrong': 838, 'easy': 1040, 'poor': 109, 'roast': 733, 'suddenly': 175, 'somewhere': 409, 'certainly': 456, 'walrus': 905, 'soothing': 928, 'story': 575, 'digging': 893, 'can': 97, 'everybody': 1048, 'which': 85, 'clinging': 968, 'seven': 832, 'shall': 92, 'notice': 1010, 'useful': 542, 'always': 369, 'among': 261, 'currants': 766, 'insult': 1105, 'ann': 581, 'passed': 617, 'three': 331, 'chatte': 919, 'world': 301, 'quiet': 930, 'walk': 317, 'chain': 592, 'ventured': 725, 'hardly': 764, 'took': 139, 'your': 88, 'sure': 136, 'solemnly': 560, 'minutes': 467, 'brass': 1168, 'dozing': 656, 'bad': 520, 'hated': 941, 'further': 742, 'why': 132, 'because': 711, 'pegs': 616, 'might': 161, 'everything': 282, 'but': 19, 're': 322, 'run': 1160, 'pope': 990, 'neck': 699, 'candle': 347, 'you': 12, 'lesson': 533, 'distance': 315, 'forgotten': 460, 'narrow': 875, 'printed': 702, 'into': 72, 'wasting': 1093, 'hard': 501, 'good': 157, 'corner': 435, 'goes': 825, 'empty': 622, 'that': 13, 'begin': 696, 'advice': 753, 'ate': 770, 'rising': 1021, 've': 143, 'an': 133, 'window': 1180, 'work': 777, 'till': 868, 'name': 319, 'drowned': 902, 'spoke': 421, 'fire': 935, 'seems': 360, 'beds': 690, 'punished': 901, 'ferrets': 578, 'wherever': 888, 'returning': 808, 'buttered': 736, 'door': 95, 'breath': 1094, 'next': 210, 'seemed': 113, 'our': 197, 'than': 223, 'splash': 882, 'joined': 1112, 'allow': 978, 'shedding': 803, 's': 36, 'cry': 493, 'guessed': 1152, 'flashed': 601, 'poker': 716, 'turning': 379, 'opening': 779, 'puzzle': 822, 'tail': 285, 'dear': 64, 'waited': 466, 'my': 70, 'began': 75, 'dipped': 610, 'me': 42, 'frowning': 998, 'suppose': 527, 'caused': 568, 'however': 135, 'tea': 653, 'noticed': 306, 'wind': 666, 'engraved': 1171, 'william': 287, 'any': 121, 'deal': 357, 'cat': 267, 'rules': 454, 'placed': 1039, 'running': 562, 'bird': 1130, 'peeped': 587, 'sad': 569, 'multiplication': 834, 'guess': 872, 'truth': 659, 'set': 776, 'hastily': 279, 'filled': 397, 'came': 163, 'waistcoat': 393, 'went': 52, 'rather': 181, 'sudden': 363, 'open': 678, 'fitted': 684, 'paused': 1033, 'patriotic': 1003, 'accustomed': 993, 'feeling': 464, 'late': 298, 'her': 14, 'hurriedly': 1011, 'party': 240, 'when': 46, 'softly': 960, 'marked': 224, 'white': 128, 'exactly': 1058, 'directly': 1177, 'explain': 561, 'children': 274, 'matter': 428, 'curtsey': 646, 'live': 862, 'animal': 924, 'dog': 945, 'speaking': 529, 'crying': 470, 'soon': 76, 'without': 174, 'try': 194, 'vanished': 1157, 'practice': 635, 'second': 679, 'get': 67, 'doing': 1159, 'large': 153, 'mixed': 726, 'trotting': 497, 'panting': 1042, 'do': 49, 'another': 254, 'luckily': 1056, 'swam': 236, 'edwin': 552, 'climb': 749, 'how': 38, 'him': 293, 'begun': 429, 'antipathies': 639, 'here': 68, 'flame': 744, 'daisy': 591, 'canterbury': 1005, 'never': 115, 'hands': 514, 'cunning': 1095, 'sort': 179, 'll': 37, 'usurpation': 994, 'hedge': 607, 'deeply': 719, 'hoped': 1181, 'poison': 340, 'called': 199, 'killing': 624, 'fright': 923, 'rat': 685, 'up': 43, 'gently': 859, 'reaching': 806, 'no': 50, 'pair': 229, 'couldn': 655, 'drop': 623, 'ada': 824, 'hate': 544, 'inclined': 1034, 'a': 5, 'different': 821, 'trial': 571, 'draggled': 966, 'lessons': 313, 'would': 59, 'iii': 964, 'latin': 909, 'yet': 929, 'bend': 1101, 'nurse': 537, 'say': 83, 'it': 7, 'learnt': 632, 'and': 2, 'humbly': 1099, 'eye': 353, 'poky': 863, 'quite': 78, 'walked': 442, 'hung': 615, 'assembled': 965, 'itself': 297, 'thirteen': 831, 'anything': 213, 'knelt': 686, 'word': 642, 'wink': 912, 'oyster': 1125, 'making': 387, 'rabbit': 63, 'looking': 203, 'nervous': 743, 'zealand': 644, 'jury': 572, 'proceed': 1000, 'feet': 101, 'knife': 720, 'ma': 320, 'bowed': 1072, 'position': 1045, 'twice': 586, 'top': 405, 'orange': 619, 'eyed': 946, 'ran': 207, 'to': 3, 'stigand': 1002, 'listening': 640, 'eagerly': 371, 'favoured': 989, 'passionate': 927, 'footsteps': 1146, 'daisies': 594, 'positively': 980, 'doth': 513, 'heads': 318, 'upstairs': 1173, 'pretend': 763, 'both': 1088, 'tale': 377, 'taste': 461, 'near': 259, 'same': 193, 'pointing': 1051, 'until': 805, 'first': 104, 'dried': 807, 'has': 564, 'show': 535, 'looked': 129, 'who': 96, 'mabel': 283, 'leaders': 992, 'fan': 126, 'thought': 53, 'disagree': 722, 'shutting': 455, 'love': 799, 'with': 23, 'uncomfortable': 971, 'tried': 211, 'waters': 849, 'growing': 483, 'beautifully': 457, 'moved': 1142, 'trembling': 368, 'case': 524, 'finds': 1166, 'afraid': 268, 'easily': 1108, 'felt': 184, 'shoulders': 695, 'small': 167, 'forgot': 485, 'whole': 290, 'spades': 896, 'lonely': 1144, 'promised': 1082, 'on': 21, 'move': 1022, 'bats': 269, 'stockings': 783, 'wanted': 551, 'inches': 333, 'frog': 1008, 'sending': 489, 'welcome': 857, 'energetic': 1027, 'several': 312, 'want': 788, 'familiarly': 973, 'particular': 1127, 'could': 58, 'sometimes': 427, 'talking': 182, 'sends': 1148, 'labelled': 618, 'grand': 636, 'wise': 703, 'worth': 388, 'smile': 1032, 'oh': 51, 'manage': 323, 'ought': 299, 'pity': 1116, 'wonder': 103, 'improve': 846, 'wept': 900, 'slipped': 521, 'toffee': 735, 'sticks': 661, 'spirited': 1145, 'six': 830, 'remarking': 1135, 'quick': 1161, 'under': 300, 'tone': 196, 'opened': 446, 'shoes': 782, 'idea': 260, 'call': 1086, 'grammar': 910, 'passion': 962, 'marmalade': 620, 'along': 334, 'alas': 332, 'minute': 473, 'two': 227, 'fancy': 321, 'question': 162, 'scale': 851, 'nobody': 577, 'doorway': 694, 'beginning': 584, 'creatures': 963, 'lying': 354, 'shiver': 997, 'fact': 463, 'rapidly': 873, 'ringlets': 506, 'knot': 1103, 'hurrying': 665, 'gloves': 125, 'unpleasant': 710, 'coming': 255, 'out': 31, 'their': 180, 'spread': 855, 'other': 221, 'nonsense': 491, 'aloud': 408, 'milk': 652, 'doors': 440, 'long': 94, 'learn': 866, 'quicker': 1115, 'little': 24, 'had': 17, 'ready': 232, 'direction': 1162, 'bit': 432, 'feathers': 967, 'magpie': 1132, 'hundred': 954, 'at': 34, 'sides': 613, 'bat': 426, 'look': 154, 'declared': 1001, 'fifteen': 681, 'surprised': 277, 'fetch': 372, 'cupboards': 398, 'having': 245, 'asking': 423, 'absurd': 1067, 'ugh': 996, 'kind': 787, 'ahem': 985, 'laugh': 1070, 'bristling': 939, 'not': 29, 'in': 10, 'or': 57, 'sulky': 976, 'neat': 1167, 'himself': 811, 'know': 62, 'face': 346, 'est': 918, 'shan': 784, 'longer': 667, 'well': 127, 'ordering': 1178, 'savage': 812, 'venture': 1128, 'stop': 495, 'perhaps': 265, 'nowhere': 1154, 'hole': 243, 'later': 724, 'family': 940, 'morning': 502, 'smaller': 767, 'remained': 771, 'knows': 826, 'certain': 343, 'o': 366, 'wild': 708, 'cake': 355, 'garden': 169, 'theirs': 1077, 'houses': 898, 'burnt': 706, 'conqueror': 532, 'they': 39, 'remembered': 349, 'this': 28, 'pocket': 251, 'dodo': 90, 'snappishly': 1123, 'locked': 671, 'as': 15, 'expecting': 773, 'straight': 608, 'hurry': 339, 'terrier': 947, 'presents': 793, 'cried': 124, 'london': 837, 'mine': 507, 'attending': 1098, 'myself': 786, 'meet': 558, 'last': 291, 'catching': 538, 'didn': 262, 'don': 171, 'lodging': 897, 'kills': 956, 'though': 411, 'pool': 137, 'cur': 1092, 'round': 108, 'going': 116, 'meaning': 1028, 'something': 286, 'coast': 889, 'dream': 657, 'should': 218, 'crab': 576, 'ring': 549, 'licking': 936, 'like': 47, 'eats': 772, 'blown': 745, 'fender': 798, 'mind': 248, 'worse': 879, 'air': 264, 'from': 256, 'dinner': 951, 'take': 253, 'putting': 867, 'fortunately': 605, 'law': 1089, 'taught': 714, 'against': 476, 'come': 86, 'violently': 815, 'somehow': 884, 'decided': 746, 'lit': 668, 'leave': 472, 'people': 216, 'will': 191, 'replied': 378, 'acceptance': 1063, 'submitted': 991, 'lap': 840, 'birds': 200, 'before': 130, 'gallons': 804, 'gravely': 1061, 'listen': 414, 'plate': 1169, 'bottle': 273, 'hunting': 579, 'disappointment': 621, 'lately': 697, 'its': 114, 'changed': 235, 'away': 165, 'tumbling': 626, 'water': 365, 'whose': 988, 'circle': 1036, 'worm': 1009, 'prize': 1060, 'feelings': 925, 'locks': 677, 'melancholy': 559, 'pleasure': 590, 'wouldn': 404, 'cross': 970, 'remarkable': 390, 'general': 886, 'finger': 341, 'swimming': 528, 'talk': 237, 'fallen': 311, 'scolded': 756, 'give': 488, 'knowledge': 413, 'dropped': 358, 'conquest': 995, 'respectable': 765, 'telescope': 450, 'hold': 458, 'there': 55, 'makes': 479, 'darkness': 817, 'fountains': 693, 'paris': 511, 'box': 350, 'tunnel': 609, 'dears': 487, 'thump': 431, 'sighed': 1117, 'been': 166, 'now': 81, 'person': 352, 'prizes': 241, 'silence': 550, 'exact': 1037, 'change': 519, 'messages': 582, 'actually': 600, 'great': 117, 'done': 516, 'room': 583, 'confused': 1053, 'shut': 449, 'pounds': 955, 'wondered': 599, 'speak': 278, 'after': 102, 'french': 531, 'just': 141, 'sooner': 723, 'complained': 1076, 'day': 250, 'avoid': 874, 'angrily': 1102, 'pulled': 1055, 'still': 314, 'bleeds': 721, 'splendidly': 809, 'best': 225, 'reach': 469, 'pretexts': 1141, 'finish': 574, 'voice': 233, 'forehead': 1044, 'apple': 732, 'yes': 415, 'wooden': 895, 'mistake': 1164, 'mercia': 555, 'so': 30, 'mouse': 25, 'croquet': 760, 'belong': 676, 'queer': 234, 'declare': 880, 'bright': 335}\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# helper snippet \n",
    "############################\n",
    "nan_indexes = [797, 1090, 1093, 1094, 1095, 1096, 1097]\n",
    "for sequence in corpus:\n",
    "    for nan_index in nan_indexes:\n",
    "        if nan_index in sequence:\n",
    "            print(sequence)\n",
    "            break\n",
    "    \n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242, 6, 26, 1, 63, 243]\n"
     ]
    }
   ],
   "source": [
    "#create co-occurrence matrix\n",
    "#print(corpus[0])\n",
    "coMatrix = np.zeros((V, V))\n",
    "for sequence in corpus:\n",
    "    for idx, termId in enumerate(sequence):\n",
    "        sl = len(sequence)\n",
    "        ## select left window\n",
    "        leftw = sequence[max(idx - window_size_corpus, 0): idx]\n",
    "        ## select right window\n",
    "        rightw = sequence[idx + 1: min(idx + window_size_corpus + 1, sl)]\n",
    "        # update co-occurrence matrix\n",
    "        neighboors = leftw + rightw\n",
    "        for neighbor in neighboors:\n",
    "            coMatrix[termId, neighbor] += 1\n",
    "        \n",
    "\n",
    "# matrix normalization\n",
    "(rows, columns) = coMatrix.shape\n",
    "for rowIdx in range(rows):\n",
    "    # ignore first row with 0 entries everywhere\n",
    "    if (rowIdx > 0):\n",
    "        row = coMatrix[rowIdx]\n",
    "        total = row.sum()\n",
    "        # avoid division by zero in words that have no neighboors\n",
    "        if total > 0:\n",
    "            coMatrix[rowIdx] = np.divide(row, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02363823 0.0364851  ... 0.00051387 0.         0.        ]\n",
      " [0.         0.04752343 0.01204819 ... 0.00066934 0.00066934 0.00066934]\n",
      " ...\n",
      " [0.         0.16666667 0.16666667 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.125      ... 0.         0.         0.        ]\n",
      " [0.         0.         0.25       ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(coMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.02646503 0.03402647 ... 0.         0.         0.        ]]\n",
      "[[0.         0.03448276 0.03448276 ... 0.         0.         0.        ]]\n",
      "[[0.         0.13559322 0.00847458 ... 0.         0.         0.        ]]\n",
      "Cosine similarity between Alice and Dinah: [[0.39360011]]\n",
      "Cosine similarity between Alice and Rabbit: [[0.47890931]]\n",
      "Cosine similarity between Dinah and Rabbit: [[0.29862324]]\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "\n",
    "#find the word vectors for Alice, Dinah, and Rabbit\n",
    "aliceIdx = tokenizer.word_index['Alice'.lower()]\n",
    "dinahIdx = tokenizer.word_index['Dinah'.lower()]\n",
    "rabbitIdx = tokenizer.word_index['Rabbit'.lower()]\n",
    "\n",
    "aliceVector = coMatrix[aliceIdx].reshape(1, -1)\n",
    "dinahVector = coMatrix[dinahIdx].reshape(1, -1)\n",
    "rabbitVector = coMatrix[rabbitIdx].reshape(1, -1)\n",
    "print(aliceVector)\n",
    "print(dinahVector)\n",
    "print(rabbitVector)\n",
    "\n",
    "cosAD = cosine_similarity(aliceVector, dinahVector)\n",
    "print(\"Cosine similarity between Alice and Dinah: {}\".format(cosAD))\n",
    "cosAR = cosine_similarity(aliceVector, rabbitVector)\n",
    "print(\"Cosine similarity between Alice and Rabbit: {}\".format(cosAR))\n",
    "cosDR = cosine_similarity(dinahVector, rabbitVector)\n",
    "print(\"Cosine similarity between Dinah and Rabbit: {}\".format(cosDR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.18693993 0.22482059 0.25996004 0.26423806 0.2653115 ]]\n",
      "[[11  4  7  1  5 41]]\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "# create an array containing the cosine similarity values for alice and the rest\n",
    "nbrs = nn(n_neighbors=6, algorithm='brute', metric='cosine').fit(coMatrix)\n",
    "distances, indices = nbrs.kneighbors(aliceVector)\n",
    "print(distances)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the computed distances and the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we observe the six closest words to 'Alice'.\n",
    "\n",
    "Word with index 11 corresponds to 'Alice' itself. \n",
    "\n",
    "The words for the remaining indexes are:\n",
    "* 4: she\n",
    "* 7: it\n",
    "* 1: the\n",
    "* 5: a\n",
    "* 41: herself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "            \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # repeat the same word several times\n",
    "                    in_words.append([word])\n",
    "                    # add the context words\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                #print(in_words)\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "                #print(all_in)\n",
    "                #print(all_in[0].shape)\n",
    "                #print(all_out)\n",
    "                #print(all_out[0].shape)\n",
    "                #break\n",
    "    return (all_in,all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the preprocessed data of Skipgram\n",
    "def save_skipgram_data(filename, x, y):\n",
    "    f = open(filename ,'w')\n",
    "    for input,outcome  in zip(x,y):\n",
    "        input = np.concatenate(input)\n",
    "        f.write(\" \".join(map(str, list(input))))\n",
    "        f.write(\",\")\n",
    "        outcome = np.concatenate(outcome)\n",
    "        f.write(\" \".join(map(str,list(outcome))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file(filename):\n",
    "    f = open(filename ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPGRAM_2WORDS = \"data_skipgram_2words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,2,V)\n",
    "save_skipgram_data(SKIPGRAM_2WORDS, x, y)\n",
    "\n",
    "SKIPGRAM_4WORDS = \"data_skipgram_4words.txt\"\n",
    "x,y = generate_data_skipgram(corpus,4,V)\n",
    "save_skipgram_data(SKIPGRAM_4WORDS, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models\n",
    "skipgram_2words_model = Sequential()\n",
    "skipgram_2words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_model.add(Reshape((dim, )))\n",
    "skipgram_2words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_model = Sequential()\n",
    "skipgram_4words_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_model.add(Reshape((dim, )))\n",
    "skipgram_4words_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgrams\n",
    "skipgram_2words_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#define loss function for Skipgram\n",
    "skipgram_4words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram 2words losses: 0 -> 27182.093020916167\n",
      "skipgram 2words losses: 1 -> 27199.516684770726\n",
      "skipgram 2words losses: 2 -> 27204.492714703203\n",
      "skipgram 2words losses: 3 -> 27218.393353700736\n",
      "skipgram 2words losses: 4 -> 27224.77045786388\n",
      "skipgram 2words losses: 5 -> 27239.30565458543\n",
      "skipgram 2words losses: 6 -> 27243.966240525297\n",
      "skipgram 2words losses: 7 -> 27260.401532769247\n",
      "skipgram 2words losses: 8 -> 27263.237661421335\n",
      "skipgram 2words losses: 9 -> 27279.125965714484\n",
      "skipgram 2words losses: 10 -> 27284.51003623011\n",
      "skipgram 2words losses: 11 -> 27297.318689405944\n",
      "skipgram 2words losses: 12 -> 27301.81248903276\n",
      "skipgram 2words losses: 13 -> 27316.966321110744\n",
      "skipgram 2words losses: 14 -> 27319.665404140964\n",
      "skipgram 4words losses: 0 -> 29733.113516869314\n",
      "skipgram 4words losses: 1 -> 29745.975221336837\n",
      "skipgram 4words losses: 2 -> 29753.071226538235\n",
      "skipgram 4words losses: 3 -> 29765.893893063563\n",
      "skipgram 4words losses: 4 -> 29774.47852456633\n",
      "skipgram 4words losses: 5 -> 29787.554097473934\n",
      "skipgram 4words losses: 6 -> 29797.098824263063\n",
      "skipgram 4words losses: 7 -> 29806.567282915345\n",
      "skipgram 4words losses: 8 -> 29817.485627472754\n",
      "skipgram 4words losses: 9 -> 29829.297745287593\n",
      "skipgram 4words losses: 10 -> 29839.047165215317\n",
      "skipgram 4words losses: 11 -> 29850.29020082966\n",
      "skipgram 4words losses: 12 -> 29858.837761700404\n",
      "skipgram 4words losses: 13 -> 29869.33707255139\n",
      "skipgram 4words losses: 14 -> 29879.372340202553\n"
     ]
    }
   ],
   "source": [
    "#train Skipgram model\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_model.train_on_batch(x, y)\n",
    "    skipgram_2words_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_model.train_on_batch(x, y)\n",
    "    skipgram_4words_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_computed_vectors(filename, vectors):\n",
    "    f = open(filename ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_FILENAME = 'vectors_skipgram_2words.txt'\n",
    "skipgram_2words_model_vectors = skipgram_2words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_FILENAME, skipgram_2words_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_FILENAME = 'vectors_skipgram_4words.txt'\n",
    "skipgram_4words_model_vectors = skipgram_4words_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_FILENAME, skipgram_4words_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram models with additional dense layer\n",
    "\n",
    "skipgram_2words_modified_model = Sequential()\n",
    "skipgram_2words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_2words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_2words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "skipgram_4words_modified_model = Sequential()\n",
    "skipgram_4words_modified_model.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_4words_modified_model.add(Reshape((dim, )))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=dim, kernel_initializer='he_uniform', activation='relu'))\n",
    "skipgram_4words_modified_model.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n",
    "skipgram_2words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "skipgram_4words_modified_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram 2words modified losses: 0 -> 40298.47370827198\n",
      "skipgram 2words modified losses: 1 -> 36718.10098481178\n",
      "skipgram 2words modified losses: 2 -> 35214.463769078255\n",
      "skipgram 2words modified losses: 3 -> 33928.63531982899\n",
      "skipgram 2words modified losses: 4 -> 32948.596044421196\n",
      "skipgram 2words modified losses: 5 -> 32201.258150160313\n",
      "skipgram 2words modified losses: 6 -> 31658.562999129295\n",
      "skipgram 2words modified losses: 7 -> 31182.39734441042\n",
      "skipgram 2words modified losses: 8 -> 30829.39433300495\n",
      "skipgram 2words modified losses: 9 -> 30590.332844138145\n",
      "skipgram 2words modified losses: 10 -> 30271.689700245857\n",
      "skipgram 2words modified losses: 11 -> 29975.5885976851\n",
      "skipgram 2words modified losses: 12 -> 29731.610060065985\n",
      "skipgram 2words modified losses: 13 -> 29555.208632495254\n",
      "skipgram 2words modified losses: 14 -> 29367.53787581995\n",
      "skipgram 4words modified losses: 0 -> 40128.40839910507\n",
      "skipgram 4words modified losses: 1 -> 37018.70784020424\n",
      "skipgram 4words modified losses: 2 -> 35872.23134851456\n",
      "skipgram 4words modified losses: 3 -> 34914.722341775894\n",
      "skipgram 4words modified losses: 4 -> 34179.64830648899\n",
      "skipgram 4words modified losses: 5 -> 33768.42542386055\n",
      "skipgram 4words modified losses: 6 -> 33500.59350526333\n",
      "skipgram 4words modified losses: 7 -> 33269.6083278656\n",
      "skipgram 4words modified losses: 8 -> 33245.30694997311\n",
      "skipgram 4words modified losses: 9 -> 33071.43763178587\n",
      "skipgram 4words modified losses: 10 -> 32703.92114406824\n",
      "skipgram 4words modified losses: 11 -> 32427.067891180515\n",
      "skipgram 4words modified losses: 12 -> 32212.412558317184\n",
      "skipgram 4words modified losses: 13 -> 32015.525918483734\n",
      "skipgram 4words modified losses: 14 -> 31841.553291022778\n"
     ]
    }
   ],
   "source": [
    "#train model for Skipgram + dense\n",
    "EPOCHS = 15\n",
    "\n",
    "skipgram_2words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_2WORDS):\n",
    "        loss += skipgram_2words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_2words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 2words modified losses: {} -> {}\".format(ite, loss))\n",
    "\n",
    "\n",
    "skipgram_4words_modified_model_losses = []\n",
    "for ite in range(EPOCHS):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram_from_file(SKIPGRAM_4WORDS):\n",
    "        loss += skipgram_4words_modified_model.train_on_batch(x, y)\n",
    "    skipgram_4words_modified_model_losses.append((ite, loss))\n",
    "    print(\"skipgram 4words modified losses: {} -> {}\".format(ite, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save computed vectors\n",
    "VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME = 'vectors_skipgram_2words_modified.txt'\n",
    "skipgram_2words_modified_model_vectors = skipgram_2words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_2WORDS_MODIFIED_FILENAME, skipgram_2words_modified_model_vectors)\n",
    "\n",
    "VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME = 'vectors_skipgram_4words_modified.txt'\n",
    "skipgram_4words_modified_model_vectors = skipgram_4words_modified_model.get_weights()[0]\n",
    "save_computed_vectors(VECTORS_SKIPGRAM_4WORDS_MODIFIED_FILENAME, skipgram_4words_modified_model_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 2,\n",
       " 'funny': 418,\n",
       " 'hippopotamus': 906,\n",
       " 'downward': 638,\n",
       " 'peeped': 587,\n",
       " 'hide': 1031,\n",
       " 'changed': 235,\n",
       " 'complained': 1076,\n",
       " 'gave': 474,\n",
       " 'histories': 705,\n",
       " 'lap': 840,\n",
       " 'conquest': 995,\n",
       " 'family': 940,\n",
       " 'softly': 960,\n",
       " 'sad': 569,\n",
       " 'his': 151,\n",
       " 'pale': 961,\n",
       " 'like': 47,\n",
       " 'sharply': 471,\n",
       " 'overhead': 664,\n",
       " 'book': 205,\n",
       " 'queer': 234,\n",
       " 'gloves': 125,\n",
       " 'yet': 929,\n",
       " 'exactly': 1058,\n",
       " 'mercia': 555,\n",
       " 'cool': 692,\n",
       " 'choked': 1079,\n",
       " 'low': 186,\n",
       " 'lodging': 897,\n",
       " 'terrier': 947,\n",
       " 'prize': 1060,\n",
       " 'northumbria': 556,\n",
       " 'crocodile': 845,\n",
       " 'being': 362,\n",
       " 'respectable': 765,\n",
       " 'naturedly': 1153,\n",
       " 'afterwards': 597,\n",
       " 'time': 66,\n",
       " 'paused': 1033,\n",
       " 'who': 96,\n",
       " 'purring': 933,\n",
       " 'whose': 988,\n",
       " 'opening': 779,\n",
       " 'right': 158,\n",
       " 'over': 100,\n",
       " 'generally': 348,\n",
       " 'considering': 382,\n",
       " 'red': 715,\n",
       " 'directions': 795,\n",
       " 'myself': 786,\n",
       " 'dear': 64,\n",
       " 'sentence': 920,\n",
       " 'or': 57,\n",
       " 'day': 250,\n",
       " 'rabbit': 63,\n",
       " 'heard': 230,\n",
       " 'confused': 1053,\n",
       " 'next': 210,\n",
       " 'chapter': 242,\n",
       " 'shedding': 803,\n",
       " 'carefully': 1134,\n",
       " 'either': 140,\n",
       " 'let': 105,\n",
       " 'familiarly': 973,\n",
       " 'flavour': 727,\n",
       " 'since': 1155,\n",
       " 'high': 168,\n",
       " 'quite': 78,\n",
       " 'with': 23,\n",
       " 'gallons': 804,\n",
       " 'feel': 383,\n",
       " 'see': 56,\n",
       " 'sleepy': 384,\n",
       " 'could': 58,\n",
       " 'mistake': 1164,\n",
       " 'might': 161,\n",
       " 'nile': 850,\n",
       " 'shrill': 926,\n",
       " 'hurt': 433,\n",
       " 'wept': 900,\n",
       " 'centre': 630,\n",
       " 'solid': 674,\n",
       " 'help': 499,\n",
       " 'general': 886,\n",
       " 'lessons': 313,\n",
       " 'drink': 338,\n",
       " 'almost': 342,\n",
       " 'improve': 846,\n",
       " 'yesterday': 819,\n",
       " 'such': 74,\n",
       " 'waters': 849,\n",
       " 'politely': 999,\n",
       " 'animals': 545,\n",
       " 'name': 319,\n",
       " 'paper': 700,\n",
       " 'after': 102,\n",
       " 'passion': 962,\n",
       " 'jaws': 861,\n",
       " 'hear': 296,\n",
       " 'has': 564,\n",
       " 'eats': 772,\n",
       " 'hands': 514,\n",
       " 'lying': 354,\n",
       " 'brown': 949,\n",
       " 'near': 259,\n",
       " 'added': 1084,\n",
       " 'voices': 1049,\n",
       " 'moved': 1142,\n",
       " 'showing': 634,\n",
       " 'conclusion': 887,\n",
       " 'crying': 470,\n",
       " 'four': 214,\n",
       " 'plenty': 612,\n",
       " 'answer': 326,\n",
       " 'glad': 419,\n",
       " 'ran': 207,\n",
       " 'ann': 581,\n",
       " 'mice': 325,\n",
       " 'stigand': 1002,\n",
       " 'did': 99,\n",
       " 'looking': 203,\n",
       " 'usual': 820,\n",
       " 'proceed': 1000,\n",
       " 'came': 163,\n",
       " 'thinking': 823,\n",
       " 'flame': 744,\n",
       " 'trial': 571,\n",
       " 'birds': 200,\n",
       " 'begged': 1081,\n",
       " 'white': 128,\n",
       " 'understand': 530,\n",
       " 'cat': 267,\n",
       " 'reading': 588,\n",
       " 'saw': 400,\n",
       " 'poor': 109,\n",
       " 'rapidly': 873,\n",
       " 'some': 177,\n",
       " 'followed': 755,\n",
       " 'smiling': 860,\n",
       " 'turning': 379,\n",
       " 'hall': 147,\n",
       " 'times': 359,\n",
       " 'calling': 1052,\n",
       " 'piece': 1059,\n",
       " 'stupid': 385,\n",
       " 'daughter': 1118,\n",
       " 'then': 79,\n",
       " 'great': 117,\n",
       " 'salt': 522,\n",
       " 'longed': 688,\n",
       " 'nervous': 743,\n",
       " 'kills': 956,\n",
       " 'golden': 188,\n",
       " 'fetch': 372,\n",
       " 'ordering': 1178,\n",
       " 'of': 8,\n",
       " 'thump': 431,\n",
       " 'if': 32,\n",
       " 'bright': 335,\n",
       " 'lock': 682,\n",
       " 'capital': 284,\n",
       " 'corner': 435,\n",
       " 'useful': 542,\n",
       " 'promised': 1082,\n",
       " 'glass': 222,\n",
       " 'creep': 768,\n",
       " 'burn': 717,\n",
       " 'thing': 89,\n",
       " 'declare': 880,\n",
       " 'hurrying': 665,\n",
       " 'sensation': 1131,\n",
       " 'on': 21,\n",
       " 'she': 4,\n",
       " 'quicker': 1115,\n",
       " 'wind': 666,\n",
       " 'leaders': 992,\n",
       " 'swimming': 528,\n",
       " 'nearly': 871,\n",
       " 'slippery': 751,\n",
       " 'come': 86,\n",
       " 'plate': 1169,\n",
       " 'door': 95,\n",
       " 'silence': 550,\n",
       " 'breath': 1094,\n",
       " 'an': 133,\n",
       " 'tears': 170,\n",
       " 'patriotic': 1003,\n",
       " 'splendidly': 809,\n",
       " 'particular': 1127,\n",
       " 'getting': 138,\n",
       " 'worse': 879,\n",
       " 'ahem': 985,\n",
       " 'houses': 898,\n",
       " 'curly': 948,\n",
       " 'nearer': 904,\n",
       " 'snappishly': 1123,\n",
       " 'please': 217,\n",
       " 'beg': 195,\n",
       " 'twenty': 833,\n",
       " 'every': 330,\n",
       " 'cold': 984,\n",
       " 'pine': 731,\n",
       " 'try': 194,\n",
       " 'ought': 299,\n",
       " 'past': 625,\n",
       " 'daisy': 591,\n",
       " 'pleaded': 1107,\n",
       " 'your': 88,\n",
       " 'tea': 653,\n",
       " 'custard': 730,\n",
       " 'really': 272,\n",
       " 'flowers': 691,\n",
       " 'frowning': 998,\n",
       " 'stop': 495,\n",
       " 'moment': 131,\n",
       " 'canary': 1138,\n",
       " 'currants': 766,\n",
       " 'trotting': 497,\n",
       " 'sends': 1148,\n",
       " 'railway': 525,\n",
       " 'row': 438,\n",
       " 'doing': 1159,\n",
       " 'savage': 812,\n",
       " 'meaning': 1028,\n",
       " 'sight': 328,\n",
       " 'fond': 478,\n",
       " 'mean': 1106,\n",
       " 'beds': 690,\n",
       " 'curiosity': 603,\n",
       " 'far': 486,\n",
       " 'perhaps': 265,\n",
       " 'shelves': 399,\n",
       " 'paws': 536,\n",
       " 'inches': 333,\n",
       " 'simple': 712,\n",
       " 'hair': 505,\n",
       " 'ready': 232,\n",
       " 'slowly': 303,\n",
       " 'stockings': 783,\n",
       " 'pop': 606,\n",
       " 'pair': 229,\n",
       " 'pity': 1116,\n",
       " 'advisable': 557,\n",
       " 'prizes': 241,\n",
       " 'voice': 233,\n",
       " 'hard': 501,\n",
       " 'would': 59,\n",
       " 'remembered': 349,\n",
       " 'lamps': 669,\n",
       " 'label': 701,\n",
       " 'took': 139,\n",
       " 'second': 679,\n",
       " 'rules': 454,\n",
       " 'better': 548,\n",
       " 'neat': 1167,\n",
       " 'dinah': 87,\n",
       " 'about': 40,\n",
       " 'earth': 410,\n",
       " 'run': 1160,\n",
       " 'forgot': 485,\n",
       " 'number': 890,\n",
       " 'caucus': 376,\n",
       " 'they': 39,\n",
       " 'end': 309,\n",
       " 'went': 52,\n",
       " 'pressed': 1043,\n",
       " 'though': 411,\n",
       " 'whiskers': 437,\n",
       " 'known': 974,\n",
       " 'shakespeare': 1046,\n",
       " 'too': 155,\n",
       " 'along': 334,\n",
       " 'neatly': 854,\n",
       " 'tart': 729,\n",
       " 'lovely': 740,\n",
       " 'ask': 263,\n",
       " 'bristling': 939,\n",
       " 'country': 643,\n",
       " 'impatiently': 1114,\n",
       " 'likely': 406,\n",
       " 'night': 324,\n",
       " 'larger': 448,\n",
       " 'head': 189,\n",
       " 'what': 44,\n",
       " 'ada': 824,\n",
       " 'nasty': 942,\n",
       " 'me': 42,\n",
       " 'kind': 787,\n",
       " 'grave': 1068,\n",
       " 'get': 67,\n",
       " 'angry': 534,\n",
       " 'learnt': 632,\n",
       " 'sister': 380,\n",
       " 'deeply': 719,\n",
       " 'fancying': 1176,\n",
       " 'knot': 1103,\n",
       " 'heap': 660,\n",
       " 'call': 1086,\n",
       " 'smile': 1032,\n",
       " 'gently': 859,\n",
       " 'pool': 137,\n",
       " 'climb': 749,\n",
       " 'thought': 53,\n",
       " 'any': 121,\n",
       " 'daresay': 913,\n",
       " 'cunning': 1095,\n",
       " 'nine': 492,\n",
       " 'hoping': 453,\n",
       " 'rest': 1047,\n",
       " 'garden': 169,\n",
       " 'desperate': 813,\n",
       " 'energetic': 1027,\n",
       " 'leave': 472,\n",
       " 'luckily': 1056,\n",
       " 'welcome': 857,\n",
       " 'other': 221,\n",
       " 'consultation': 972,\n",
       " 'tongue': 1121,\n",
       " 'offer': 1014,\n",
       " 'feelings': 925,\n",
       " 'cats': 107,\n",
       " 'mentioned': 1143,\n",
       " 'death': 1097,\n",
       " 'guess': 872,\n",
       " 'swim': 1156,\n",
       " 'esq': 796,\n",
       " 'clinging': 968,\n",
       " 'kid': 231,\n",
       " 'having': 245,\n",
       " 'usurpation': 994,\n",
       " 'moderate': 1017,\n",
       " 'remained': 771,\n",
       " 'knowledge': 413,\n",
       " 'position': 1045,\n",
       " 'sending': 489,\n",
       " 'box': 350,\n",
       " 'both': 1088,\n",
       " 'curtain': 680,\n",
       " 'marmalade': 620,\n",
       " 'eaten': 707,\n",
       " 'doth': 513,\n",
       " 'speaking': 529,\n",
       " 'fitted': 684,\n",
       " 'cried': 124,\n",
       " 'real': 1175,\n",
       " 'children': 274,\n",
       " 'possibly': 747,\n",
       " 'disagree': 722,\n",
       " 'sound': 641,\n",
       " 'trembling': 368,\n",
       " 'lonely': 1144,\n",
       " 'pounds': 955,\n",
       " 'humbly': 1099,\n",
       " 'roast': 733,\n",
       " 'thousand': 631,\n",
       " 'fanning': 818,\n",
       " 'licking': 936,\n",
       " 'inquisitively': 911,\n",
       " 'for': 18,\n",
       " 'hadn': 526,\n",
       " 'when': 46,\n",
       " 'grammar': 910,\n",
       " 'insult': 1105,\n",
       " 'farmer': 953,\n",
       " 'shook': 1113,\n",
       " 'swam': 236,\n",
       " 'longitude': 417,\n",
       " 'crown': 1015,\n",
       " 'wonder': 103,\n",
       " 'alone': 515,\n",
       " 'passage': 327,\n",
       " 'unpleasant': 710,\n",
       " 'telescopes': 698,\n",
       " 'why': 132,\n",
       " 'sand': 894,\n",
       " 'practice': 635,\n",
       " 'locks': 677,\n",
       " 'set': 776,\n",
       " 'gravely': 1061,\n",
       " 'iii': 964,\n",
       " 'spades': 896,\n",
       " 'listening': 640,\n",
       " 'live': 862,\n",
       " 'suddenly': 175,\n",
       " 'crossly': 1006,\n",
       " 'whole': 290,\n",
       " 'oyster': 1125,\n",
       " 'milk': 652,\n",
       " 'feet': 101,\n",
       " 'just': 141,\n",
       " 'solemn': 1073,\n",
       " 'certainly': 456,\n",
       " 'subject': 539,\n",
       " 'leap': 921,\n",
       " 'pointing': 1051,\n",
       " 'toys': 864,\n",
       " 'easy': 1040,\n",
       " 'disappointment': 621,\n",
       " 'ma': 320,\n",
       " 'dark': 305,\n",
       " 'question': 162,\n",
       " 'winter': 1035,\n",
       " 'digging': 893,\n",
       " 'pairs': 1182,\n",
       " 'before': 130,\n",
       " 'bank': 381,\n",
       " 'made': 206,\n",
       " 'dry': 164,\n",
       " 'against': 476,\n",
       " 'began': 75,\n",
       " 'fifth': 1100,\n",
       " 'noticed': 306,\n",
       " 'avoid': 874,\n",
       " 'law': 1089,\n",
       " 'dull': 774,\n",
       " 'three': 331,\n",
       " 'slipped': 521,\n",
       " 'are': 106,\n",
       " 'rome': 512,\n",
       " 'reaching': 806,\n",
       " 'tone': 196,\n",
       " 'nurse': 537,\n",
       " 'started': 394,\n",
       " 'conduct': 1016,\n",
       " 'fire': 935,\n",
       " 'spirited': 1145,\n",
       " 'kept': 281,\n",
       " 'reply': 1110,\n",
       " 'way': 45,\n",
       " 'somewhere': 409,\n",
       " 'course': 204,\n",
       " 'chatte': 919,\n",
       " 'true': 629,\n",
       " 'hated': 941,\n",
       " 'two': 227,\n",
       " 'reach': 469,\n",
       " 'skurried': 816,\n",
       " 'afraid': 268,\n",
       " 'station': 899,\n",
       " 'severely': 475,\n",
       " 'stairs': 627,\n",
       " 'making': 387,\n",
       " 'fan': 126,\n",
       " 'ah': 503,\n",
       " 'room': 583,\n",
       " 'sometimes': 427,\n",
       " 'dipped': 610,\n",
       " 'immediate': 1025,\n",
       " 'simply': 1071,\n",
       " 'engraved': 1171,\n",
       " 'fountains': 693,\n",
       " 'shape': 1038,\n",
       " 'adjourn': 1024,\n",
       " 'addressing': 1126,\n",
       " 'existence': 877,\n",
       " 'their': 180,\n",
       " 'belong': 676,\n",
       " 'soothing': 928,\n",
       " 'finger': 341,\n",
       " 'got': 118,\n",
       " 'circle': 1036,\n",
       " 'whisper': 1085,\n",
       " 'which': 85,\n",
       " 'age': 504,\n",
       " 'put': 178,\n",
       " 'paris': 511,\n",
       " 'leaves': 662,\n",
       " 'measure': 870,\n",
       " 'miles': 407,\n",
       " 'minute': 473,\n",
       " 'now': 81,\n",
       " 'est': 918,\n",
       " 'where': 1151,\n",
       " 'thimble': 294,\n",
       " 'fishes': 858,\n",
       " 'legs': 750,\n",
       " 'housemaid': 1165,\n",
       " 'watch': 392,\n",
       " 'shore': 543,\n",
       " 'many': 310,\n",
       " 'knows': 826,\n",
       " 'puzzle': 822,\n",
       " 'first': 104,\n",
       " 'grand': 636,\n",
       " 'acceptance': 1063,\n",
       " 'life': 356,\n",
       " 'take': 253,\n",
       " 'm': 77,\n",
       " 'wild': 708,\n",
       " 'brass': 1168,\n",
       " 'learn': 866,\n",
       " 'eagerly': 371,\n",
       " 'looked': 129,\n",
       " 'curious': 345,\n",
       " 'six': 830,\n",
       " 'later': 724,\n",
       " 'the': 1,\n",
       " 'latitude': 416,\n",
       " 'among': 261,\n",
       " 'care': 769,\n",
       " 'five': 828,\n",
       " 'will': 191,\n",
       " 'condemn': 1096,\n",
       " 'advise': 752,\n",
       " 'guessed': 1152,\n",
       " 'not': 29,\n",
       " 're': 322,\n",
       " 'ignorant': 648,\n",
       " 'ears': 436,\n",
       " 'legged': 673,\n",
       " 'sulky': 976,\n",
       " 'young': 1122,\n",
       " 'ou': 917,\n",
       " 'walked': 442,\n",
       " 'schoolroom': 633,\n",
       " 'matter': 428,\n",
       " 'side': 441,\n",
       " 'hope': 650,\n",
       " 'drowned': 902,\n",
       " 'minutes': 467,\n",
       " 'out': 31,\n",
       " 'talk': 237,\n",
       " 'new': 420,\n",
       " 'shutting': 455,\n",
       " 'meet': 558,\n",
       " 'somebody': 307,\n",
       " 'apple': 732,\n",
       " 'waited': 466,\n",
       " 'venture': 1128,\n",
       " 'jumped': 663,\n",
       " 'sudden': 363,\n",
       " 'until': 805,\n",
       " 'wrapping': 1133,\n",
       " 'words': 159,\n",
       " 'uncomfortable': 971,\n",
       " 'us': 373,\n",
       " 'allow': 978,\n",
       " 'bend': 1101,\n",
       " 'harm': 907,\n",
       " 'jury': 572,\n",
       " 'without': 174,\n",
       " 'deep': 302,\n",
       " 'dare': 1069,\n",
       " 'race': 202,\n",
       " 'found': 73,\n",
       " 'lately': 697,\n",
       " 'must': 69,\n",
       " 'forgotten': 460,\n",
       " 'means': 1007,\n",
       " 'stopping': 611,\n",
       " 'dripping': 969,\n",
       " 'splashing': 903,\n",
       " 'eyes': 176,\n",
       " 'advice': 753,\n",
       " 'because': 711,\n",
       " 'says': 541,\n",
       " 's': 36,\n",
       " 'd': 198,\n",
       " 'called': 199,\n",
       " 'twelve': 829,\n",
       " 'knife': 720,\n",
       " 'left': 351,\n",
       " 'meeting': 1023,\n",
       " 'him': 293,\n",
       " 'machines': 892,\n",
       " 'confusion': 1075,\n",
       " 'small': 167,\n",
       " 'canterbury': 1005,\n",
       " 'everything': 282,\n",
       " 'toffee': 735,\n",
       " 'shoes': 782,\n",
       " 'eye': 353,\n",
       " 'muttering': 498,\n",
       " 'shiver': 997,\n",
       " 'fright': 923,\n",
       " 'managed': 403,\n",
       " 'manage': 323,\n",
       " 'fact': 463,\n",
       " 'give': 488,\n",
       " 'ferrets': 578,\n",
       " 'pet': 1129,\n",
       " 'lory': 201,\n",
       " 'am': 160,\n",
       " 'atheling': 1013,\n",
       " 'labelled': 618,\n",
       " 'those': 271,\n",
       " 'seven': 832,\n",
       " 'longer': 667,\n",
       " 'seaside': 885,\n",
       " 'passed': 617,\n",
       " 'animal': 924,\n",
       " 'knew': 451,\n",
       " 'it': 7,\n",
       " 'nonsense': 491,\n",
       " 'home': 308,\n",
       " 'executed': 1150,\n",
       " 'few': 337,\n",
       " 'blown': 745,\n",
       " 'knowing': 979,\n",
       " 'pocket': 251,\n",
       " 'caused': 568,\n",
       " 'this': 28,\n",
       " 'frightened': 518,\n",
       " 'water': 365,\n",
       " 'asked': 1050,\n",
       " 'doorway': 694,\n",
       " 'field': 604,\n",
       " 'dogs': 370,\n",
       " 'commotion': 959,\n",
       " 'latin': 909,\n",
       " 'forehead': 1044,\n",
       " 'further': 742,\n",
       " 'poison': 340,\n",
       " 'bent': 1030,\n",
       " 'melancholy': 559,\n",
       " 'late': 298,\n",
       " 'shan': 784,\n",
       " 'were': 65,\n",
       " 'except': 675,\n",
       " 'hot': 249,\n",
       " 'laugh': 1070,\n",
       " 'bring': 757,\n",
       " 'sides': 613,\n",
       " 'there': 55,\n",
       " 'punished': 901,\n",
       " 'pour': 848,\n",
       " 'begin': 696,\n",
       " 'large': 153,\n",
       " 'elegant': 1064,\n",
       " 'waistcoat': 393,\n",
       " 'common': 775,\n",
       " 'presently': 637,\n",
       " 'be': 20,\n",
       " 'listen': 414,\n",
       " 'brightened': 739,\n",
       " 'bill': 1149,\n",
       " 'lesson': 533,\n",
       " 'don': 171,\n",
       " 'maps': 614,\n",
       " 'direction': 1162,\n",
       " 'adoption': 1026,\n",
       " 'deal': 357,\n",
       " 'sure': 136,\n",
       " 'quiver': 922,\n",
       " 'putting': 867,\n",
       " 'by': 71,\n",
       " 'rat': 685,\n",
       " 'frog': 1008,\n",
       " 'patted': 1080,\n",
       " 'play': 865,\n",
       " 'bleeds': 721,\n",
       " 'doesn': 173,\n",
       " 'absurd': 1067,\n",
       " 'lest': 1174,\n",
       " 'pointed': 1163,\n",
       " 'brave': 628,\n",
       " 'neck': 699,\n",
       " 'dried': 807,\n",
       " 'wise': 703,\n",
       " 'seldom': 754,\n",
       " 'notion': 915,\n",
       " 'wasting': 1093,\n",
       " 'smaller': 767,\n",
       " 'edwin': 552,\n",
       " 'important': 986,\n",
       " 'change': 519,\n",
       " 'favoured': 989,\n",
       " 'pleasure': 590,\n",
       " 'fallen': 311,\n",
       " 'normans': 1019,\n",
       " 'multiplication': 834,\n",
       " 'met': 1087,\n",
       " 'turkey': 734,\n",
       " 'happened': 336,\n",
       " 'said': 16,\n",
       " 'mixed': 726,\n",
       " 'falling': 396,\n",
       " 'sir': 500,\n",
       " 'through': 144,\n",
       " 'anxiously': 276,\n",
       " 'duchess': 280,\n",
       " 'hung': 615,\n",
       " 'begun': 429,\n",
       " 'doors': 440,\n",
       " 'know': 62,\n",
       " 'distance': 315,\n",
       " 'ringlets': 506,\n",
       " 'presented': 1062,\n",
       " 'running': 562,\n",
       " 'signify': 835,\n",
       " 'soon': 76,\n",
       " 'hanging': 670,\n",
       " 'once': 98,\n",
       " 'cut': 718,\n",
       " 'bad': 520,\n",
       " 'key': 148,\n",
       " 'telescope': 450,\n",
       " 'till': 868,\n",
       " 'placed': 1039,\n",
       " 'puzzling': 509,\n",
       " 'fall': 257,\n",
       " 'away': 165,\n",
       " 'positively': 980,\n",
       " 'her': 14,\n",
       " 'curtsey': 646,\n",
       " 'actually': 600,\n",
       " 'claws': 856,\n",
       " 'close': 295,\n",
       " 'upstairs': 1173,\n",
       " 'thirteen': 831,\n",
       " 'earnestly': 658,\n",
       " 'antipathies': 639,\n",
       " 'as': 15,\n",
       " 'history': 367,\n",
       " 'soft': 938,\n",
       " 'shrink': 741,\n",
       " 'bit': 432,\n",
       " 'expecting': 773,\n",
       " 'anything': 213,\n",
       " 'hopeless': 801,\n",
       " 'vulgar': 943,\n",
       " 'house': 156,\n",
       " 'wanted': 551,\n",
       " 'able': 785,\n",
       " 'rats': 957,\n",
       " 'curiouser': 484,\n",
       " 'besides': 827,\n",
       " 'tiny': 445,\n",
       " 'window': 1180,\n",
       " 'scale': 851,\n",
       " 'have': 84,\n",
       " 'goes': 825,\n",
       " 'remember': 219,\n",
       " 'feeling': 464,\n",
       " 'creatures': 963,\n",
       " 'zealand': 644,\n",
       " 'belongs': 952,\n",
       " 'make': 212,\n",
       " 'short': 1065,\n",
       " 'throat': 1137,\n",
       " 'fear': 402,\n",
       " 'show': 535,\n",
       " 'delight': 683,\n",
       " 'draggled': 966,\n",
       " 'french': 531,\n",
       " 'spread': 855,\n",
       " 'cross': 970,\n",
       " 'led': 447,\n",
       " 'ones': 1078,\n",
       " 'orange': 619,\n",
       " 'several': 312,\n",
       " 'conversation': 944,\n",
       " 'worm': 1009,\n",
       " 'knocking': 1172,\n",
       " 'denial': 1091,\n",
       " 'w': 1170,\n",
       " 'fender': 798,\n",
       " 'how': 38,\n",
       " 'sounded': 842,\n",
       " 'returning': 808,\n",
       " 'at': 34,\n",
       " 'judge': 573,\n",
       " 'patience': 1124,\n",
       " 'lose': 1119,\n",
       " 'cake': 355,\n",
       " 'hold': 458,\n",
       " 'struck': 800,\n",
       " 'say': 83,\n",
       " 'ventured': 725,\n",
       " 'sea': 523,\n",
       " 'mary': 580,\n",
       " 'candle': 347,\n",
       " 'hurriedly': 1011,\n",
       " 'comfits': 566,\n",
       " 'grow': 480,\n",
       " 'fancy': 321,\n",
       " 'throw': 950,\n",
       " 'ten': 738,\n",
       " 'himself': 811,\n",
       " 'footsteps': 1146,\n",
       " 'conqueror': 532,\n",
       " 'plainly': 748,\n",
       " 'planning': 791,\n",
       " 'word': 642,\n",
       " 'sort': 179,\n",
       " 'a': 5,\n",
       " 've': 143,\n",
       " 'rising': 1021,\n",
       " 'makes': 479,\n",
       " 'twice': 586,\n",
       " 'toast': 737,\n",
       " 'bottle': 273,\n",
       " 'crowded': 289,\n",
       " 'quick': 1161,\n",
       " 'pretending': 762,\n",
       " 'c': 1083,\n",
       " 'continued': 1020,\n",
       " 'solemnly': 560,\n",
       " 'jar': 401,\n",
       " 'geography': 836,\n",
       " 'only': 122,\n",
       " 'certain': 343,\n",
       " 'else': 266,\n",
       " 'no': 50,\n",
       " 'wondering': 672,\n",
       " 'hunting': 579,\n",
       " 'alice': 11,\n",
       " 'darkness': 817,\n",
       " 'face': 346,\n",
       " 'behind': 329,\n",
       " 'largest': 780,\n",
       " 'our': 197,\n",
       " 'poky': 863,\n",
       " 'bowed': 1072,\n",
       " 'half': 149,\n",
       " 'bats': 269,\n",
       " 'others': 1111,\n",
       " 'australia': 645,\n",
       " 'cheated': 758,\n",
       " 'usually': 459,\n",
       " 'been': 166,\n",
       " 'hundred': 954,\n",
       " 'daisies': 594,\n",
       " 'cupboards': 398,\n",
       " 'directly': 1177,\n",
       " 'chin': 883,\n",
       " 'mouse': 25,\n",
       " 'cheerfully': 852,\n",
       " 'sticks': 661,\n",
       " 'different': 821,\n",
       " 'mine': 507,\n",
       " 'oh': 51,\n",
       " 'speak': 278,\n",
       " 'assembled': 965,\n",
       " 'indeed': 190,\n",
       " 'wet': 547,\n",
       " 'tale': 377,\n",
       " 'angrily': 1102,\n",
       " 'shall': 92,\n",
       " 'beasts': 709,\n",
       " 'burning': 602,\n",
       " 'london': 837,\n",
       " 'people': 216,\n",
       " 'wooden': 895,\n",
       " 'herself': 41,\n",
       " 'edgar': 1012,\n",
       " 'fell': 209,\n",
       " 'buttered': 736,\n",
       " 'hole': 243,\n",
       " 'written': 649,\n",
       " 'little': 24,\n",
       " 'remarkable': 390,\n",
       " 'table': 120,\n",
       " 'morcar': 553,\n",
       " 'washing': 937,\n",
       " 'dressed': 810,\n",
       " 'magpie': 1132,\n",
       " 'cur': 1092,\n",
       " 'nicely': 934,\n",
       " 'hand': 185,\n",
       " 'brother': 908,\n",
       " 'finds': 1166,\n",
       " 'person': 352,\n",
       " 'ii': 778,\n",
       " 'last': 291,\n",
       " 'shoulders': 695,\n",
       " 'growing': 483,\n",
       " 'we': 239,\n",
       " 'should': 218,\n",
       " 'argument': 975,\n",
       " 'wondered': 599,\n",
       " 'insolence': 1018,\n",
       " 'marked': 224,\n",
       " 'cheered': 1066,\n",
       " 'finished': 344,\n",
       " 'read': 704,\n",
       " 'alas': 332,\n",
       " 'seems': 360,\n",
       " 'despair': 1054,\n",
       " 'very': 27,\n",
       " 'william': 287,\n",
       " 'happen': 304,\n",
       " 'long': 94,\n",
       " 'coming': 255,\n",
       " 'much': 91,\n",
       " 'lazily': 931,\n",
       " 'find': 150,\n",
       " 'vanished': 1157,\n",
       " 'done': 516,\n",
       " 'lit': 668,\n",
       " 'best': 225,\n",
       " 'again': 54,\n",
       " 'walk': 317,\n",
       " 'sat': 226,\n",
       " 'hastily': 279,\n",
       " 'fixed': 983,\n",
       " 'miss': 424,\n",
       " 'timid': 814,\n",
       " 'rather': 181,\n",
       " 'pattering': 496,\n",
       " 'undo': 1104,\n",
       " 'escape': 876,\n",
       " 'nor': 596,\n",
       " 'hour': 1041,\n",
       " 'in': 10,\n",
       " 'open': 678,\n",
       " 'prosecute': 1090,\n",
       " 'waiting': 452,\n",
       " 'sadly': 443,\n",
       " 'older': 977,\n",
       " 'completely': 1158,\n",
       " 'mind': 248,\n",
       " 'believe': 1029,\n",
       " 'altogether': 468,\n",
       " 'always': 369,\n",
       " 'opened': 446,\n",
       " 'crossed': 839,\n",
       " 'pegs': 616,\n",
       " 'dropped': 358,\n",
       " 'round': 108,\n",
       " 'printed': 702,\n",
       " 'morning': 502,\n",
       " 'go': 82,\n",
       " 'used': 510,\n",
       " 'that': 13,\n",
       " 'refused': 981,\n",
       " 'good': 157,\n",
       " 'down': 26,\n",
       " 'spoke': 421,\n",
       " 'suppose': 527,\n",
       " 'from': 256,\n",
       " 'tell': 146,\n",
       " 'll': 37,\n",
       " 'tidy': 1179,\n",
       " 'look': 154,\n",
       " 'occurred': 598,\n",
       " 'up': 43,\n",
       " 'pretend': 763,\n",
       " 'child': 477,\n",
       " 'hate': 544,\n",
       " 'nobody': 577,\n",
       " 'grin': 853,\n",
       " 'locked': 671,\n",
       " 'than': 223,\n",
       " 'lost': 434,\n",
       " 'curtseying': 647,\n",
       " 'hardly': 764,\n",
       " 'whether': 386,\n",
       " 'story': 575,\n",
       " 'tunnel': 609,\n",
       " 'off': 80,\n",
       " 'hurried': 252,\n",
       " 'pretexts': 1141,\n",
       " 'across': 395,\n",
       " 'catch': 425,\n",
       " 'cherry': 728,\n",
       " 'is': 33,\n",
       " 'aloud': 408,\n",
       " 'stay': 361,\n",
       " 'beginning': 584,\n",
       " 'taste': 461,\n",
       " 'attending': 1098,\n",
       " 'another': 254,\n",
       " 'happens': 481,\n",
       " 'sits': 932,\n",
       " 'tail': 285,\n",
       " 'o': 366,\n",
       " 'work': 777,\n",
       " 'wouldn': 404,\n",
       " 'yourself': 494,\n",
       " 'friends': 713,\n",
       " 'ate': 770,\n",
       " 'saying': 183,\n",
       " 'these': 881,\n",
       " 'seen': 208,\n",
       " 'natural': 391,\n",
       " 'beautifully': 457,\n",
       " 'can': 97,\n",
       " 'burst': 869,\n",
       " 'shining': 847,\n",
       " 'going': 116,\n",
       " 'air': 264,\n",
       " 'violently': 815,\n",
       " 'itself': 297,\n",
       " 'liked': 563,\n",
       " 'narrow': 875,\n",
       " 'you': 12,\n",
       " 'fifteen': 681,\n",
       " 'surprised': 277,\n",
       " 'walrus': 905,\n",
       " 'rate': 270,\n",
       " 'offended': 238,\n",
       " 'one': 48,\n",
       " 'decided': 746,\n",
       " 'girl': 422,\n",
       " 'growled': 1109,\n",
       " 'notice': 1010,\n",
       " 'wherever': 888,\n",
       " 'into': 72,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train nearest neighbors model\n",
    "nn_skipgram_2words = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_2words_model_vectors)\n",
    "nn_skipgram_4words = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_4words_model_vectors)\n",
    "nn_skipgram_2words_modified = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_2words_modified_model_vectors)\n",
    "nn_skipgram_4words_modified = nn(n_neighbors=5, algorithm='brute', metric='cosine').fit(skipgram_4words_modified_model_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183, 100)\n",
      "1182\n"
     ]
    }
   ],
   "source": [
    "## --- SNIPPET USED TO UNDERSTAND SOME OPERATIONS ---- ##\n",
    "print(skipgram_2words_modified_model_vectors.shape)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "# implementation of the function argmax(w.(w3 + w2 - w1)), with w - w3 =' w2 - w1\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.neighbors import NearestNeighbors as nn\n",
    "#nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "\n",
    "def analogy(word1, word2, word3, word_vectors, nnmodel):\n",
    "    vector_w1 = word_vectors[tokenizer.word_index[word1]]\n",
    "    vector_w2 = word_vectors[tokenizer.word_index[word2]]\n",
    "    vector_w3 = word_vectors[tokenizer.word_index[word3]]\n",
    "    composed_vector = vector_w1 - vector_w2 + vector_w3\n",
    "    distances, indices = nnmodel.kneighbors(composed_vector.reshape(1, -1))\n",
    "    return (distances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the four different word embeddings\n",
    "\n",
    "Implement your own function to perform the analogy task with.\n",
    "    \n",
    "Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    \n",
    "Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.\n",
    "    \n",
    "Visualize your results and interpret your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to try with: melancholy - funny + smile = sad\n",
    "\n",
    "(distances_skipgram_2words, indices_skipgram_2words) = analogy(\"cry\", \"funny\", \"smile\", \n",
    "                                                               skipgram_2words_model_vectors, \n",
    "                                                              nn_skipgram_2words)\n",
    "\n",
    "(distances_skipgram_4words, indices_skipgram_4words) = analogy(\"cry\", \"funny\", \"smile\", \n",
    "                                                               skipgram_4words_model_vectors, \n",
    "                                                              nn_skipgram_4words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile\n",
      "melancholy\n",
      "game\n",
      "soothing\n",
      "patriotic\n"
     ]
    }
   ],
   "source": [
    "#print(distances_skipgram_2words)\n",
    "#print(indices_skipgram_2words)\n",
    "for index in indices_skipgram_2words[0]:\n",
    "    print(inverted_index[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile\n",
      "melancholy\n",
      "hide\n",
      "yourself\n",
      "circle\n"
     ]
    }
   ],
   "source": [
    "#print(distances_skipgram_4words)\n",
    "#print(indices_skipgram_4words)\n",
    "for index in indices_skipgram_4words[0]:\n",
    "    print(inverted_index[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
