{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Microsoft COCO (Common Objects in Context) data set to train our \"Image Caption Retrieval Model\". This data set consists of pretrained 10-crop VGG19 features (Neural codes) and its corresponding text caption. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embeddings'\n",
    "MODEL_PATH = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create above directories and locate data set provided in directory 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading pairs of image (VGG19 features) - caption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "import collections\n",
    "\n",
    "np_train_data = np.load(os.path.join(DATA_PATH,'train_data.npy'))\n",
    "np_val_data = np.load(os.path.join(DATA_PATH,'val_data.npy'))\n",
    "\n",
    "train_data = collections.OrderedDict()\n",
    "for i in range(len(np_train_data.item())):\n",
    "    cap =  np_train_data.item()['caps']\n",
    "    img =  np_train_data.item()['ims']\n",
    "    train_data['caps'] = cap\n",
    "    train_data['ims'] = img\n",
    "    \n",
    "val_data = collections.OrderedDict()\n",
    "for i in range(len(np_val_data.item())):\n",
    "    cap =  np_val_data.item()['caps']\n",
    "    img =  np_val_data.item()['ims']\n",
    "    val_data['caps'] = cap\n",
    "    val_data['ims'] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a woman wearing a net on her head cutting a cake'\n",
      "(10000, 4096)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# example of caption\n",
    "print(train_data['caps'][0])\n",
    "print(train_data['ims'].shape)\n",
    "print(len(train_data['caps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00109166 0.         0.         ... 0.         0.         0.        ]\n",
      "(5000, 4096)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# example of pre-computed VGG19 features\n",
    "print(val_data['ims'][0])\n",
    "print(val_data['ims'].shape)\n",
    "print(len(val_data['caps']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading caption and information about its corresponding raw images from Microsoft COCO website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "# use them for your own additional preprocessing step\n",
    "# to map precomputed features and location of raw images \n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'instances_val2014.json')) as json_file:\n",
    "    coco_instances_val = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH,'captions_val2014.json')) as json_file:\n",
    "    coco_caption_val = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own function to map pairs of precomputed features and filepath of raw images\n",
    "# this will be used later for visualization part\n",
    "# simple approach: based on matched text caption (see json file)\n",
    "\n",
    "def match_features_and_filepath(index, features_data, coco_annotations, coco_images):\n",
    "    low_index = index*5\n",
    "    top_index = (index+1)*5\n",
    "    features_captions = features_data['caps'][low_index: top_index]\n",
    "    features_vector = features_data['ims'][index]\n",
    "    img_id = None\n",
    "    selected_caption = None\n",
    "    \n",
    "    for caption in features_captions:\n",
    "        if img_id != None:\n",
    "            break\n",
    "        str_caption = caption.decode('utf-8')\n",
    "        for annotation in coco_annotations:\n",
    "            coco_caption = annotation['caption']\n",
    "            if coco_caption.endswith('.'):\n",
    "                coco_caption = coco_caption[:-1]\n",
    "            if str_caption.lower() == coco_caption.lower():\n",
    "                selected_caption = str_caption\n",
    "                img_id = annotation['image_id']\n",
    "                #print(annotation)\n",
    "                break\n",
    "    \n",
    "    img_url = None\n",
    "    for img_data in coco_images:\n",
    "        if img_data['id'] == img_id:\n",
    "            img_url = img_data['coco_url']\n",
    "            break\n",
    "    \n",
    "    return (img_url, selected_caption, features_vector)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000522418.jpg', 'a woman cutting a large white sheet cake', array([0.00495117, 0.        , 0.        , ..., 0.        , 0.00079085,\n",
      "       0.01148911], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000318219.jpg', 'a young boy standing in front of a computer keyboard', array([0.00460475, 0.02057743, 0.01465815, ..., 0.        , 0.        ,\n",
      "       0.02613593], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000554625.jpg', 'a boy wearing headphones using one computer in a long row of computers', array([0.00487754, 0.0193617 , 0.00561136, ..., 0.        , 0.        ,\n",
      "       0.01967784], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000397133.jpg', 'a man is in a kitchen making pizzas', array([0.03168815, 0.02294972, 0.00275241, ..., 0.        , 0.        ,\n",
      "       0.00495452], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000574769.jpg', 'a woman in a room with a cat', array([0.00537503, 0.00276216, 0.03403776, ..., 0.        , 0.03405748,\n",
      "       0.00679124], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000309022.jpg', 'a kitchen has all stainless steel appliances and counters', array([0.01313584, 0.01320351, 0.04603083, ..., 0.00181162, 0.        ,\n",
      "       0.01283785], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000005802.jpg', 'chefs preparing food in a professional metallic style kitchen', array([2.1481903e-02, 0.0000000e+00, 5.9161161e-05, ..., 0.0000000e+00,\n",
      "       7.7512334e-03, 3.0743960e-02], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000222564.jpg', 'two cooks are cooking the food someone ordered at this restaurant', array([0.03231374, 0.00249605, 0.07357662, ..., 0.00164444, 0.        ,\n",
      "       0.01275615], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000118113.jpg', 'this is a very dark picture of a room with a shelf', array([0.01480379, 0.02325615, 0.        , ..., 0.        , 0.00504187,\n",
      "       0.        ], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000193271.jpg', 'a kitchen filled with black appliances and lots of counter top space', array([0.00124641, 0.05636261, 0.02407175, ..., 0.        , 0.01012944,\n",
      "       0.01593034], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# debug implementation of previous function\n",
    "for i in range(10):\n",
    "    image_info = match_features_and_filepath(i, train_data, coco_caption_val['annotations'], coco_caption_val['images'])\n",
    "    print(image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 11473\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "def build_dictionary(text):\n",
    "\n",
    "    wordcount = OrderedDict()\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "    words = list(wordcount.keys())\n",
    "    freqs = list(wordcount.values())\n",
    "    sorted_idx = np.argsort(freqs)[::-1]\n",
    "    \n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict['<pad>'] = 0\n",
    "    worddict['<unk>'] = 1\n",
    "    for idx, sidx in enumerate(sorted_idx):\n",
    "        worddict[words[sidx]] = idx+2  # 0: <pad>, 1: <unk>\n",
    "    \n",
    "\n",
    "    return worddict\n",
    "\n",
    "# use the resulting vocabulary index as your look up dictionary\n",
    "# to transform raw text into integer sequences\n",
    "\n",
    "all_captions = []\n",
    "all_captions = train_data['caps'] + val_data['caps']\n",
    "\n",
    "# decode bytes to string format\n",
    "caps = []\n",
    "for w in all_captions:\n",
    "    caps.append(w.decode())\n",
    "    \n",
    "words_indices = build_dictionary(caps)\n",
    "print ('Dictionary size: ' + str(len(words_indices)))\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image - Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "#from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "#from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_neural_codes (Dense)   (None, 1024)              4195328   \n",
      "=================================================================\n",
      "Total params: 4,195,328\n",
      "Trainable params: 4,195,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE \n",
    "image_input = Input(shape=(4096, ))\n",
    "\n",
    "image_model = Sequential()\n",
    "image_model.add(Dense(1024, input_shape=(4096, ), name=\"image_neural_codes\", activation=\"sigmoid\"))\n",
    "image_model.summary()\n",
    "\n",
    "image_encoding = image_model(image_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pretrained embedding\n",
    "def load_embedding(vocab, dimension, filename):\n",
    "    print('loading embeddings from \"%s\"' % filename, file=sys.stderr)\n",
    "    embedding = np.zeros((max(vocab.values()) + 1, dimension), dtype=np.float32)\n",
    "    seen = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            tokens = line.strip().split(' ')\n",
    "            if len(tokens) == dimension + 1:\n",
    "                word = tokens[0]\n",
    "                if word in vocab:\n",
    "                    embedding[vocab[word]] = [float(x) for x in tokens[1:]]\n",
    "                    seen.add(word)\n",
    "                    if len(seen) == len(vocab):\n",
    "                        break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"data/glove.6B.100d.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights = load_embedding(words_indices, 100, os.path.join(DATA_PATH,'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding (Embedding)   (None, 50, 100)           1147300   \n",
      "_________________________________________________________________\n",
      "caption_neural_codes (LSTM)  (None, 1024)              4608000   \n",
      "=================================================================\n",
      "Total params: 5,755,300\n",
      "Trainable params: 4,608,000\n",
      "Non-trainable params: 1,147,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "caption_max_length = 50\n",
    "embedding_size = 100\n",
    "rnn_output_units = 1024\n",
    "# For embedding layer, initialize with pretrained word embedding (GloVe)\n",
    "caption_input = Input(shape=(caption_max_length, ), name='input_layer', dtype='int32')\n",
    "noisy_input = Input(shape=(caption_max_length, ),  name='noisy_input_layer', dtype='int32')\n",
    "\n",
    "caption_model = Sequential()\n",
    "caption_model.add(Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "                            trainable=False, name='word_embedding'))\n",
    "caption_model.add(LSTM(rnn_output_units, name='caption_neural_codes'))\n",
    "# cts image and text (caption) in the same representation space. To buildthis model, you need two (2) sub models:  model for encoding image and model for encoding textas shown in figure 1.  You also need to train the model so that an image is c\n",
    "#model = Sequential()\n",
    "#model.add(Lambda(binarize, output_shape=binarize_outshape,name='char_embedding', \\\n",
    "#                 input_shape=(max_sequence_length,), dtype='int32'))\n",
    "#model.add(LSTM(rnn_dim, name='lstm_layer'))\n",
    "#model.add(Dense(1 , name='prediction_layer', activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## the input lenght of the embedding layer corresponds to the lenght of the encoded text sequences\n",
    "## which have a maximum of 50 words\n",
    "#embedding_layer = Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "#                            trainable=False, name='word_embedding')\n",
    "#embedded_captions = embedding_layer(caption_input)\n",
    "#lstm_layer = LSTM(rnn_output_units, name='caption_neural_codes')(embedded_captions)\n",
    "print(caption_model.summary())\n",
    "caption_encoding = caption_model(caption_input)\n",
    "noisy_encoding = caption_model(noisy_input)\n",
    "#print(caption_encoding.summary())\n",
    "#print(noisy_encoding.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "dot_product = lambda x: K.dot(x[0], x[1])\n",
    "positive_examples = Lambda(dot_product)([image_encoding, caption_encoding])\n",
    "\n",
    "## think about how to implement this\n",
    "negative_examples = Lambda(dot_product)([image_encoding, noisy_encoding])\n",
    "# layer for computing dot product between tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training model\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading the training model\")\n",
    "training_model = Model(inputs=[image_input, caption_input, noisy_input], outputs=[positive_examples, negative_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sub-models for retrieving Neural codes\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading sub-models for retrieving Neural codes\")\n",
    "caption_model = Model(inputs=caption_input, outputs=caption_encoding)\n",
    "image_model = Model(inputs=image_input, outputs=image_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function as a loss for maximizing the margin between a positive and\n",
    "negative example.  If we call $p_i$ the score of the positive pair of the $i$-th example, and $n_i$ the score of the negative pair of that example, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "loss = \\sum_i{max(0, 1 -p_i + n_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    loss_ = K.sum(K.maximum(0., 1. - y_true + y_pred))\n",
    "    \n",
    "    return loss_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric for max-margin loss\n",
    "How many times did the positive pair effectively get a higher value than the negative pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.argmax((y_true, y_pred), axis=1))\n",
    "    #return K.sum(K.ceil(K.max(y_true - y_pred, 0.)))\n",
    "    # return K.sum(y_true > y_pred)\n",
    "#     merged = np.concatenate([y_true, y_pred], axis=1)\n",
    "#     return 1 - np.sum(np.argmax(merged, axis=1))/y_true.size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the training model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax() got multiple values for argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-13336cd77125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT CHANGE BELOW CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"compiling the training model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_margin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m                         \u001b[0mappend_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[0;34m(metrics, weights)\u001b[0m\n\u001b[1;32m    909\u001b[0m                             metric_result = weighted_metric_fn(y_true, y_pred,\n\u001b[1;32m    910\u001b[0m                                                                \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                                                                mask=masks[i])\n\u001b[0m\u001b[1;32m    912\u001b[0m                         \u001b[0mappend_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-bf1da540bc7c>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#return K.sum(K.ceil(K.max(y_true - y_pred, 0.)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# return K.sum(y_true > y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax() got multiple values for argument 'axis'"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "print (\"compiling the training model\")\n",
    "training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation for training the model\n",
    "\n",
    "* adjust the length of captions into fixed maximum length (50 words)\n",
    "* sampling caption for each image, while shuffling the image data\n",
    "* encode captions into integer format based on look-up vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['caps'][0:10])\n",
    "print(train_data['ims'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling one caption per image\n",
    "# return image_ids, caption_ids\n",
    "import random\n",
    "\n",
    "def sampling_img_cap(data):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # first sample for each image a caption\n",
    "    pairs_ids = []\n",
    "    \n",
    "    for image_id in range(len(data['ims'])):\n",
    "        caption_delta = random.randint(0, 4)\n",
    "        caption_id = image_id*5 + caption_delta\n",
    "        pairs_ids.append((image_id, caption_id))\n",
    "\n",
    "    # then shuffle the image data with the corresponding captions\n",
    "    random.shuffle(pairs_ids)\n",
    "    image_ids, caption_ids = zip(*pairs_ids)\n",
    "    \n",
    "    return image_ids, caption_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform raw text caption into integer sequences of fixed maximum length\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def prepare_caption(caption_ids, caption_data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    selected_captions = []\n",
    "    for i in caption_ids:\n",
    "        caption_text = caption_data[i]\n",
    "        caption_tokens = [words_indices[w] if w in words_indices.keys() else words_indices['<unk>'] for w in caption_text]\n",
    "        selected_captions.append(caption_tokens)\n",
    "        \n",
    "    selected_captions_arr = np.array(selected_captions)        \n",
    "    caption_seqs = sequence.pad_sequences(selected_captions_arr, maxlen=caption_max_length, value=0)\n",
    "      \n",
    "    return caption_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_caps = []\n",
    "for cap in train_data['caps']:\n",
    "    train_caps.append(cap.decode())\n",
    "\n",
    "val_caps = []\n",
    "for cap in val_data['caps']:\n",
    "    val_caps.append(cap.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_image_ids, train_caption_ids = sampling_img_cap(train_data)\n",
    "val_image_ids, val_caption_ids = sampling_img_cap(val_data)\n",
    "\n",
    "x_caption = prepare_caption(train_caption_ids, train_caps)\n",
    "x_image = train_data['ims'][np.array(train_image_ids)]\n",
    "\n",
    "x_val_caption = prepare_caption(val_caption_ids, val_caps)\n",
    "x_val_image = val_data['ims'][np.array(val_image_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create noise set for negative examples of image-fake caption and dummy output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not have real output with labels for training the model. Keras architecture expects labels, so we need to create dummy output -- which is numpy array of zeros. This dummy labels or output is never used since we compute loss function based on margin between positive examples (image-real caption) and negative examples (image-fake caption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_caption[0].shape)\n",
    "print(len(words_indices))\n",
    "print(x_caption[0])\n",
    "ids = np.random.choice(x_caption[0].shape[0], 10, replace=False)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def generate_noisy_caption(caption):\n",
    "    ids = np.random.choice(caption.shape[0], 20, replace=False)\n",
    "    values = np.random.choice(len(words_indices), 20)\n",
    "    #print(values)\n",
    "    #print(caption)\n",
    "    new_caption = caption\n",
    "    #print(new_caption)\n",
    "    \n",
    "    #for i in range\n",
    "    for i, val in zip(ids, values):\n",
    "        new_caption[i] = val\n",
    "    \n",
    "    return new_caption\n",
    "    \n",
    "#print(generate_noisy_caption(x_caption[0]))\n",
    "    \n",
    "def generate_noisy_captions(images, captions):\n",
    "    noisy_captions = np.empty(captions.shape, dtype=np.int32)\n",
    "    \n",
    "    i = 0\n",
    "    for img, caption in zip(images, captions):\n",
    "        noisy_caption = generate_noisy_caption(caption)\n",
    "        noisy_captions[i, :] = noisy_caption\n",
    "        i += 1\n",
    "    \n",
    "    return noisy_captions\n",
    "\n",
    "#print(x_caption[:2])\n",
    "#print(generate_noisy_captions(x_image[:2], x_caption[:2]))\n",
    "\n",
    "train_noise = generate_noisy_captions(x_image, x_caption)\n",
    "val_noise = generate_noisy_captions(x_val_image, x_val_caption)\n",
    "\n",
    "y_train_labels = np.zeros((len(x_image), rnn_output_units))\n",
    "y_val_labels = np.zeros((len(x_val_image), rnn_output_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_labels.shape)\n",
    "print(y_val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "X_train = \n",
    "Y_train = \n",
    "X_valid = \n",
    "Y_valid = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# fit the model on training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing models and weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "# Save model\n",
    "training_model.save(os.path.join(MODEL_PATH,'image_caption_model.h5'))\n",
    "# Save weight parameters\n",
    "training_model.save_weights(os.path.join(MODEL_PATH, 'weights_image_caption.hdf5'))\n",
    "\n",
    "# Save model for encoding caption and image\n",
    "caption_model.save(os.path.join(MODEL_PATH,'caption_model.h5'))\n",
    "image_model.save(os.path.join(MODEL_PATH,'image_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature extraction (Neural codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Use caption_model and image_model to produce \"Neural codes\" \n",
    "# for both image and caption from validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Caption Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display original image as query and its ground truth caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# choose one image_id from validation set\n",
    "# use this id to get filepath of image\n",
    "img_id = \n",
    "filepath_image = \n",
    "\n",
    "# display original caption\n",
    "original_caption = \n",
    "print(original_caption)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "img = image.load_img(os.path.join(IMAGE_DATA,filepath_image), target_size=(224,224))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve caption, given an image query\n",
    "\n",
    "def get_caption(image_filename, n=10):   \n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "get_caption(filepath_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given text query, display retrieved image, similarity score, and its original caption \n",
    "\n",
    "def search_image(text_caption, n=10):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider to use the following settings for image retrieval task.\n",
    "\n",
    "* use real caption that is available in validation set as a query.\n",
    "* use part of caption as query. For instance, instead of use the whole text sentence of the\n",
    "caption, you may consider to use key phrase or combination of words that is included in\n",
    "corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text query \n",
    "# text = 'two giraffes standing near trees'\n",
    "\n",
    "# YOUR QUERY-1\n",
    "text1 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR QUERY-2\n",
    "text2 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
