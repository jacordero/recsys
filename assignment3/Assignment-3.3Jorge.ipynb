{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Microsoft COCO (Common Objects in Context) data set to train our \"Image Caption Retrieval Model\". This data set consists of pretrained 10-crop VGG19 features (Neural codes) and its corresponding text caption. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embeddings'\n",
    "MODEL_PATH = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create above directories and locate data set provided in directory 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading pairs of image (VGG19 features) - caption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "import collections\n",
    "\n",
    "np_train_data = np.load(os.path.join(DATA_PATH,'train_data.npy'))\n",
    "np_val_data = np.load(os.path.join(DATA_PATH,'val_data.npy'))\n",
    "\n",
    "train_data = collections.OrderedDict()\n",
    "for i in range(len(np_train_data.item())):\n",
    "    cap =  np_train_data.item()['caps']\n",
    "    img =  np_train_data.item()['ims']\n",
    "    train_data['caps'] = cap\n",
    "    train_data['ims'] = img\n",
    "    \n",
    "val_data = collections.OrderedDict()\n",
    "for i in range(len(np_val_data.item())):\n",
    "    cap =  np_val_data.item()['caps']\n",
    "    img =  np_val_data.item()['ims']\n",
    "    val_data['caps'] = cap\n",
    "    val_data['ims'] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a woman wearing a net on her head cutting a cake'\n",
      "(10000, 4096)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# example of caption\n",
    "print(train_data['caps'][0])\n",
    "print(train_data['ims'].shape)\n",
    "print(len(train_data['caps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00109166 0.         0.         ... 0.         0.         0.        ]\n",
      "(5000, 4096)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# example of pre-computed VGG19 features\n",
    "print(val_data['ims'][0])\n",
    "print(val_data['ims'].shape)\n",
    "print(len(val_data['caps']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading caption and information about its corresponding raw images from Microsoft COCO website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "# use them for your own additional preprocessing step\n",
    "# to map precomputed features and location of raw images \n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'instances_val2014.json')) as json_file:\n",
    "    coco_instances_val = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH,'captions_val2014.json')) as json_file:\n",
    "    coco_caption_val = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own function to map pairs of precomputed features and filepath of raw images\n",
    "# this will be used later for visualization part\n",
    "# simple approach: based on matched text caption (see json file)\n",
    "\n",
    "\n",
    "def get_features_from_filename(image_filename, features_data, caption_data):\n",
    "    coco_annotations = coco_caption_val['annotations']\n",
    "    coco_images = coco_caption_val['images']\n",
    "\n",
    "    coco_img_index = None\n",
    "    for annotation in coco_images:\n",
    "        coco_filename = annotation['file_name']\n",
    "        if image_filename == coco_filename:\n",
    "            coco_img_index = annotation['id']\n",
    "            break;\n",
    "            \n",
    "    possible_captions = []\n",
    "    for annotation in coco_annotations:\n",
    "        #print(annotation.keys())\n",
    "        caption_img_id = annotation['image_id']\n",
    "        if coco_img_index == caption_img_id:\n",
    "            possible_captions.append(annotation['caption'])\n",
    "\n",
    "    #print(possible_captions)\n",
    "    selected_feature_id = None\n",
    "    selected_caption = None\n",
    "    for i, caption in enumerate(caption_data):\n",
    "        caption = caption.decode('utf-8')\n",
    "        #print(i, caption)\n",
    "        #break\n",
    "        for possible_caption in possible_captions:\n",
    "            if possible_caption.endswith('.'):\n",
    "                possible_caption = possible_caption[:-1]\n",
    "            if caption.lower() == possible_caption.lower():\n",
    "                selected_feature_id = i\n",
    "                selected_caption = caption\n",
    "                \n",
    "        if selected_feature_id != None:\n",
    "                break\n",
    "\n",
    "        \n",
    "    # (not needed anymore) map the selected feature id [i*5: (i+1)*5] to the actual id\n",
    "    #feature_id, _ = divmod(selected_feature_id, 5)\n",
    "    features_vector = features_data[selected_feature_id]\n",
    "    #compare selected captions with captions from \n",
    "    return (features_vector, selected_feature_id, selected_caption)\n",
    "    \n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "    \n",
    "def get_features_info(index, dataset_captions, dataset_images, coco_annotations, coco_images):\n",
    "    #low_index = index*5\n",
    "    #top_index = (index+1)*5\n",
    "    caption = dataset_captions[index]\n",
    "    str_caption = caption.decode('utf-8').strip()\n",
    "    #print(\"str_caption: {}\".format(str_caption))\n",
    "    features_vector = dataset_images[index]\n",
    "    img_id = None\n",
    "    selected_caption = None\n",
    "    \n",
    "    for annotation in coco_annotations:\n",
    "        coco_caption = annotation['caption']\n",
    "        coco_caption = coco_caption.strip()\n",
    "        if coco_caption.endswith('.'):\n",
    "            coco_caption = coco_caption[:-1]\n",
    "        if str_caption.lower() == coco_caption.lower():\n",
    "            selected_caption = str_caption\n",
    "            img_id = annotation['image_id']\n",
    "            #print(\"\\n*** Found image index for: {}\\n\".format(str_caption))\n",
    "            break\n",
    "        #if SequenceMatcher(None, str_caption.lower(), coco_caption.lower()).ratio() > 0.8:\n",
    "        #    print(\"caption: <{}>\".format(coco_caption))\n",
    "    \n",
    "    file_name = None\n",
    "    coco_url = None\n",
    "    for img_data in coco_images:\n",
    "        if img_data['id'] == img_id:\n",
    "            file_name = img_data['file_name']\n",
    "            coco_url = img_data['coco_url']\n",
    "            break\n",
    "    \n",
    "    return (file_name, coco_url, selected_caption, features_vector)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 11473\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "def build_dictionary(text):\n",
    "\n",
    "    wordcount = OrderedDict()\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "    words = list(wordcount.keys())\n",
    "    freqs = list(wordcount.values())\n",
    "    sorted_idx = np.argsort(freqs)[::-1]\n",
    "    \n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict['<pad>'] = 0\n",
    "    worddict['<unk>'] = 1\n",
    "    for idx, sidx in enumerate(sorted_idx):\n",
    "        worddict[words[sidx]] = idx+2  # 0: <pad>, 1: <unk>\n",
    "    \n",
    "\n",
    "    return worddict\n",
    "\n",
    "# use the resulting vocabulary index as your look up dictionary\n",
    "# to transform raw text into integer sequences\n",
    "\n",
    "all_captions = []\n",
    "all_captions = train_data['caps'] + val_data['caps']\n",
    "\n",
    "# decode bytes to string format\n",
    "caps = []\n",
    "for w in all_captions:\n",
    "    caps.append(w.decode())\n",
    "    \n",
    "words_indices = build_dictionary(caps)\n",
    "print ('Dictionary size: ' + str(len(words_indices)))\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image - Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Lambda, dot\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "#from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "#from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_neural_codes (Dense)   (None, 1024)              4195328   \n",
      "=================================================================\n",
      "Total params: 4,195,328\n",
      "Trainable params: 4,195,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE \n",
    "image_input = Input(shape=(4096, ))\n",
    "\n",
    "image_model = Sequential()\n",
    "image_model.add(Dense(1024, input_shape=(4096, ), name=\"image_neural_codes\", activation=\"sigmoid\"))\n",
    "image_model.summary()\n",
    "\n",
    "image_encoding = image_model(image_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model description \n",
    "\n",
    "We implemented the image model as shown in the pdf of this assignment. For the input we use an Input layer with shape (4096, ) because the VGG features for the images have 4096 dimensions. We only use a dense layer to reduce the input from 4096 to 1024 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pretrained embedding\n",
    "def load_embedding(vocab, dimension, filename):\n",
    "    print('loading embeddings from \"%s\"' % filename, file=sys.stderr)\n",
    "    embedding = np.zeros((max(vocab.values()) + 1, dimension), dtype=np.float32)\n",
    "    seen = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            tokens = line.strip().split(' ')\n",
    "            if len(tokens) == dimension + 1:\n",
    "                word = tokens[0]\n",
    "                if word in vocab:\n",
    "                    embedding[vocab[word]] = [float(x) for x in tokens[1:]]\n",
    "                    seen.add(word)\n",
    "                    if len(seen) == len(vocab):\n",
    "                        break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"data/glove.6B.100d.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights = load_embedding(words_indices, 100, os.path.join(DATA_PATH,'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 100)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding (Embedding)   (None, 50, 100)           1147300   \n",
      "_________________________________________________________________\n",
      "caption_neural_codes (LSTM)  (None, 1024)              4608000   \n",
      "=================================================================\n",
      "Total params: 5,755,300\n",
      "Trainable params: 4,608,000\n",
      "Non-trainable params: 1,147,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "caption_max_length = 50\n",
    "embedding_size = 100\n",
    "rnn_output_units = 1024\n",
    "# For embedding layer, initialize with pretrained word embedding (GloVe)\n",
    "caption_input = Input(shape=(caption_max_length, ), name='input_layer', dtype='int32')\n",
    "noisy_input = Input(shape=(caption_max_length, ),  name='noisy_input_layer', dtype='int32')\n",
    "\n",
    "caption_model = Sequential()\n",
    "caption_model.add(Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "                            trainable=False, name='word_embedding'))\n",
    "caption_model.add(LSTM(rnn_output_units, name='caption_neural_codes'))\n",
    "# cts image and text (caption) in the same representation space. To buildthis model, you need two (2) sub models:  model for encoding image and model for encoding textas shown in figure 1.  You also need to train the model so that an image is c\n",
    "#model = Sequential()\n",
    "#model.add(Lambda(binarize, output_shape=binarize_outshape,name='char_embedding', \\\n",
    "#                 input_shape=(max_sequence_length,), dtype='int32'))\n",
    "#model.add(LSTM(rnn_dim, name='lstm_layer'))\n",
    "#model.add(Dense(1 , name='prediction_layer', activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## the input lenght of the embedding layer corresponds to the lenght of the encoded text sequences\n",
    "## which have a maximum of 50 words\n",
    "#embedding_layer = Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "#                            trainable=False, name='word_embedding')\n",
    "#embedded_captions = embedding_layer(caption_input)\n",
    "#lstm_layer = LSTM(rnn_output_units, name='caption_neural_codes')(embedded_captions)\n",
    "print(caption_model.summary())\n",
    "caption_encoding = caption_model(caption_input)\n",
    "noisy_encoding = caption_model(noisy_input)\n",
    "#print(caption_encoding.summary())\n",
    "#print(noisy_encoding.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption model description\n",
    "\n",
    "For the captions we define a noisy encoding model and a caption encoding model. Each model have an input layer with shape (50, ) because 50 is the maximum lenght defined for our captions. Both encoding models share the same architecture which contains the following elements:\n",
    "* An embedding layer which is initialized using the embeddings contained in the glove.6B.100d.txt document. This layer is not trained because the amount of data we have for our captions is small compared to the data used to create these embeddings.\n",
    "* An LSTM layer with 1024 output units that represent the neural codes corresponding to captions. \n",
    "\n",
    "Inspired by the construction of a siamese net, we use these architecture we create the noisy and the caption encoding models using both input layers. This way we are able to generate the caption encoding and the noisy encoding using the same base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Print_3:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.print_tensor(image_encoding)\n",
    "K.print_tensor(caption_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# dot_product = lambda x: K.dot(x[0], K.transpose(x[1]))\n",
    "# dot_product = lambda x: K.dot(x[0], x[1])\n",
    "# positive_examples = Lambda(dot_product)([image_encoding, caption_encoding])\n",
    "# ## think about how to implement this\n",
    "# negative_examples = Lambda(dot_product)([image_encoding, noisy_encoding])\n",
    "# layer for computing dot product between tensors\n",
    "positive_examples = dot([image_encoding, caption_encoding], axes=-1)\n",
    "negative_examples = dot([image_encoding, noisy_encoding], axes=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training model\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading the training model\")\n",
    "training_model = Model(inputs=[image_input, caption_input, noisy_input], outputs=[positive_examples, negative_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sub-models for retrieving Neural codes\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading sub-models for retrieving Neural codes\")\n",
    "caption_model = Model(inputs=caption_input, outputs=caption_encoding)\n",
    "image_model = Model(inputs=image_input, outputs=image_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function as a loss for maximizing the margin between a positive and\n",
    "negative example.  If we call $p_i$ the score of the positive pair of the $i$-th example, and $n_i$ the score of the negative pair of that example, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "loss = \\sum_i{max(0, 1 -p_i + n_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "#     K.print_tensor(y_true)\n",
    "#     K.print_tensor(y_pred)\n",
    "    loss_ = K.sum(K.maximum(0., 1. - y_pred[0] + y_pred[1]))\n",
    "    #loss_ = K.sum(K.maximum(0., 1. - y_true + y_pred))\n",
    "    \n",
    "    return loss_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric for max-margin loss\n",
    "How many times did the positive pair effectively get a higher value than the negative pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def accuracy(y_true, y_pred):\n",
    "    # this line is used to debug the dimensions of y_pred[0]\n",
    "    #return K.sum(K.maximum(0., 1. - y_pred[0] + y_true))\n",
    "\n",
    "    # positive values take position 1 and negative position 0\n",
    "    return K.mean(K.argmax((y_pred[1], y_pred[0]), axis=1))\n",
    "    #return K.mean(K.max(K.round(y_pred[0] - y_pred[1] + 0.5), 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the training model\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "print (\"compiling the training model\")\n",
    "training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=['acc', accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation description\n",
    "During the compilation process we use our loss and accuracy functions previously defined. The optimizer used is adam for the advantages it has over plain stochastic gradient descent and RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation for training the model\n",
    "\n",
    "* adjust the length of captions into fixed maximum length (50 words)\n",
    "* sampling caption for each image, while shuffling the image data\n",
    "* encode captions into integer format based on look-up vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a woman wearing a net on her head cutting a cake', b'a woman cutting a large white sheet cake', b'a woman wearing a hair net cutting a large sheet cake', b'there is a woman that is cutting a white cake', b'a woman marking a cake with the back of a chefs knife', b'a young boy standing in front of a computer keyboard', b'a little boy wearing headphones and looking at a computer monitor', b'he is listening intently to the computer at school', b'a young boy stares up at the computer monitor', b'a young kid with head phones on using a computer']\n",
      "[[0.00495117 0.         0.         ... 0.         0.00079085 0.01148911]\n",
      " [0.00460475 0.02057743 0.01465815 ... 0.         0.         0.02613593]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['caps'][0:10])\n",
    "print(train_data['ims'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling one caption per image\n",
    "# return image_ids, caption_ids\n",
    "import random\n",
    "\n",
    "def sampling_img_cap(data):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # first sample for each image a caption\n",
    "    pairs_ids = []\n",
    "    \n",
    "    for image_id in range(len(data['ims'])):\n",
    "        caption_delta = random.randint(0, 4)\n",
    "        caption_id = image_id*5 + caption_delta\n",
    "        pairs_ids.append((image_id, caption_id))\n",
    "\n",
    "    # then shuffle the image data with the corresponding captions\n",
    "    random.shuffle(pairs_ids)\n",
    "    image_ids, caption_ids = zip(*pairs_ids)\n",
    "    \n",
    "    return image_ids, caption_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform raw text caption into integer sequences of fixed maximum length\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def prepare_caption(caption_ids, caption_data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    selected_captions = []\n",
    "    for i in caption_ids:\n",
    "        caption_text = caption_data[i]\n",
    "        caption_tokens = [words_indices[w] if w in words_indices.keys() else words_indices['<unk>'] for w in caption_text]\n",
    "        selected_captions.append(caption_tokens)\n",
    "        \n",
    "    selected_captions_arr = np.array(selected_captions)        \n",
    "    caption_seqs = sequence.pad_sequences(selected_captions_arr, maxlen=caption_max_length, value=0)\n",
    "      \n",
    "    return caption_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_caps = []\n",
    "for cap in train_data['caps']:\n",
    "    train_caps.append(cap.decode())\n",
    "\n",
    "val_caps = []\n",
    "for cap in val_data['caps']:\n",
    "    val_caps.append(cap.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_image_ids, train_caption_ids = sampling_img_cap(train_data)\n",
    "val_image_ids, val_caption_ids = sampling_img_cap(val_data)\n",
    "\n",
    "x_caption = prepare_caption(train_caption_ids, train_caps)\n",
    "x_image = train_data['ims'][np.array(train_image_ids)]\n",
    "\n",
    "x_val_caption = prepare_caption(val_caption_ids, val_caps)\n",
    "x_val_image = val_data['ims'][np.array(val_image_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a subway station with a parked subway train' b'formal living area in nice home with a fire place'\n",
      "[b'a slice of pizza on a white plate', b'two people carrying snowboards and walking through the snow']\n",
      "[b'a subway station with a parked subway train', b'formal living area in nice home with a fire place']\n",
      "Debug info\n",
      "5000\n",
      "5000\n",
      "('COCO_val2014_000000538153.jpg', 'http://images.cocodataset.org/val2014/COCO_val2014_000000538153.jpg', 'a subway station with a parked subway train', array([0.00182756, 0.01700551, 0.00838085, ..., 0.00813467, 0.        ,\n",
      "       0.        ], dtype=float32))\n",
      "(None, None, None, array([0.00034935, 0.05715366, 0.        , ..., 0.        , 0.00069344,\n",
      "       0.        ], dtype=float32))\n",
      "('COCO_val2014_000000255158.jpg', 'http://images.cocodataset.org/val2014/COCO_val2014_000000255158.jpg', 'a stylized photo of a woman riding a bicycle in a city', array([0.00023883, 0.00135924, 0.00537193, ..., 0.        , 0.01723759,\n",
      "       0.01040419], dtype=float32))\n",
      "('COCO_val2014_000000402903.jpg', 'http://images.cocodataset.org/val2014/COCO_val2014_000000402903.jpg', 'a reflection on the exterior window of a vehicle', array([0.00269235, 0.00016258, 0.00044815, ..., 0.00011246, 0.        ,\n",
      "       0.        ], dtype=float32))\n",
      "('COCO_val2014_000000087144.jpg', 'http://images.cocodataset.org/val2014/COCO_val2014_000000087144.jpg', 'a baby sitting between a lady and a man on a bench under a tree', array([0.        , 0.03284162, 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(val_data['caps'][val_caption_ids[0]], val_data['caps'][val_caption_ids[1]])\n",
    "#print(train_data['caps'][:2])\n",
    "\n",
    "sample_train_caption = [train_data['caps'][x] for x in train_caption_ids]\n",
    "sample_val_caption = [val_data['caps'][x] for x in val_caption_ids]\n",
    "\n",
    "print(sample_train_caption[0:2])\n",
    "print(sample_val_caption[0:2])\n",
    "\n",
    "\n",
    "# debug implementation of get_features_info\n",
    "\n",
    "print(\"Debug info\")\n",
    "print(len(x_val_image))\n",
    "print(len(sample_val_caption))\n",
    "\n",
    "for i in range(5):\n",
    "    image_info = get_features_info(i, sample_val_caption, x_val_image,\n",
    "                                   coco_caption_val['annotations'], coco_caption_val['images'])\n",
    "    print(image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create noise set for negative examples of image-fake caption and dummy output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not have real output with labels for training the model. Keras architecture expects labels, so we need to create dummy output -- which is numpy array of zeros. This dummy labels or output is never used since we compute loss function based on margin between positive examples (image-real caption) and negative examples (image-fake caption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "11473\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     2     1  1867  4107  1010  5632  5035\n",
      "     1  2086 10788     1  6453  1010  7408  7408     2     1  2086  3512\n",
      "     1     2     1  3314  6000  1010  1315  5035     1  6453  4107     2\n",
      "  1315  5035]\n",
      "[ 2 23 11 38 49 29 17 48 43 47]\n"
     ]
    }
   ],
   "source": [
    "print(x_caption[0].shape)\n",
    "print(len(words_indices))\n",
    "print(x_caption[0])\n",
    "ids = np.random.choice(x_caption[0].shape[0], 10, replace=False)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    2, 1315, 5035],\n",
       "       [5035,    1, 5632, ..., 3512, 2086, 3314],\n",
       "       [5632,    1,    1, ..., 2086, 6453, 5035],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 6000, 1315, 1867],\n",
       "       [   0,    0,    0, ..., 1010, 4107, 4107],\n",
       "       [   0,    0,    0, ...,    2, 4107, 4107]], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def generate_noisy_caption(caption):\n",
    "    ids = np.random.choice(caption.shape[0], 20, replace=False)\n",
    "    values = np.random.choice(len(words_indices), 20)\n",
    "    #print(values)\n",
    "    #print(caption)\n",
    "    new_caption = caption\n",
    "    #print(new_caption)\n",
    "    \n",
    "    #for i in range\n",
    "    for i, val in zip(ids, values):\n",
    "        new_caption[i] = val\n",
    "    \n",
    "    return new_caption\n",
    "    \n",
    "#print(generate_noisy_caption(x_caption[0]))\n",
    "    \n",
    "def generate_noisy_captions(images, captions):\n",
    "    noisy_captions = np.empty(captions.shape, dtype=np.int32)\n",
    "    \n",
    "    i = 0\n",
    "    for img, caption in zip(images, captions):\n",
    "        noisy_caption = generate_noisy_caption(caption)\n",
    "        noisy_captions[i, :] = noisy_caption\n",
    "        i += 1\n",
    "    \n",
    "    return noisy_captions\n",
    "\n",
    "#print(x_caption[:2])\n",
    "#print(generate_noisy_captions(x_image[:2], x_caption[:2]))\n",
    "\n",
    "train_noise = generate_noisy_captions(x_image, x_caption)\n",
    "val_noise = generate_noisy_captions(x_val_image, x_val_caption)\n",
    "\n",
    "y_train_labels = np.zeros((len(x_image), rnn_output_units))\n",
    "y_val_labels = np.zeros((len(x_val_image), rnn_output_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 11395     0 ...     2  1315  5043]\n",
      " [ 5035  6181  5632 ...  3512  2086  3314]\n",
      " [ 5632     1  9506 ... 10465  6128  9343]\n",
      " ...\n",
      " [    0     0  6120 ...  7222  1149  8203]\n",
      " [    0     0  3324 ...  1010  4107 11399]\n",
      " [    0     0 11246 ...     2  4531  4107]]\n",
      "[[    0 11395     0 ...     2  1315  5043]\n",
      " [ 5035  6181  5632 ...  3512  2086  3314]\n",
      " [ 5632     1  9506 ... 10465  6128  9343]\n",
      " ...\n",
      " [    0     0  6120 ...  7222  1149  8203]\n",
      " [    0     0  3324 ...  1010  4107 11399]\n",
      " [    0     0 11246 ...     2  4531  4107]]\n"
     ]
    }
   ],
   "source": [
    "print(train_noise)\n",
    "print(x_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 11395,     0, ...,     2,  1315,  5043],\n",
       "       [ 5035,  6181,  5632, ...,  3512,  2086,  3314],\n",
       "       [ 5632,     1,  9506, ..., 10465,  6128,  9343],\n",
       "       ...,\n",
       "       [    0,     0,  6120, ...,  7222,  1149,  8203],\n",
       "       [    0,     0,  3324, ...,  1010,  4107, 11399],\n",
       "       [    0,     0, 11246, ...,     2,  4531,  4107]], dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_labels.shape)\n",
    "print(y_val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of noisy caption generation\n",
    "\n",
    "To generate noisy captions we make a copy of the original encoded captions. For each copy we modify 20 entries with elements from the word_index vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4096)\n",
      "(10000, 50)\n",
      "(10000, 50)\n",
      "(10000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(x_image.shape)\n",
    "print(x_caption.shape)\n",
    "print(train_noise.shape)\n",
    "print(y_train_labels.shape)\n",
    "\n",
    "X_train = [x_image, x_caption, train_noise]\n",
    "Y_train = [y_train_labels, y_train_labels]\n",
    "X_valid = [x_val_image, x_val_caption, val_noise]\n",
    "Y_valid = [y_val_labels, y_val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 538s 54ms/step - loss: 4.9789 - dot_3_loss: 2.4895 - dot_4_loss: 2.4895 - dot_3_acc: 0.0256 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0256 - dot_4_accuracy: 0.0000e+00 - val_loss: 4.3790 - val_dot_3_loss: 2.1895 - val_dot_4_loss: 2.1895 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 540s 54ms/step - loss: 5.6231 - dot_3_loss: 2.8116 - dot_4_loss: 2.8116 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 3.8019 - val_dot_3_loss: 1.9009 - val_dot_4_loss: 1.9009 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 541s 54ms/step - loss: 4.6512 - dot_3_loss: 2.3256 - dot_4_loss: 2.3256 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 4.4312 - val_dot_3_loss: 2.2156 - val_dot_4_loss: 2.2156 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 540s 54ms/step - loss: 3.4524 - dot_3_loss: 1.7262 - dot_4_loss: 1.7262 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 4.4668 - val_dot_3_loss: 2.2334 - val_dot_4_loss: 2.2334 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 539s 54ms/step - loss: 4.2332 - dot_3_loss: 2.1166 - dot_4_loss: 2.1166 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 4.4076 - val_dot_3_loss: 2.2038 - val_dot_4_loss: 2.2038 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 538s 54ms/step - loss: 5.6091 - dot_3_loss: 2.8045 - dot_4_loss: 2.8045 - dot_3_acc: 0.0406 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0406 - dot_4_accuracy: 0.0000e+00 - val_loss: 5.3896 - val_dot_3_loss: 2.6948 - val_dot_4_loss: 2.6948 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 537s 54ms/step - loss: 3.3684 - dot_3_loss: 1.6842 - dot_4_loss: 1.6842 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 3.2197 - val_dot_3_loss: 1.6099 - val_dot_4_loss: 1.6099 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 537s 54ms/step - loss: 2.4046 - dot_3_loss: 1.2023 - dot_4_loss: 1.2023 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 3.3930 - val_dot_3_loss: 1.6965 - val_dot_4_loss: 1.6965 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 537s 54ms/step - loss: 3.2239 - dot_3_loss: 1.6120 - dot_4_loss: 1.6120 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00 - val_loss: 3.1968 - val_dot_3_loss: 1.5984 - val_dot_4_loss: 1.5984 - val_dot_3_acc: 0.0000e+00 - val_dot_3_accuracy: 0.0000e+00 - val_dot_4_acc: 0.0000e+00 - val_dot_4_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 2.8050 - dot_3_loss: 1.4025 - dot_4_loss: 1.4025 - dot_3_acc: 0.0000e+00 - dot_3_accuracy: 0.0000e+00 - dot_4_acc: 0.0000e+00 - dot_4_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "training_model.fit(X_train, Y_train, batch_size=128, validation_data=(X_valid, Y_valid), epochs=10)\n",
    "# fit the model on training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing models and weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "# Save model\n",
    "training_model.save(os.path.join(MODEL_PATH,'image_caption_model.h5'))\n",
    "# Save weight parameters\n",
    "training_model.save_weights(os.path.join(MODEL_PATH, 'weights_image_caption.hdf5'))\n",
    "\n",
    "# Save model for encoding caption and image\n",
    "caption_model.save(os.path.join(MODEL_PATH,'caption_model.h5'))\n",
    "image_model.save(os.path.join(MODEL_PATH,'image_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature extraction (Neural codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Use caption_model and image_model to produce \"Neural codes\" \n",
    "# for both image and caption from validation set\n",
    "val_image_codes = image_model.predict(x_val_image)\n",
    "val_caption_codes = caption_model.predict(x_val_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_image_codes.shape)\n",
    "print(val_caption_codes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Caption Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display original image as query and its ground truth caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#print(coco_caption_val['images'][:5])\n",
    "#print(val_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# choose one image_id from validation set\n",
    "# use this id to get filepath of image\n",
    "img_id = 16\n",
    "filepath_image, coco_url, original_caption, _ = get_features_info(img_id, sample_val_caption, x_val_image,\n",
    "                                                   coco_caption_val['annotations'], coco_caption_val['images'])\n",
    "\n",
    "# display original caption\n",
    "#original_caption = \n",
    "print(original_caption)\n",
    "IMAGE_DATA = DATA_PATH + \"/val2014\"\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "img = image.load_img(os.path.join(IMAGE_DATA,filepath_image), target_size=(224,224))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Debug prints\")\n",
    "print(get_features_from_filename(filepath_image, x_val_image, sample_val_caption))\n",
    "print(val_caption_ids[img_id])\n",
    "print(x_val_image[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the knn model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "images_nn = NearestNeighbors(n_neighbors=11, p=2)\n",
    "images_nn.fit(val_image_codes)\n",
    "\n",
    "# function to retrieve caption, given an image query\n",
    "def get_caption(image_filename, image_codes = val_image_codes, n=10):\n",
    "    features, features_id, selected_caption = get_features_from_filename(image_filename, image_codes, sample_val_caption)\n",
    "    #print(features)\n",
    "    print(\"Caption of the query image: {}\".format(selected_caption))\n",
    "    similarity_scores, nn_ids = images_nn.kneighbors(np.array([features]))\n",
    "    #print(similarity_scores)\n",
    "    #print(nn_ids[0])\n",
    "    selected_captions = []\n",
    "    for nn_id, score in zip(nn_ids[0], similarity_scores[0]):\n",
    "        caption_id = val_caption_ids[nn_id]\n",
    "        print(\"Score: {:.2f}, caption: {}\".format(score, val_data['caps'][caption_id]))\n",
    "        #selected_captions.append(val_data['caps'][caption_id])\n",
    "    # YOUR CODE HERE\n",
    "    #print(selected_captions)\n",
    "    \n",
    "    \n",
    "    #return selected_captions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "get_caption(filepath_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "The captions retrieval from image queries procedure works as follows:\n",
    "* First, we construct a KNN instance using the neural codes of each image in the validation set.\n",
    "* Next, given an image query, we obtain its neural code representation.\n",
    "* Then, we select the 11 most similar images using the KNN instance. The first most similar image corresponds to the image itself and the other 10 the the 10 closest neighbors.\n",
    "* Using the ids of the selected neighbors we return the selected captions.\n",
    "\n",
    "In the example shown above, the query image shows a dog with a tuxedo. The retrieved captions are about pooh bear, woman, child, man, apple, etc. They are not related to the caption of the query image. The similarity values of the captions are very close to each other. This could indicate that our join model fails to generate caption representations that can represent wide ranges of similarity values between two captions.\n",
    "\n",
    "From the training results we observe that the model doesn't perform very well. Hence, our computed neural codes for the images are not able to represent similar pairs of (image, caption) in our dataset. Thus, it is not a surprise that our caption retrieval implementation retrieves captions unrelated to the image query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given text query, display retrieved image, similarity score, and its original caption \n",
    "\n",
    "captions_nn = NearestNeighbors(n_neighbors=10, p=2)\n",
    "captions_nn.fit(val_caption_codes)\n",
    "\n",
    "def encode_text(caption_text):\n",
    "    selected_captions = []\n",
    "    caption_tokens = [words_indices[w] if w in words_indices.keys() else words_indices['<unk>'] for w in caption_text]\n",
    "    selected_captions.append(caption_tokens)\n",
    "        \n",
    "    selected_captions_arr = np.array(selected_captions)        \n",
    "    caption_seqs = sequence.pad_sequences(selected_captions_arr, maxlen=caption_max_length, value=0)\n",
    "    \n",
    "    caption_codes = caption_model.predict(caption_seqs)\n",
    "    #print(caption_codes)\n",
    "    return caption_codes\n",
    "\n",
    "\n",
    "def search_image(text_caption, n=10):\n",
    "    #print(text_caption)\n",
    "    encoded_caption = encode_text(text_caption)\n",
    "    \n",
    "    #print(encoded_caption)\n",
    "    similarity_scores, nn_ids = captions_nn.kneighbors(np.array(encoded_caption))\n",
    "    #print(nn_ids)\n",
    "    print(similarity_scores)\n",
    "    selected_images = []\n",
    "    scores = []\n",
    "    selected_captions = []\n",
    "    for score, nn_id in zip(similarity_scores[0], nn_ids[0]): \n",
    "    #for nn_id in nn_ids[0]:\n",
    "        filepath_image, _, _, _ = get_features_info(nn_id, sample_val_caption, x_val_image, \n",
    "                                                    coco_caption_val['annotations'], \n",
    "                                                    coco_caption_val['images'])\n",
    "        if (filepath_image != None):\n",
    "            selected_images.append(filepath_image)\n",
    "            scores.append(score)\n",
    "            selected_caption = sample_val_caption[nn_id]\n",
    "            selected_captions.append(selected_caption)\n",
    "        \n",
    "    #print(selected_images)\n",
    "    #print(selected_captions)\n",
    "\n",
    "    if len(selected_images) < 10:\n",
    "        print(\"It appears that some images couldn't be retrieved from the selected captions\")\n",
    "    \n",
    "    for i in range(len(selected_images)):\n",
    "        image_filename = selected_images[i]\n",
    "        caption = selected_captions[i]\n",
    "        score = scores[i]\n",
    "        img = image.load_img(os.path.join(IMAGE_DATA,image_filename), target_size=(224,224))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        print(\"Caption: {}\".format(caption))\n",
    "        print(\"Score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider to use the following settings for image retrieval task.\n",
    "\n",
    "* use real caption that is available in validation set as a query.\n",
    "* use part of caption as query. For instance, instead of use the whole text sentence of the\n",
    "caption, you may consider to use key phrase or combination of words that is included in\n",
    "corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Debug info: \")\n",
    "print(sample_val_caption[15])\n",
    "\n",
    "print(get_features_info(3719, sample_val_caption, x_val_image, coco_caption_val['annotations'], coco_caption_val['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look similar captions\n",
    "def similar_captions(caption):\n",
    "    coco_annotations = coco_caption_val['annotations']\n",
    "\n",
    "    coco_img_index = None\n",
    "    for annotation in coco_images:\n",
    "        coco_filename = annotation['file_name']\n",
    "        if image_filename == coco_filename:\n",
    "            coco_img_index = annotation['id']            \n",
    "            break;\n",
    "            \n",
    "    possible_captions = []\n",
    "    for annotation in coco_annotations:\n",
    "        #print(annotation.keys())\n",
    "        caption_img_id = annotation['image_id']\n",
    "        if coco_img_index == caption_img_id:\n",
    "            possible_captions.append(annotation['caption'])\n",
    "\n",
    "    #print(possible_captions)\n",
    "    selected_feature_id = None\n",
    "    selected_caption = None\n",
    "    for i, caption in enumerate(caption_data):\n",
    "        caption = caption.decode('utf-8')\n",
    "        #print(i, caption)\n",
    "        #break\n",
    "        for possible_caption in possible_captions:\n",
    "            if possible_caption.endswith('.'):\n",
    "                possible_caption = possible_caption[:-1]\n",
    "            if caption.lower() == possible_caption.lower():\n",
    "                selected_feature_id = i\n",
    "                selected_caption = caption\n",
    "                \n",
    "        if selected_feature_id != None:\n",
    "                break\n",
    "\n",
    "        \n",
    "    # (not needed anymore) map the selected feature id [i*5: (i+1)*5] to the actual id\n",
    "    #feature_id, _ = divmod(selected_feature_id, 5)\n",
    "    features_vector = features_data[selected_feature_id]\n",
    "    #compare selected captions with captions from \n",
    "    return (features_vector, selected_feature_id, selected_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text query \n",
    "# text = 'two giraffes standing near trees'\n",
    "\n",
    "# YOUR QUERY-1\n",
    "text1 = sample_val_caption[2].decode('utf-8')\n",
    "#text1 = sample_val_caption[10].decode('utf-8')\n",
    "\n",
    "print(text1)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR QUERY-2\n",
    "text2 = \"pancakes with blueberries\"\n",
    "print(text2)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Our image retrieval procedure works as follows:\n",
    "\n",
    "* First, we create a KNN instance using the caption neural codes for the validation set.\n",
    "* Next, we create a neural code for a given caption query.\n",
    "* Then, we retrieve the 10 most similar captions for the given caption query.\n",
    "* Finally, we use the ids of the retrieved captions to return their associated images.\n",
    "\n",
    "Here, we also obtain bad performance. First, we query with the following text __\"pancakes with blueberries and log cabin maple syrup on a table\"__ and we obtain few images related to food. Then, we try with the following modification of the previous query __\"pancakes with blueberries\"__ and we obtain pictures of animals, houses, and people, but no pictures of food. Observing the similarity values obtained in both queries, we notice that they don't reach a 0.1 similarity value. This implies that our caption model is not able to generate good representations for captions. Also, from the results obtained during the training procedure of our join model we know that we are not able to learn good representations for the (image, caption) pairs. Thus, is not a surprise that our image retrieval procedure returns several unrelated images to the caption query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Our retrieval methods perform poorly on their tasks. Further analysis could help to determine whether we need more data to learn better representations or we need to modify the architecture of the model. So far, it looks like the training model should be modified to obtain better performance on the retrieval tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
