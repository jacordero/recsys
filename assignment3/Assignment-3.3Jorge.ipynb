{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Microsoft COCO (Common Objects in Context) data set to train our \"Image Caption Retrieval Model\". This data set consists of pretrained 10-crop VGG19 features (Neural codes) and its corresponding text caption. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "EMBEDDING_PATH = 'embeddings'\n",
    "MODEL_PATH = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to create above directories and locate data set provided in directory 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading pairs of image (VGG19 features) - caption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "import collections\n",
    "\n",
    "np_train_data = np.load(os.path.join(DATA_PATH,'train_data.npy'))\n",
    "np_val_data = np.load(os.path.join(DATA_PATH,'val_data.npy'))\n",
    "\n",
    "train_data = collections.OrderedDict()\n",
    "for i in range(len(np_train_data.item())):\n",
    "    cap =  np_train_data.item()['caps']\n",
    "    img =  np_train_data.item()['ims']\n",
    "    train_data['caps'] = cap\n",
    "    train_data['ims'] = img\n",
    "    \n",
    "val_data = collections.OrderedDict()\n",
    "for i in range(len(np_val_data.item())):\n",
    "    cap =  np_val_data.item()['caps']\n",
    "    img =  np_val_data.item()['ims']\n",
    "    val_data['caps'] = cap\n",
    "    val_data['ims'] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a woman wearing a net on her head cutting a cake'\n",
      "(10000, 4096)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# example of caption\n",
    "print(train_data['caps'][0])\n",
    "print(train_data['ims'].shape)\n",
    "print(len(train_data['caps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00109166 0.         0.         ... 0.         0.         0.        ]\n",
      "(5000, 4096)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# example of pre-computed VGG19 features\n",
    "print(val_data['ims'][0])\n",
    "print(val_data['ims'].shape)\n",
    "print(len(val_data['caps']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading caption and information about its corresponding raw images from Microsoft COCO website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "# use them for your own additional preprocessing step\n",
    "# to map precomputed features and location of raw images \n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(DATA_PATH,'instances_val2014.json')) as json_file:\n",
    "    coco_instances_val = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(DATA_PATH,'captions_val2014.json')) as json_file:\n",
    "    coco_caption_val = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own function to map pairs of precomputed features and filepath of raw images\n",
    "# this will be used later for visualization part\n",
    "# simple approach: based on matched text caption (see json file)\n",
    "\n",
    "def match_features_and_filepath(index, features_data, coco_annotations, coco_images):\n",
    "    low_index = index*5\n",
    "    top_index = (index+1)*5\n",
    "    features_captions = features_data['caps'][low_index: top_index]\n",
    "    features_vector = features_data['ims'][index]\n",
    "    img_id = None\n",
    "    selected_caption = None\n",
    "    \n",
    "    for caption in features_captions:\n",
    "        if img_id != None:\n",
    "            break\n",
    "        str_caption = caption.decode('utf-8')\n",
    "        for annotation in coco_annotations:\n",
    "            coco_caption = annotation['caption']\n",
    "            if coco_caption.endswith('.'):\n",
    "                coco_caption = coco_caption[:-1]\n",
    "            if str_caption.lower() == coco_caption.lower():\n",
    "                selected_caption = str_caption\n",
    "                img_id = annotation['image_id']\n",
    "                #print(annotation)\n",
    "                break\n",
    "    \n",
    "    img_url = None\n",
    "    for img_data in coco_images:\n",
    "        if img_data['id'] == img_id:\n",
    "            img_url = img_data['coco_url']\n",
    "            break\n",
    "    \n",
    "    return (img_url, selected_caption, features_vector)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000522418.jpg', 'a woman cutting a large white sheet cake', array([0.00495117, 0.        , 0.        , ..., 0.        , 0.00079085,\n",
      "       0.01148911], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000318219.jpg', 'a young boy standing in front of a computer keyboard', array([0.00460475, 0.02057743, 0.01465815, ..., 0.        , 0.        ,\n",
      "       0.02613593], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000554625.jpg', 'a boy wearing headphones using one computer in a long row of computers', array([0.00487754, 0.0193617 , 0.00561136, ..., 0.        , 0.        ,\n",
      "       0.01967784], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000397133.jpg', 'a man is in a kitchen making pizzas', array([0.03168815, 0.02294972, 0.00275241, ..., 0.        , 0.        ,\n",
      "       0.00495452], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000574769.jpg', 'a woman in a room with a cat', array([0.00537503, 0.00276216, 0.03403776, ..., 0.        , 0.03405748,\n",
      "       0.00679124], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000309022.jpg', 'a kitchen has all stainless steel appliances and counters', array([0.01313584, 0.01320351, 0.04603083, ..., 0.00181162, 0.        ,\n",
      "       0.01283785], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000005802.jpg', 'chefs preparing food in a professional metallic style kitchen', array([2.1481903e-02, 0.0000000e+00, 5.9161161e-05, ..., 0.0000000e+00,\n",
      "       7.7512334e-03, 3.0743960e-02], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000222564.jpg', 'two cooks are cooking the food someone ordered at this restaurant', array([0.03231374, 0.00249605, 0.07357662, ..., 0.00164444, 0.        ,\n",
      "       0.01275615], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000118113.jpg', 'this is a very dark picture of a room with a shelf', array([0.01480379, 0.02325615, 0.        , ..., 0.        , 0.00504187,\n",
      "       0.        ], dtype=float32))\n",
      "('http://images.cocodataset.org/val2014/COCO_val2014_000000193271.jpg', 'a kitchen filled with black appliances and lots of counter top space', array([0.00124641, 0.05636261, 0.02407175, ..., 0.        , 0.01012944,\n",
      "       0.01593034], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# debug implementation of previous function\n",
    "for i in range(10):\n",
    "    image_info = match_features_and_filepath(i, train_data, coco_caption_val['annotations'], coco_caption_val['images'])\n",
    "    print(image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 11473\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "def build_dictionary(text):\n",
    "\n",
    "    wordcount = OrderedDict()\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "    words = list(wordcount.keys())\n",
    "    freqs = list(wordcount.values())\n",
    "    sorted_idx = np.argsort(freqs)[::-1]\n",
    "    \n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict['<pad>'] = 0\n",
    "    worddict['<unk>'] = 1\n",
    "    for idx, sidx in enumerate(sorted_idx):\n",
    "        worddict[words[sidx]] = idx+2  # 0: <pad>, 1: <unk>\n",
    "    \n",
    "\n",
    "    return worddict\n",
    "\n",
    "# use the resulting vocabulary index as your look up dictionary\n",
    "# to transform raw text into integer sequences\n",
    "\n",
    "all_captions = []\n",
    "all_captions = train_data['caps'] + val_data['caps']\n",
    "\n",
    "# decode bytes to string format\n",
    "caps = []\n",
    "for w in all_captions:\n",
    "    caps.append(w.decode())\n",
    "    \n",
    "words_indices = build_dictionary(caps)\n",
    "print ('Dictionary size: ' + str(len(words_indices)))\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image - Caption Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "#from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "#from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_neural_codes (Dense)   (None, 1024)              4195328   \n",
      "=================================================================\n",
      "Total params: 4,195,328\n",
      "Trainable params: 4,195,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE \n",
    "image_input = Input(shape=(4096, ))\n",
    "\n",
    "image_model = Sequential()\n",
    "image_model.add(Dense(1024, input_shape=(4096, ), name=\"image_neural_codes\", activation=\"sigmoid\"))\n",
    "image_model.summary()\n",
    "\n",
    "image_encoding = image_model(image_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pretrained embedding\n",
    "def load_embedding(vocab, dimension, filename):\n",
    "    print('loading embeddings from \"%s\"' % filename, file=sys.stderr)\n",
    "    embedding = np.zeros((max(vocab.values()) + 1, dimension), dtype=np.float32)\n",
    "    seen = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            tokens = line.strip().split(' ')\n",
    "            if len(tokens) == dimension + 1:\n",
    "                word = tokens[0]\n",
    "                if word in vocab:\n",
    "                    embedding[vocab[word]] = [float(x) for x in tokens[1:]]\n",
    "                    seen.add(word)\n",
    "                    if len(seen) == len(vocab):\n",
    "                        break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"data/glove.6B.100d.txt\"\n"
     ]
    }
   ],
   "source": [
    "weights = load_embedding(words_indices, 100, os.path.join(DATA_PATH,'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11473, 100)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_embedding (Embedding)   (None, 50, 100)           1147300   \n",
      "_________________________________________________________________\n",
      "caption_neural_codes (LSTM)  (None, 1024)              4608000   \n",
      "=================================================================\n",
      "Total params: 5,755,300\n",
      "Trainable params: 4,608,000\n",
      "Non-trainable params: 1,147,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "caption_max_length = 50\n",
    "embedding_size = 100\n",
    "rnn_output_units = 1024\n",
    "# For embedding layer, initialize with pretrained word embedding (GloVe)\n",
    "caption_input = Input(shape=(caption_max_length, ), name='input_layer', dtype='int32')\n",
    "noisy_input = Input(shape=(caption_max_length, ),  name='noisy_input_layer', dtype='int32')\n",
    "\n",
    "caption_model = Sequential()\n",
    "caption_model.add(Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "                            trainable=False, name='word_embedding'))\n",
    "caption_model.add(LSTM(rnn_output_units, name='caption_neural_codes'))\n",
    "# cts image and text (caption) in the same representation space. To buildthis model, you need two (2) sub models:  model for encoding image and model for encoding textas shown in figure 1.  You also need to train the model so that an image is c\n",
    "#model = Sequential()\n",
    "#model.add(Lambda(binarize, output_shape=binarize_outshape,name='char_embedding', \\\n",
    "#                 input_shape=(max_sequence_length,), dtype='int32'))\n",
    "#model.add(LSTM(rnn_dim, name='lstm_layer'))\n",
    "#model.add(Dense(1 , name='prediction_layer', activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## the input lenght of the embedding layer corresponds to the lenght of the encoded text sequences\n",
    "## which have a maximum of 50 words\n",
    "#embedding_layer = Embedding(len(words_indices), embedding_size, weights=[weights], input_length=caption_max_length, \\\n",
    "#                            trainable=False, name='word_embedding')\n",
    "#embedded_captions = embedding_layer(caption_input)\n",
    "#lstm_layer = LSTM(rnn_output_units, name='caption_neural_codes')(embedded_captions)\n",
    "print(caption_model.summary())\n",
    "caption_encoding = caption_model(caption_input)\n",
    "noisy_encoding = caption_model(noisy_input)\n",
    "#print(caption_encoding.summary())\n",
    "#print(noisy_encoding.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# dot_product = lambda x: K.dot(x[0], K.transpose(x[1]))\n",
    "dot_product = lambda x: K.dot(x[0], x[1])\n",
    "positive_examples = Lambda(dot_product)([image_encoding, caption_encoding])\n",
    "\n",
    "## think about how to implement this\n",
    "negative_examples = Lambda(dot_product)([image_encoding, noisy_encoding])\n",
    "# layer for computing dot product between tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the training model\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading the training model\")\n",
    "training_model = Model(inputs=[image_input, caption_input, noisy_input], outputs=[positive_examples, negative_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sub-models for retrieving Neural codes\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define your model input and output\n",
    "\n",
    "print (\"loading sub-models for retrieving Neural codes\")\n",
    "caption_model = Model(inputs=caption_input, outputs=caption_encoding)\n",
    "image_model = Model(inputs=image_input, outputs=image_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function as a loss for maximizing the margin between a positive and\n",
    "negative example.  If we call $p_i$ the score of the positive pair of the $i$-th example, and $n_i$ the score of the negative pair of that example, the loss is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "loss = \\sum_i{max(0, 1 -p_i + n_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    loss_ = K.sum(K.maximum(0., 1. - y_true + y_pred))\n",
    "    \n",
    "    return loss_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric for max-margin loss\n",
    "How many times did the positive pair effectively get a higher value than the negative pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def accuracy(y_true, y_pred):\n",
    "    # return K.mean(K.argmax((y_true, y_pred), axis=1))\n",
    "    return K.mean(K.round(y_true - y_pred + 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the training model\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "print (\"compiling the training model\")\n",
    "training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation for training the model\n",
    "\n",
    "* adjust the length of captions into fixed maximum length (50 words)\n",
    "* sampling caption for each image, while shuffling the image data\n",
    "* encode captions into integer format based on look-up vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a woman wearing a net on her head cutting a cake', b'a woman cutting a large white sheet cake', b'a woman wearing a hair net cutting a large sheet cake', b'there is a woman that is cutting a white cake', b'a woman marking a cake with the back of a chefs knife', b'a young boy standing in front of a computer keyboard', b'a little boy wearing headphones and looking at a computer monitor', b'he is listening intently to the computer at school', b'a young boy stares up at the computer monitor', b'a young kid with head phones on using a computer']\n",
      "[[0.00495117 0.         0.         ... 0.         0.00079085 0.01148911]\n",
      " [0.00460475 0.02057743 0.01465815 ... 0.         0.         0.02613593]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['caps'][0:10])\n",
    "print(train_data['ims'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling one caption per image\n",
    "# return image_ids, caption_ids\n",
    "import random\n",
    "\n",
    "def sampling_img_cap(data):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # first sample for each image a caption\n",
    "    pairs_ids = []\n",
    "    \n",
    "    for image_id in range(len(data['ims'])):\n",
    "        caption_delta = random.randint(0, 4)\n",
    "        caption_id = image_id*5 + caption_delta\n",
    "        pairs_ids.append((image_id, caption_id))\n",
    "\n",
    "    # then shuffle the image data with the corresponding captions\n",
    "    random.shuffle(pairs_ids)\n",
    "    image_ids, caption_ids = zip(*pairs_ids)\n",
    "    \n",
    "    return image_ids, caption_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform raw text caption into integer sequences of fixed maximum length\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def prepare_caption(caption_ids, caption_data):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    selected_captions = []\n",
    "    for i in caption_ids:\n",
    "        caption_text = caption_data[i]\n",
    "        caption_tokens = [words_indices[w] if w in words_indices.keys() else words_indices['<unk>'] for w in caption_text]\n",
    "        selected_captions.append(caption_tokens)\n",
    "        \n",
    "    selected_captions_arr = np.array(selected_captions)        \n",
    "    caption_seqs = sequence.pad_sequences(selected_captions_arr, maxlen=caption_max_length, value=0)\n",
    "      \n",
    "    return caption_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_caps = []\n",
    "for cap in train_data['caps']:\n",
    "    train_caps.append(cap.decode())\n",
    "\n",
    "val_caps = []\n",
    "for cap in val_data['caps']:\n",
    "    val_caps.append(cap.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "train_image_ids, train_caption_ids = sampling_img_cap(train_data)\n",
    "val_image_ids, val_caption_ids = sampling_img_cap(val_data)\n",
    "\n",
    "x_caption = prepare_caption(train_caption_ids, train_caps)\n",
    "x_image = train_data['ims'][np.array(train_image_ids)]\n",
    "\n",
    "x_val_caption = prepare_caption(val_caption_ids, val_caps)\n",
    "x_val_image = val_data['ims'][np.array(val_image_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create noise set for negative examples of image-fake caption and dummy output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not have real output with labels for training the model. Keras architecture expects labels, so we need to create dummy output -- which is numpy array of zeros. This dummy labels or output is never used since we compute loss function based on margin between positive examples (image-real caption) and negative examples (image-fake caption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "11473\n",
      "[   0    0    0    0    0    0    0    0    0    0 6453 5035 2086 6453\n",
      " 4107 5035    1 6050 1010 3995 1010 3512 6096    1 2086 3512    1 5035\n",
      " 4107 5035 6453 6000    2 3512 1315 1867    1 1010 3512    1 1315 6000\n",
      " 5035    1    1 8894 3512 6096 4107 5035]\n",
      "[ 7 48 35  6 47 19 40 32 33 31]\n"
     ]
    }
   ],
   "source": [
    "print(x_caption[0].shape)\n",
    "print(len(words_indices))\n",
    "print(x_caption[0])\n",
    "ids = np.random.choice(x_caption[0].shape[0], 10, replace=False)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  6096,  4107,  5035],\n",
       "       [    0,     0,     0, ...,  6050,  5035,     2],\n",
       "       [ 2086,  3758,     2, ...,  1010, 10788,  5035],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     2,  1867,  5035],\n",
       "       [    1,  5632,  5035, ...,  3756,  4107,  5035],\n",
       "       [ 1010,  5035,     1, ...,     2,  1315,  5035]], dtype=int32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def generate_noisy_caption(caption):\n",
    "    ids = np.random.choice(caption.shape[0], 20, replace=False)\n",
    "    values = np.random.choice(len(words_indices), 20)\n",
    "    #print(values)\n",
    "    #print(caption)\n",
    "    new_caption = caption\n",
    "    #print(new_caption)\n",
    "    \n",
    "    #for i in range\n",
    "    for i, val in zip(ids, values):\n",
    "        new_caption[i] = val\n",
    "    \n",
    "    return new_caption\n",
    "    \n",
    "#print(generate_noisy_caption(x_caption[0]))\n",
    "    \n",
    "def generate_noisy_captions(images, captions):\n",
    "    noisy_captions = np.empty(captions.shape, dtype=np.int32)\n",
    "    \n",
    "    i = 0\n",
    "    for img, caption in zip(images, captions):\n",
    "        noisy_caption = generate_noisy_caption(caption)\n",
    "        noisy_captions[i, :] = noisy_caption\n",
    "        i += 1\n",
    "    \n",
    "    return noisy_captions\n",
    "\n",
    "#print(x_caption[:2])\n",
    "#print(generate_noisy_captions(x_image[:2], x_caption[:2]))\n",
    "\n",
    "train_noise = generate_noisy_captions(x_image, x_caption)\n",
    "val_noise = generate_noisy_captions(x_val_image, x_val_caption)\n",
    "\n",
    "y_train_labels = np.zeros((len(x_image), rnn_output_units))\n",
    "y_val_labels = np.zeros((len(x_val_image), rnn_output_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 6315,    0, ..., 6096, 4107, 5035],\n",
       "       [   0,    0, 9687, ..., 4756, 5035, 7251],\n",
       "       [2086, 3758,   14, ..., 7926, 5465, 5035],\n",
       "       ...,\n",
       "       [   0, 5547,    0, ..., 5130, 5683, 6360],\n",
       "       [5182, 5632, 5035, ...,  966, 4107, 5035],\n",
       "       [5955,  363,    1, ..., 5476, 1846, 5035]], dtype=int32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 6315,    0, ..., 6096, 4107, 5035],\n",
       "       [   0,    0, 9687, ..., 4756, 5035, 7251],\n",
       "       [2086, 3758,   14, ..., 7926, 5465, 5035],\n",
       "       ...,\n",
       "       [   0, 5547,    0, ..., 5130, 5683, 6360],\n",
       "       [5182, 5632, 5035, ...,  966, 4107, 5035],\n",
       "       [5955,  363,    1, ..., 5476, 1846, 5035]], dtype=int32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_labels.shape)\n",
    "print(y_val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train = [x_image, x_caption, train_noise]\n",
    "Y_train = [y_train_labels, y_train_labels]\n",
    "X_valid = [x_val_image, x_val_caption, val_noise]\n",
    "Y_valid = [y_val_labels, y_val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [256,1024] vs. [256,256]\n\t [[Node: training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@loss_20/lambda_11_loss/add\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape, training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape_1)]]\n\nCaused by op 'training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs', defined at:\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-160-f712045472bd>\", line 2, in <module>\n    training_model.fit(X_train, Y_train, batch_size=256, validation_data=(X_valid, Y_valid), epochs=15)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1646, in fit\n    self._make_train_function()\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 970, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 434, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2512, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 751, in _AddGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 528, in _broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'loss_20/lambda_11_loss/add', defined at:\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 17 identical lines from previous traceback]\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-155-13336cd77125>\", line 3, in <module>\n    training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 827, in compile\n    sample_weight, mask)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 426, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-153-fc8385fe3bc3>\", line 6, in max_margin_loss\n    loss_ = K.sum(K.maximum(0., 1. - y_true + y_pred))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 907, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 183, in add\n    \"Add\", x=x, y=y, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [256,1024] vs. [256,256]\n\t [[Node: training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@loss_20/lambda_11_loss/add\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape, training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [256,1024] vs. [256,256]\n\t [[Node: training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@loss_20/lambda_11_loss/add\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape, training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-f712045472bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit the model on training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [256,1024] vs. [256,256]\n\t [[Node: training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@loss_20/lambda_11_loss/add\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape, training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape_1)]]\n\nCaused by op 'training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs', defined at:\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-160-f712045472bd>\", line 2, in <module>\n    training_model.fit(X_train, Y_train, batch_size=256, validation_data=(X_valid, Y_valid), epochs=15)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1646, in fit\n    self._make_train_function()\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 970, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 434, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2512, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 751, in _AddGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 528, in _broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'loss_20/lambda_11_loss/add', defined at:\n  File \"/home/mathyn/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 17 identical lines from previous traceback]\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-155-13336cd77125>\", line 3, in <module>\n    training_model.compile(optimizer='adam', loss=max_margin_loss, metrics=[accuracy])\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 827, in compile\n    sample_weight, mask)\n  File \"/home/mathyn/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 426, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-153-fc8385fe3bc3>\", line 6, in max_margin_loss\n    loss_ = K.sum(K.maximum(0., 1. - y_true + y_pred))\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 907, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 183, in add\n    \"Add\", x=x, y=y, name=name)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/mathyn/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [256,1024] vs. [256,256]\n\t [[Node: training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@loss_20/lambda_11_loss/add\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape, training_2/Adam/gradients/loss_20/lambda_11_loss/add_grad/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "training_model.fit(X_train, Y_train, batch_size=256, validation_data=(X_valid, Y_valid), epochs=15)\n",
    "# fit the model on training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing models and weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "\n",
    "# Save model\n",
    "training_model.save(os.path.join(MODEL_PATH,'image_caption_model.h5'))\n",
    "# Save weight parameters\n",
    "training_model.save_weights(os.path.join(MODEL_PATH, 'weights_image_caption.hdf5'))\n",
    "\n",
    "# Save model for encoding caption and image\n",
    "caption_model.save(os.path.join(MODEL_PATH,'caption_model.h5'))\n",
    "image_model.save(os.path.join(MODEL_PATH,'image_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature extraction (Neural codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Use caption_model and image_model to produce \"Neural codes\" \n",
    "# for both image and caption from validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Caption Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display original image as query and its ground truth caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# choose one image_id from validation set\n",
    "# use this id to get filepath of image\n",
    "img_id = \n",
    "filepath_image = \n",
    "\n",
    "# display original caption\n",
    "original_caption = \n",
    "print(original_caption)\n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "img = image.load_img(os.path.join(IMAGE_DATA,filepath_image), target_size=(224,224))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve caption, given an image query\n",
    "\n",
    "def get_caption(image_filename, n=10):   \n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE BELOW CODE\n",
    "get_caption(filepath_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given text query, display retrieved image, similarity score, and its original caption \n",
    "\n",
    "def search_image(text_caption, n=10):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider to use the following settings for image retrieval task.\n",
    "\n",
    "* use real caption that is available in validation set as a query.\n",
    "* use part of caption as query. For instance, instead of use the whole text sentence of the\n",
    "caption, you may consider to use key phrase or combination of words that is included in\n",
    "corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text query \n",
    "# text = 'two giraffes standing near trees'\n",
    "\n",
    "# YOUR QUERY-1\n",
    "text1 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR QUERY-2\n",
    "text2 = \n",
    "\n",
    "# DO NOT CHANGE BELOW CODE\n",
    "search_image(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly discuss the result. Why or how it works, and why do you think it does not work at some point.\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "=== write your answer here ==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
